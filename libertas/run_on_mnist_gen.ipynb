{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/is/ei/fleeb/workspace/foundation\n"
     ]
    }
   ],
   "source": [
    "import sys, os, time\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.distributions as distrib\n",
    "import torch.multiprocessing as mp\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import gym\n",
    "import numpy as np\n",
    "%matplotlib notebook\n",
    "#%matplotlib tk\n",
    "import matplotlib.pyplot as plt\n",
    "#plt.switch_backend('Qt5Agg') #('Qt5Agg')\n",
    "import foundation as fd\n",
    "from foundation import models\n",
    "from foundation import util\n",
    "from foundation import train\n",
    "\n",
    "from langimg import *\n",
    "\n",
    "np.set_printoptions(linewidth=120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[autoreload of langimg failed: Traceback (most recent call last):\n",
      "  File \"/is/ei/fleeb/anaconda3/lib/python3.7/site-packages/IPython/extensions/autoreload.py\", line 245, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/is/ei/fleeb/anaconda3/lib/python3.7/site-packages/IPython/extensions/autoreload.py\", line 450, in superreload\n",
      "    update_generic(old_obj, new_obj)\n",
      "  File \"/is/ei/fleeb/anaconda3/lib/python3.7/site-packages/IPython/extensions/autoreload.py\", line 387, in update_generic\n",
      "    update(a, b)\n",
      "  File \"/is/ei/fleeb/anaconda3/lib/python3.7/site-packages/IPython/extensions/autoreload.py\", line 342, in update_class\n",
      "    if update_generic(old_obj, new_obj): continue\n",
      "  File \"/is/ei/fleeb/anaconda3/lib/python3.7/site-packages/IPython/extensions/autoreload.py\", line 387, in update_generic\n",
      "    update(a, b)\n",
      "  File \"/is/ei/fleeb/anaconda3/lib/python3.7/site-packages/IPython/extensions/autoreload.py\", line 266, in update_function\n",
      "    setattr(old, name, getattr(new, name))\n",
      "ValueError: reset() requires a code object with 0 free vars, not 1\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "x = torch.zeros(5,3, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.]], requires_grad=True)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], grad_fn=<PowBackward0>)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dists = F.pdist(x).pow(2)\n",
    "dists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = 1 / dists[dists > 0]\n",
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(nan, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "mx = lt.max(-1)[0]\n",
    "sel = mx > 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ True, False,  True,  True,  True])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "lt[sel, :] /= mx[sel].unsqueeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.4462,  0.2269,  1.0000],\n",
       "        [-0.0323, -1.2539, -1.1374],\n",
       "        [ 1.0000, -0.7698, -1.0279],\n",
       "        [ 0.1622, -0.2820, -0.2243],\n",
       "        [ 1.0000,  0.3112,  0.5726]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (3) must match the size of tensor b (2) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-6d3998c5e080>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmn\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mlt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0mmn\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (3) must match the size of tensor b (2) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "mn = lt.min(-1)[0]\n",
    "sel = mn < -1\n",
    "lt[sel, :] /= mn[sel].abs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save dir: ../trained_nets/direct_decoder_dim20/19-10-01-155147\n",
      "Logging in ../trained_nets/direct_decoder_dim20/19-10-01-155147\n",
      "Using device cuda:0 - random seed set to 0\n"
     ]
    }
   ],
   "source": [
    "args = util.NS()\n",
    "\n",
    "args.device = 'cuda:0'\n",
    "args.seed = 0\n",
    "\n",
    "args.logdate = True\n",
    "args.tblog = True\n",
    "args.txtlog = False\n",
    "args.saveroot = '../trained_nets'\n",
    "\n",
    "args.dataset = 'mnist'\n",
    "args.indexed = True\n",
    "args.use_val = False\n",
    "\n",
    "args.num_workers = 4\n",
    "args.batch_size = 128\n",
    "\n",
    "args.start_epoch = 0\n",
    "args.epochs = 10\n",
    "\n",
    "args.optim_type = 'adam'\n",
    "args.lr = 1e-2\n",
    "args.weight_decay = 1e-4\n",
    "args.momentum = 0\n",
    "\n",
    "\n",
    "args.latent_dim = 20\n",
    "\n",
    "args.zero_embedding = True\n",
    "args.normalize_latent = False\n",
    "args.latent_lr = None\n",
    "\n",
    "args.name = 'direct_decoder_dim{}'.format(args.latent_dim)\n",
    "\n",
    "args.save_model = False\n",
    "\n",
    "\n",
    "now = time.strftime(\"%y-%m-%d-%H%M%S\")\n",
    "if args.logdate:\n",
    "    args.name = os.path.join(args.name, now)\n",
    "args.save_dir = os.path.join(args.saveroot, args.name)\n",
    "print('Save dir: {}'.format(args.save_dir))\n",
    "\n",
    "if args.tblog or args.txtlog:\n",
    "    util.create_dir(args.save_dir)\n",
    "    print('Logging in {}'.format(args.save_dir))\n",
    "logger = util.Logger(args.save_dir, tensorboard=args.tblog, txt=args.txtlog)\n",
    "\n",
    "# Set seed\n",
    "if not hasattr(args, 'seed') or args.seed is None:\n",
    "    args.seed = util.get_random_seed()\n",
    "torch.manual_seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "try:\n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "    args.device = 'cpu'\n",
    "print('Using device {} - random seed set to {}'.format(args.device, args.seed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "traindata len=60000, trainloader len=469\n",
      "testdata len=10000, testloader len=79\n",
      "Batch size: 128 samples\n"
     ]
    }
   ],
   "source": [
    "datasets = train.load_data(args=args)\n",
    "\n",
    "loaders = train.get_loader(*datasets, batch_size=args.batch_size, num_workers=args.num_workers, \n",
    "                           shuffle=True, drop_last=False,)\n",
    "\n",
    "trainloader, testloader = loaders[0], loaders[-1]\n",
    "valloader = None if len(loaders) == 2 else loaders[1]\n",
    "\n",
    "print('traindata len={}, trainloader len={}'.format(len(datasets[0]), len(trainloader)))\n",
    "if valloader is not None:\n",
    "    print('valdata len={}, valloader len={}'.format(len(datasets[1]), len(valloader)))\n",
    "print('testdata len={}, testloader len={}'.format(len(datasets[-1]), len(testloader)))\n",
    "print('Batch size: {} samples'.format(args.batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DirectDecoder(\n",
      "  (table): Embedding(60000, 20)\n",
      "  (dec): Decoder(\n",
      "    (fc): Sequential(\n",
      "      (0): Linear(in_features=20, out_features=64, bias=True)\n",
      "      (1): PReLU(num_parameters=1)\n",
      "      (2): Linear(in_features=64, out_features=128, bias=True)\n",
      "      (3): PReLU(num_parameters=1)\n",
      "      (4): Linear(in_features=128, out_features=512, bias=True)\n",
      "      (5): PReLU(num_parameters=1)\n",
      "    )\n",
      "    (deconv): Sequential(\n",
      "      (0): DeconvLayer(\n",
      "        (deconv): Sequential(\n",
      "          (0): Upsample(size=(7, 7), mode=bilinear)\n",
      "          (1): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        )\n",
      "        (norm): InstanceNorm2d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (nonlin): PReLU(num_parameters=1)\n",
      "      )\n",
      "      (1): DeconvLayer(\n",
      "        (deconv): Sequential(\n",
      "          (0): Upsample(size=(14, 14), mode=bilinear)\n",
      "          (1): Conv2d(16, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        )\n",
      "        (norm): InstanceNorm2d(8, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (nonlin): PReLU(num_parameters=1)\n",
      "      )\n",
      "      (2): DeconvLayer(\n",
      "        (deconv): Sequential(\n",
      "          (0): Upsample(size=(28, 28), mode=bilinear)\n",
      "          (1): Conv2d(8, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        )\n",
      "        (nonlin): Sigmoid()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (criterion): BCELoss()\n",
      ")\n",
      "Model has 1281574 parameters\n"
     ]
    }
   ],
   "source": [
    "# Define Model\n",
    "args.total_samples = {'train': 0, 'test': 0}\n",
    "epoch = 0\n",
    "best_loss = None\n",
    "all_train_stats = []\n",
    "all_test_stats = []\n",
    "    \n",
    "model = DirectDecoder(latent_dim=args.latent_dim, out_shape=(1, 28, 28), vocab_size=len(datasets[0]),\n",
    "                      \n",
    "                      normalize_latent=args.normalize_latent, zero_embedding=args.zero_embedding,\n",
    "\n",
    "                          nonlin='prelu', output_nonlin='sigmoid',\n",
    "                          channels=[32, 16, 8], kernels=[3, 3, 3], ups=[2, 2, 2], upsampling='bilinear',\n",
    "                          output_norm_type=None,\n",
    "                          hidden_fc=[64, 128],\n",
    "                          )\n",
    "\n",
    "# optim = \n",
    "\n",
    "model.set_optim(optim_type=args.optim_type, lr=args.lr, weight_decay=args.weight_decay, momentum=args.momentum)\n",
    "# optim = util.get_optimizer('sgd', model.parameters(), )\n",
    "scheduler = None#torch.optim.lr_scheduler.StepLR(optim, step_size=6, gamma=0.2)\n",
    "\n",
    "model.to(args.device)\n",
    "print(model)\n",
    "print('Model has {} parameters'.format(util.count_parameters(model)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/is/ei/fleeb/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:2479: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  \"See the documentation of nn.Upsample for details.\".format(mode))\n",
      "/is/ei/fleeb/workspace/foundation/foundation/util/stats.py:180: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.val = torch.tensor(val).float()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 15:51:57 ] Epoch 1 Train=0.151 (0.517)\n",
      "[ 15:52:01 ] Epoch 2 Train=0.087 (0.356)\n",
      "[ 15:52:04 ] Epoch 3 Train=0.074 (0.308)\n",
      "[ 15:52:08 ] Epoch 4 Train=0.070 (0.290)\n",
      "[ 15:52:11 ] Epoch 5 Train=0.069 (0.281)\n",
      "[ 15:52:15 ] Epoch 6 Train=0.069 (0.276)\n",
      "[ 15:52:19 ] Epoch 7 Train=0.068 (0.273)\n",
      "[ 15:52:23 ] Epoch 8 Train=0.068 (0.272)\n",
      "[ 15:52:27 ] Epoch 9 Train=0.068 (0.270)\n",
      "[ 15:52:31 ] Epoch 10 Train=0.068 (0.270)\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "lr = model.optim.param_groups[0]['lr']\n",
    "for _ in range(10):\n",
    "    \n",
    "    old_lr = lr\n",
    "    if scheduler is not None:\n",
    "        scheduler.step()\n",
    "    lr = model.optim.param_groups[0]['lr']\n",
    "    \n",
    "    if lr != old_lr:\n",
    "        print('--- lr update: {:.3E} -> {:.3E} ---'.format(old_lr, lr))\n",
    "    \n",
    "    train_stats = util.StatsMeter('lr', tau=0.1)\n",
    "    train_stats.update('lr', lr)\n",
    "\n",
    "    train_stats = train.run_epoch(model, trainloader, args, mode='train',  \n",
    "                                      epoch=epoch, print_freq=10, logger=logger, silent=True,\n",
    "                                      viz_criterion_args=['reconstruction', 'original'],\n",
    "                                      stats=train_stats, )\n",
    "    \n",
    "    all_train_stats.append(train_stats)\n",
    "    \n",
    "    print('[ {} ] Epoch {} Train={:.3f} ({:.3f})'.format(\n",
    "        time.strftime(\"%H:%M:%S\"), epoch+1,\n",
    "        train_stats['loss-viz'].avg.item(), train_stats['loss'].avg.item(),\n",
    "    ))\n",
    "    \n",
    "    if args.save_model:\n",
    "        \n",
    "        av_loss = test_stats['loss'].avg.item()\n",
    "        is_best = best_loss is None or av_loss < best_loss\n",
    "        if is_best:\n",
    "            best_loss = av_loss\n",
    "        \n",
    "        path = train.save_checkpoint({\n",
    "            'epoch': epoch,\n",
    "            'args': args,\n",
    "            'model_str': str(model),\n",
    "            'model_state': model.state_dict(),\n",
    "            'all_train_stats': all_train_stats,\n",
    "            'all_test_stats': all_test_stats,\n",
    "            \n",
    "        }, args.save_dir, is_best=is_best, epoch=epoch+1)\n",
    "        print('--- checkpoint saved to {} ---'.format(path))\n",
    "\n",
    "    epoch += 1\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
