{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "To run this you need several packages. First of all, you need anaconda, which you most likely already have if you're viewing this through jupyter. If not then check the readme on the class page.\n",
    "\n",
    "System requirements: This should work on all operating systems (Linux, Mac, and Windows). However, several of the environments in the OpenAI-gym require additional simulators which don't aren't easy to get on Windows. In any case, it is strongly recommended that you use Linux, although you should be ok with Mac. (HINT: if you're on Windows check out the Windows Subsystem for Linux (WSL), although it'll make visualizing your policies a little tricky).\n",
    "\n",
    "Then install the following packages (using conda or pip):\n",
    "\n",
    "- pytorch --> `conda install pytorch -c pytorch; pip install torchvision`\n",
    "- gym --> `pip install gym`\n",
    "- gym (the cool environments, doesnt work on Windows) --> `pip install gym[all]`\n",
    "(When install gym[all] don't worry if the mujoco installation doesn't work. That's a more advanced 3D physics simulator that has to be set up separately (see website). Anyway, we don't need it necessarily)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.multiprocessing as mp\n",
    "from torch import distributions\n",
    "from torch.distributions import Categorical\n",
    "from itertools import islice\n",
    "\n",
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "Welcome to the RL playground. Your task is to implement the REINFORCE and A3C algorithm to solve various OpenAI-gym environments. If you are not familiar with OpenAI-gym, stop reading and visit https://gym.openai.com/envs/ to see all the tasks you can try to solve.\n",
    "\n",
    "In this homework, we will only look at tasks with a discrete (and small) action space. That being said, both algorithms can be modified slightly to work on tasks with continuous action spaces. For full credit you must fill in the code below so you achieve an average total reward per episode on the cartpole task (CartPole-v1) of at least 450 (for an episode length of 500) for both REINFORCE and A3C. Then you must apply your code to any one other environment in OpenAI-gym, and plot and compare the learning curves (average total reward per episode vs number of episodes trained on) between REINFORCE and A3C (where at least one of the algorithms shows significant improvement from initialization).\n",
    "\n",
    "Below there's an overview of what every iteration will look like, regardless of whether you want to train or evaluate your agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rlhw_util import * # <-- look whats inside here - it could save you a lot of work!\n",
    "\n",
    "def run_iteration(mode, N, agent, gen, horizon=None, render=False):\n",
    "    train = mode == 'train'\n",
    "    if train:\n",
    "        agent.train()\n",
    "    else:\n",
    "        agent.eval()\n",
    "\n",
    "    states, actions, rewards = zip(*[gen(horizon=horizon, render=render) for _ in range(N)])\n",
    "\n",
    "    loss = None\n",
    "    if train:\n",
    "        loss = agent.learn(states, actions, rewards)\n",
    "\n",
    "    reward = sum([r.sum() for r in rewards]) / N\n",
    "\n",
    "    return reward, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Actor\n",
    "\n",
    "We need to learn a policy which, given some state, outputs a distribution over all possible actions. As this is deep RL, we'll use a deep neural network to turn the observed state into the requisite action distribution. From this action distribution we can choose what action to take using `get_action`. Pytorch, brilliant as it is, makes our task incredibly easy, as we can use the `torch.distributions.Categorical` class for sampling.\n",
    "\n",
    "You can experiment with all sorts of network architectures, but remember this is RL, not image classification on ImageNet, so you probably won't need a very deep network (HINT: look below at the state and action dimensionality to get a feel for the task)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(Actor, self).__init__()\n",
    "        \n",
    "        # TODO: Fill in the code to define you policy\n",
    "        \n",
    "        self.net = fd.nets.make_MLP(input_dim=state_dim,\n",
    "                                    output_dim=action_dim, \n",
    "                                    output_nonlin='softmax',\n",
    "                                    hidden_dims=[12, 6], nonlin='elu')\n",
    "        \n",
    "        self.net = nn.Sequential(nn.Linear(state_dim, state_dim*2),\n",
    "                                 nn.ELU(),\n",
    "                                 nn.Linear(state_dim*2, action_dim*2),\n",
    "                                 nn.ELU(),\n",
    "                                 nn.Linear(action_dim*2, action_dim)\n",
    "                                 nn.Softmax()\n",
    "                                )\n",
    "\n",
    "    def forward(self, state):\n",
    "        \n",
    "        # TODO: Fill in the code to run a forward pass of your policy to get a distribution over actions (HINT: probabilities sum to 1)\n",
    "        \n",
    "        return self.net(state)\n",
    "\n",
    "    def get_policy(self, state):\n",
    "        return Categorical(self(state))\n",
    "\n",
    "    def get_action(self, state, greedy=None):\n",
    "        if greedy is None:\n",
    "            greedy = not self.training\n",
    "\n",
    "        policy = self.get_policy(state)\n",
    "        return MLE(policy) if greedy else policy.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The REINFORCE Agent\n",
    "\n",
    "The Actor defines our policy, but we also have to define how and when we'll be updating our policy, which brings us to the agent. The agent will house the policy (an `Actor`), and can then be used to generate rollouts (using `forward()`) or update the policy given a list of rollouts (using `learn()`).\n",
    "\n",
    "The REINFORCE algorithm naively uses the returns directly to weight the gradients, however this makes the variance in the policy gradient estimation very large. As a result, we will use a baseline which is a linear model which takes in a state and outputs the return (sounds like a value function, right?). Except we're not going to train our baseline using gradient descent, instead we'll just solve the linear system analytically in every iteration, and use the solution in the next iteration. Don't worry about training/updating the baseline, but you do have to use it in the right way. (Optional experiment: try removing the baseline and see how performance changes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class REINFORCE(nn.Module):\n",
    "    \n",
    "    def __init__(self, state_dim, action_dim, discount=0.97, lr=1e-3, weight_decay=1e-4):\n",
    "        super(REINFORCE, self).__init__()\n",
    "        self.actor = Actor(state_dim, action_dim)\n",
    "        \n",
    "        self.baseline = nn.Linear(state_dim, 1)\n",
    "        \n",
    "        # TODO: create an optimizer for the parameters of your actor (HINT: use the passed in lr and weight_decay args)\n",
    "        self.optim = optim.RMSprop(self.actor.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        \n",
    "        self.discount = discount\n",
    "        \n",
    "    def forward(self, state):\n",
    "        return self.actor.get_action(state)\n",
    "    \n",
    "    def learn(self, states, actions, rewards):\n",
    "        '''\n",
    "        Takes in three arguments each of which is a list with equal length. Each element in the list is a \n",
    "        pytorch tensor with 1 row for every step in the episode, and the columns are state_dim, action_dim, \n",
    "        and 1, respectively.\n",
    "        '''\n",
    "        \n",
    "        # TODO: implement the REINFORCE algorithm (HINT: check the slides/papers)\n",
    "        \n",
    "        returns = [compute_returns(rs, discount=self.discount) for rs in rewards]\n",
    "        \n",
    "        states, actions, returns = torch.cat(states), torch.cat(actions), torch.cat(returns)\n",
    "        \n",
    "        advantages = returns - self.baseline(states).squeeze()\n",
    "        \n",
    "        pi = self.model.get_policy(states)\n",
    "        \n",
    "        self.optim.zero_grad()\n",
    "        perf = -returns * pi.log_prob(actions)\n",
    "        perf = perf.mean()\n",
    "        perf.backward()\n",
    "        self.optim.step()\n",
    "        \n",
    "        error = F.mse_loss(self.baseline(states).squeeze(), returns).detach()\n",
    "        solve(states, returns, out=self.baseline)\n",
    "        #error = F.mse_loss(self.baseline(states).squeeze(), returns).detach()\n",
    "        \n",
    "        return error.item() # Returns a rough estimate of the error in the baseline (dont worry about this too much)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Critic\n",
    "\n",
    "Now we can introduce a critic, which is essentially a value function to estimate the expected discounted reward of a state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_dim):\n",
    "        super(Critic, self).__init__()\n",
    "        \n",
    "        # TODO: define your value function network\n",
    "        \n",
    "        self.net = fd.nets.make_MLP(input_dim=state_dim,\n",
    "                                    output_dim=1,\n",
    "                                    hidden_dims=[12, 6], nonlin='elu')\n",
    "\n",
    "    def forward(self, state):\n",
    "        \n",
    "        # TODO: apply your value function network to get a value given this batch of states\n",
    "        \n",
    "        return self.net(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The A3C Agent\n",
    "\n",
    "Now we can put the actor and critic together using the A3C algorithm. It turns out, the tasks in the gym are all so simple that there is essentially no gain in parallelization, so technically we're implementing A2C (no async), but the RL part is the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class A3C(REINFORCE):\n",
    "    \n",
    "    def __init__(self, state_dim, action_dim, discount=0.97, lr=1e-3, weight_decay=1e-4):\n",
    "        super(A3C, self).__init__()\n",
    "        self.actor = Actor(state_dim, action_dim)\n",
    "        self.critic = ValueFunction(state_dim)\n",
    "        \n",
    "        # TODO: create an optimizer for the parameters of your actor (HINT: use the passed in lr and weight_decay args)\n",
    "        # (HINT: the actor and critic have different objectives, so how many optimizers do you need?)\n",
    "        self.actor_optim = optim.Adam(self.actor.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        self.critic_optim = optim.Adam(self.critic.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        \n",
    "        self.discount = discount\n",
    "        \n",
    "    def forward(self, state):\n",
    "        return self.actor.get_action(state)\n",
    "    \n",
    "    def learn(self, states, actions, rewards):\n",
    "        \n",
    "        returns = [compute_returns(rs, discount=self.discount) for rs in rewards]\n",
    "        \n",
    "        states, actions, returns = torch.cat(states), torch.cat(actions), torch.cat(returns)\n",
    "        \n",
    "        # TODO: implement A3C (HINT: algorithm details found in A3C paper supplement) \n",
    "        # (HINT2: the algorithm is actually very similar to REINFORCE, the only difference is now we have a critic, what might that do?)\n",
    "        \n",
    "        advantages = returns - self.critic(states).squeeze()\n",
    "        \n",
    "        pi = self.actor.get_policy(states)\n",
    "        \n",
    "        self.actor_optim.zero_grad()\n",
    "        perf = -advantages.detach() * pi.log_prob(actions)\n",
    "        perf = perf.mean()\n",
    "        perf.backward()\n",
    "        self.actor_optim.step()\n",
    "        \n",
    "        self.critic_optim.zero_grad()\n",
    "        loss = advantages.pow(2).mean()\n",
    "        loss.backward()\n",
    "        self.critic_optim.step()\n",
    "        \n",
    "        return loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Balancing a pole with a cart\n",
    "\n",
    "First, we'll test both algorithms on a very simple toy system: the cartpole. Eventhough it's very low dimensional (state=4, action=2), this task is nontrival because it is underactuated. Nevertheless after a few thousand episodes our policy shouldn't have a problem! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimization hyperparameters\n",
    "lr = 1e-3\n",
    "weight_decay = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = 'CartPole-v1' \n",
    "#env_name = 'LunarLander-v2'\n",
    "e = Pytorch_Gym_Env(env_name)\n",
    "state_dim = e.observation_space.shape[0]\n",
    "action_dim = e.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REINFORCE(\n",
      "  (actor): Actor(\n",
      "    (net): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=12, bias=True)\n",
      "      (1): ELU(alpha=1.0, inplace)\n",
      "      (2): Linear(in_features=12, out_features=6, bias=True)\n",
      "      (3): ELU(alpha=1.0, inplace)\n",
      "      (4): Linear(in_features=6, out_features=2, bias=True)\n",
      "      (5): Softmax()\n",
      "    )\n",
      "  )\n",
      "  (baseline): Linear(in_features=4, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Choose what agent to use\n",
    "agent = REINFORCE(state_dim, action_dim, lr=lr, weight_decay=weight_decay)\n",
    "#agent = A3C(state_dim, action_dim, lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "total_episodes = 0\n",
    "print(agent) # Let's take a look at what we're working with..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a \n",
    "gen = Generator(e, agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's do this!!\n",
    "\n",
    "Below is the loop to train and evaluate your agent. You can play around with the number of iterations to run, and the number of rollouts per iteration. \n",
    "\n",
    "You can rerun this cell multiple times to keep training your model for more episodes. In any case, it shouldn't take more than 30 min to an 1 hour to train. (training never took me more than 5 min). HINT: Keep an eye on the eval_reward, it'll be pretty noisy, but if that should be slowly increasing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep:1010: reward=-173.869, loss=-2127.754, eval=-154.177\n",
      "Ep:1020: reward=-152.698, loss=-153.377, eval=-89.262\n",
      "Ep:1030: reward=-135.426, loss=-385.774, eval=-128.973\n",
      "Ep:1040: reward=-169.967, loss=-957.208, eval=-105.427\n",
      "Ep:1050: reward=-186.767, loss=-539.879, eval=-87.073\n",
      "Ep:1060: reward=-187.879, loss=-245.682, eval=-88.195\n",
      "Ep:1070: reward=-171.062, loss=-168.135, eval=-111.922\n",
      "Ep:1080: reward=-160.016, loss=-551.179, eval=-97.515\n",
      "Ep:1090: reward=-144.153, loss=-427.814, eval=-107.051\n",
      "Ep:1100: reward=-154.275, loss=-1910.290, eval=-76.290\n",
      "Ep:1110: reward=-138.869, loss=-270.807, eval=-92.523\n",
      "Ep:1120: reward=-128.868, loss=-1163.767, eval=-103.200\n",
      "Ep:1130: reward=-129.614, loss=-1500.005, eval=-67.393\n",
      "Ep:1140: reward=-141.027, loss=-321.352, eval=-91.707\n",
      "Ep:1150: reward=-155.568, loss=-146.106, eval=-87.661\n",
      "Ep:1160: reward=-158.226, loss=-198.593, eval=-83.321\n",
      "Ep:1170: reward=-156.104, loss=-115.921, eval=-80.870\n",
      "Ep:1180: reward=-137.709, loss=-92.210, eval=-49.774\n",
      "Ep:1190: reward=-141.111, loss=-138.009, eval=-125.595\n",
      "Ep:1200: reward=-138.731, loss=-134.408, eval=-85.735\n",
      "Ep:1210: reward=-166.987, loss=-348.738, eval=-93.202\n",
      "Ep:1220: reward=-152.868, loss=-181.397, eval=-134.762\n",
      "Ep:1230: reward=-177.072, loss=-214.767, eval=-97.433\n",
      "Ep:1240: reward=-138.147, loss=-89.715, eval=-123.741\n",
      "Ep:1250: reward=-153.856, loss=-671.620, eval=-107.031\n",
      "Ep:1260: reward=-167.896, loss=-413.259, eval=-94.195\n",
      "Ep:1270: reward=-98.680, loss=-968.309, eval=-69.369\n",
      "Ep:1280: reward=-181.098, loss=-1764.937, eval=-88.271\n",
      "Ep:1290: reward=-146.997, loss=-476.389, eval=-88.372\n",
      "Ep:1300: reward=-141.025, loss=-207.979, eval=-95.928\n",
      "Ep:1310: reward=-164.514, loss=-654.353, eval=-111.273\n",
      "Ep:1320: reward=-142.448, loss=-138.064, eval=-104.843\n",
      "Ep:1330: reward=-113.380, loss=-205.274, eval=-114.294\n",
      "Ep:1340: reward=-128.136, loss=-66.058, eval=-103.163\n",
      "Ep:1350: reward=-148.435, loss=-375.904, eval=-98.223\n",
      "Ep:1360: reward=-169.207, loss=-207.470, eval=-71.150\n",
      "Ep:1370: reward=-183.735, loss=-270.275, eval=-117.849\n",
      "Ep:1380: reward=-175.069, loss=-714.726, eval=-123.225\n",
      "Ep:1390: reward=-117.143, loss=-202.265, eval=-76.630\n",
      "Ep:1400: reward=-105.172, loss=-11385.067, eval=-95.189\n",
      "Ep:1410: reward=-135.703, loss=-156.194, eval=-44.200\n",
      "Ep:1420: reward=-153.168, loss=-600.994, eval=-107.389\n",
      "Ep:1430: reward=-125.723, loss=-524.457, eval=-123.714\n",
      "Ep:1440: reward=-130.201, loss=-299.488, eval=-54.789\n",
      "Ep:1450: reward=-159.617, loss=-209.075, eval=-98.977\n",
      "Ep:1460: reward=-165.959, loss=-15128.216, eval=-43.694\n",
      "Ep:1470: reward=-114.526, loss=-914.481, eval=-90.273\n",
      "Ep:1480: reward=-196.458, loss=-736.610, eval=-102.214\n",
      "Ep:1490: reward=-139.309, loss=-857.009, eval=-84.997\n",
      "Ep:1500: reward=-228.628, loss=-5136.836, eval=-81.813\n",
      "Ep:1510: reward=-142.570, loss=-985.034, eval=-40.133\n",
      "Ep:1520: reward=-190.396, loss=-802.151, eval=-78.061\n",
      "Ep:1530: reward=-118.155, loss=-481.583, eval=-93.568\n",
      "Ep:1540: reward=-144.937, loss=-143.577, eval=-83.483\n",
      "Ep:1550: reward=-176.097, loss=-501.831, eval=-93.324\n",
      "Ep:1560: reward=-188.809, loss=-498.177, eval=-59.306\n",
      "Ep:1570: reward=-159.029, loss=-211.671, eval=-85.328\n",
      "Ep:1580: reward=-102.359, loss=-852.676, eval=-15.578\n",
      "Ep:1590: reward=-115.225, loss=-277.588, eval=-71.685\n",
      "Ep:1600: reward=-126.395, loss=-412.443, eval=-91.942\n",
      "Ep:1610: reward=-120.768, loss=-2517.688, eval=-52.267\n",
      "Ep:1620: reward=-143.672, loss=-1119.477, eval=-98.501\n",
      "Ep:1630: reward=-150.151, loss=-268.344, eval=-92.761\n",
      "Ep:1640: reward=-123.488, loss=-241.116, eval=-54.427\n",
      "Ep:1650: reward=-142.883, loss=-227.243, eval=-56.555\n",
      "Ep:1660: reward=-197.015, loss=-978.738, eval=-54.709\n",
      "Ep:1670: reward=-125.778, loss=-256.135, eval=-124.157\n",
      "Ep:1680: reward=-122.000, loss=-658.797, eval=-74.469\n",
      "Ep:1690: reward=-116.795, loss=-736.369, eval=12.262\n",
      "Ep:1700: reward=-126.345, loss=-632.430, eval=-40.864\n",
      "Ep:1710: reward=-162.521, loss=-1016.662, eval=-13.218\n",
      "Ep:1720: reward=-88.565, loss=-603.849, eval=-89.884\n",
      "Ep:1730: reward=-170.643, loss=-297.804, eval=-85.946\n",
      "Ep:1740: reward=-97.570, loss=-2338.363, eval=-120.028\n",
      "Ep:1750: reward=-101.167, loss=-398.818, eval=-174.386\n",
      "Ep:1760: reward=-107.845, loss=-281.681, eval=-94.653\n",
      "Ep:1770: reward=-110.982, loss=-116.451, eval=-85.847\n",
      "Ep:1780: reward=-100.758, loss=-170.939, eval=-104.604\n",
      "Ep:1790: reward=-131.352, loss=-102.068, eval=-121.479\n",
      "Ep:1800: reward=-167.535, loss=-974.358, eval=-164.339\n",
      "Ep:1810: reward=-185.337, loss=-599.907, eval=-184.070\n",
      "Ep:1820: reward=-120.512, loss=-826.898, eval=-128.518\n",
      "Ep:1830: reward=-116.503, loss=-164.507, eval=-129.854\n",
      "Ep:1840: reward=-134.967, loss=-349.130, eval=-143.439\n",
      "Ep:1850: reward=-124.599, loss=-250.801, eval=-127.404\n",
      "Ep:1860: reward=-127.379, loss=-415.129, eval=-42.663\n",
      "Ep:1870: reward=-183.677, loss=-610.918, eval=-105.647\n",
      "Ep:1880: reward=-118.967, loss=-792.779, eval=-196.656\n",
      "Ep:1890: reward=-134.639, loss=-2141.255, eval=-260.074\n",
      "Ep:1900: reward=-143.179, loss=-518.202, eval=-175.807\n",
      "Ep:1910: reward=-121.798, loss=-173.559, eval=-100.942\n",
      "Ep:1920: reward=-118.861, loss=-74.156, eval=-291.352\n",
      "Ep:1930: reward=-124.161, loss=-309.252, eval=-352.607\n",
      "Ep:1940: reward=-157.389, loss=-715.144, eval=-218.481\n",
      "Ep:1950: reward=-99.457, loss=-1087.935, eval=-361.670\n",
      "Ep:1960: reward=-152.184, loss=-1393.514, eval=-185.181\n",
      "Ep:1970: reward=-174.911, loss=-611.230, eval=-283.125\n",
      "Ep:1980: reward=-107.443, loss=-3381.223, eval=-86.477\n",
      "Ep:1990: reward=-108.083, loss=-322.781, eval=-212.818\n",
      "Ep:2000: reward=-172.381, loss=-397.032, eval=-307.724\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "num_iter = 100\n",
    "num_train = 10\n",
    "num_eval = 5 # dont change this\n",
    "for itr in range(num_iter):\n",
    "    #agent.model.epsilon = epsilon * epsilon_decay ** (total_episodes / epsilon_decay_episodes)\n",
    "    #print('** Iteration {}/{} **'.format(itr+1, num_iter))\n",
    "    train_reward, train_loss = run_iteration('train', num_train, agent, gen)\n",
    "    eval_reward, _ = run_iteration('eval', num_eval, agent, gen)\n",
    "    total_episodes += num_train\n",
    "    print('Ep:{}: reward={:.3f}, loss={:.3f}, eval={:.3f}'.format(total_episodes, train_reward, train_loss, eval_reward))\n",
    "    \n",
    "    if eval_reward > 450 and env_name == 'CartPole-v1': # dont change this\n",
    "        print('Success!!! You have solved cartpole task! Time for a bigger challenge!')\n",
    "    \n",
    "    # save model\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-1102.3806), None)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You can visualize your policy at any time\n",
    "run_iteration('eval', 1, agent, gen, render=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Analysis\n",
    "\n",
    "Plot the performance of each of your agents for the cartpole task and one additional task. When choosing a new environment, make sure is has a discrete action space. For each plot the x axis should show the total number of episodes the model was trained on, and the y axis shows the average total reward per episode.\n",
    "\n",
    "You can leave the plots as cell outputs below, or you can save them as images and submit them separately.\n",
    "\n",
    "### Deliverables\n",
    "- single plot showing both the REINFORCE algorithm's performance, and A3C's performance on the same plot for the cartpole environment (CartPole-v1).\n",
    "- single plot showing both the REINFORCE algorithm's performance, and A3C's performance on the same plot for a second environment of your choice (suggested -> LunarLander-v2, it's a little tricky but watching the agent fly spaceships is very entertaining!).\n",
    "- in every case you models have to learn something for full credit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
