{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# install\n",
    "# pytorch - pip install torch\n",
    "# gym - pip install gym\n",
    "\n",
    "# gym[atari] - pip install gym[atari]\n",
    "\n",
    "from collections import deque\n",
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.multiprocessing as mp\n",
    "from torch import distributions\n",
    "from torch.distributions import Categorical\n",
    "import foundation as fd\n",
    "import foundation.util as util\n",
    "from itertools import islice\n",
    "from tabulate import tabulate\n",
    "\n",
    "import gym\n",
    "\n",
    "from light import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "args = util.NS()\n",
    "\n",
    "args.name = 'test-a3c'\n",
    "args.save_dir = 'results'\n",
    "\n",
    "args.num_iter = 100\n",
    "args.num_train = 10\n",
    "args.num_eval = 5\n",
    "args.logdate = True\n",
    "args.tblog = True\n",
    "args.txtlog = False\n",
    "args.small_print = True\n",
    "\n",
    "args.agent = 'ppo'#'vpg'#'ddpg'#'vpg'\n",
    "args.env = 'InvertedPendulum-v2'#'CartPole-v1' # 'LunarLander-v2'\n",
    "\n",
    "args.policy = 'full'\n",
    "args.hidden_dims = []#[6,4]\n",
    "args.nonlin = 'prelu'\n",
    "args.discount = 0.99\n",
    "args.epsilon = 0.01\n",
    "args.tau = 0.001\n",
    "args.use_replica = True\n",
    "args.actor_steps = 1\n",
    "\n",
    "args.critic_hidden_dims = [4,4]\n",
    "args.critic_nonlin = 'prelu'\n",
    "\n",
    "args.buffer_max_episodes = 20\n",
    "args.buffer_min_start = 1000\n",
    "args.buffer_batch_size = 128\n",
    "\n",
    "args.cpi_clip = 0.3\n",
    "args.target_kl = None\n",
    "args.kl_weight = 1.\n",
    "args.agent_epochs = 5\n",
    "args.agent_batch_size = 32\n",
    "\n",
    "args.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "args.optim = 'adam'\n",
    "args.lr = 1e-4\n",
    "args.weight_decay = 1e-4\n",
    "\n",
    "print('Using device: {}'.format(args.device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving to results/test-a3c/Dec-14-2018-012718\n",
      "Env: Pendulum-v0 - state-dim=3, action-dim=1 (continuous)\n"
     ]
    }
   ],
   "source": [
    "now = time.strftime(\"%b-%d-%Y-%H%M%S\")\n",
    "args.save = os.path.join(args.save_dir, args.name, now if args.logdate else '')\n",
    "util.create_dir(args.save)\n",
    "logger = util.Logger(args.save, tensorboard=args.tblog, txt=args.txtlog)\n",
    "print('Saving to {}'.format(args.save))\n",
    "\n",
    "env = utils.Pytorch_Gym_Env(args.env, device=args.device)\n",
    "args.state_dim = env.observation_space.shape[0]\n",
    "args.discrete_action_space = isinstance(env.action_space, gym.spaces.Discrete)\n",
    "args.action_dim = env.action_space.n if args.discrete_action_space else env.action_space.shape[0]\n",
    "print('Env: {} - state-dim={}, action-dim={} ({})'.format(args.env, args.state_dim, args.action_dim, 'discrete' if args.discrete_action_space else 'continuous'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "REINFORCE(\n",
       "  (policy): Full_Gaussian_Policy(\n",
       "    (net): Sequential(\n",
       "      (0): Linear(in_features=3, out_features=2, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (baseline): Linear(in_features=3, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if args.agent == 'dqn':\n",
    "    policy = policies.ActionOut_QFunction(args.state_dim, args.action_dim, hidden_dims=args.hidden_dims, nonlin=args.nonlin,\n",
    "                                          epsilon=args.epsilon)\n",
    "elif args.discrete_action_space:\n",
    "    policy = policies.Discrete_Policy(args.state_dim, args.action_dim, hidden_dims=args.hidden_dims, nonlin=args.nonlin)\n",
    "elif args.policy == 'full':\n",
    "    policy = policies.Full_Gaussian_Policy(args.state_dim, args.action_dim, hidden_dims=args.hidden_dims, nonlin=args.nonlin)\n",
    "else:\n",
    "    policy = policies.Gaussian_Policy(args.state_dim, args.action_dim, hidden_dims=args.hidden_dims, nonlin=args.nonlin)\n",
    "    \n",
    "if args.agent == 'vpg':\n",
    "    agent = agents.REINFORCE(policy, discount=args.discount, \n",
    "                             optim_type=args.optim, lr=args.lr, weight_decay=args.weight_decay)\n",
    "elif args.agent == 'ppo':\n",
    "    agent = agents.PPO(policy, discount=args.discount, \n",
    "                       optim_type=args.optim, lr=args.lr, weight_decay=args.weight_decay,\n",
    "                       clipping=args.cpi_clip, target_kl=args.target_kl, kl_weight=args.kl_weight, \n",
    "                       epochs=args.agent_epochs, batch_size=args.agent_batch_size, )\n",
    "elif args.agent == 'a3c':\n",
    "    value_fn = policies.ValueFunction(args.state_dim, hidden_dims=args.critic_hidden_dims, nonlin=args.critic_nonlin)\n",
    "    agent = agents.A3C(actor=policy, critic=value_fn, discount=args.discount, \n",
    "                       optim_type=args.optim, lr=args.lr, weight_decay=args.weight_decay)\n",
    "elif args.agent == 'dqn':\n",
    "    buffer = utils.Replay_Buffer(max_transition_size=args.buffer_max_transitions, device=args.device)\n",
    "    agent = agents.DQN(policy, discount=args.discount, buffer=buffer, batch_size=args.buffer_batch_size, \n",
    "                       min_buffer_size=args.buffer_min_start, tau=args.tau, use_replica=args.use_replica,\n",
    "                       optim_type=args.optim, lr=args.lr, weight_decay=args.weight_decay)\n",
    "elif args.agent == 'ddpg':\n",
    "    assert not args.discrete_action_space\n",
    "    qnet = policies.QFunction(args.state_dim, args.action_dim, hidden_dims=args.critic_hidden_dims, nonlin=args.critic_nonlin)\n",
    "    policy = policies.ActorCritic(policy, qnet)\n",
    "    buffer = utils.Replay_Buffer(max_episode_size=args.buffer_max_episodes, device=args.device)\n",
    "    agent = agents.DDPG(policy, discount=args.discount, actor_steps=args.actor_steps, buffer=buffer, \n",
    "                        min_buffer_size=args.buffer_min_start, batch_size=args.buffer_batch_size, tau=args.tau,\n",
    "                        optim_type=args.optim, lr=args.lr, weight_decay=args.weight_decay)\n",
    "else:\n",
    "    raise Exception('Unknown agent: {}'.format(args.agent))\n",
    "    \n",
    "agent.to(args.device)\n",
    "gen = utils.Generator(env, agent)\n",
    "score = utils.Score(tau=0.01)\n",
    "stats = util.StatsMeter('score', 'rewards-train', 'rewards-eval')\n",
    "stats.shallow_join(agent.stats)\n",
    "total_episodes = 0\n",
    "agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 10: train=-1300.205, eval=-1255.676, score=-1027.552\n",
      "Ep 20: train=-1134.677, eval=-1419.308, score=-1046.678\n",
      "Ep 30: train=-1216.518, eval=-1290.932, score=-1058.646\n",
      "Ep 40: train=-1131.098, eval=-1218.439, score=-1066.419\n",
      "Ep 50: train=-1275.410, eval=-1422.644, score=-1084.021\n",
      "Ep 60: train=-1136.201, eval=-1467.669, score=-1102.764\n",
      "Ep 70: train=-1323.634, eval=-1153.983, score=-1105.246\n",
      "Ep 80: train=-1220.293, eval=-1221.794, score=-1110.921\n",
      "Ep 90: train=-1133.004, eval=-1268.235, score=-1118.750\n",
      "Ep 100: train=-1208.555, eval=-1235.151, score=-1124.492\n",
      "Ep 110: train=-1155.352, eval=-1178.252, score=-1127.134\n",
      "Ep 120: train=-1225.893, eval=-1156.927, score=-1128.540\n",
      "Ep 130: train=-1374.182, eval=-1473.363, score=-1145.452\n",
      "Ep 140: train=-1336.741, eval=-1311.535, score=-1153.649\n",
      "Ep 150: train=-1183.891, eval=-1273.542, score=-1159.605\n",
      "Ep 160: train=-1249.083, eval=-1204.392, score=-1161.794\n",
      "Ep 170: train=-1157.868, eval=-1325.797, score=-1169.764\n",
      "Ep 180: train=-1365.605, eval=-1241.430, score=-1173.188\n",
      "Ep 190: train=-1262.388, eval=-1260.322, score=-1177.431\n",
      "Ep 200: train=-1210.891, eval=-1176.437, score=-1177.363\n",
      "Ep 210: train=-1177.268, eval=-1244.876, score=-1180.573\n",
      "Ep 220: train=-1158.731, eval=-1537.459, score=-1197.978\n",
      "Ep 230: train=-1155.912, eval=-1436.762, score=-1209.769\n",
      "Ep 240: train=-1096.480, eval=-1162.217, score=-1207.430\n",
      "Ep 250: train=-1311.425, eval=-1189.659, score=-1206.542\n",
      "Ep 260: train=-1161.863, eval=-1122.818, score=-1202.444\n",
      "Ep 270: train=-1186.905, eval=-1155.556, score=-1200.157\n",
      "Ep 280: train=-1151.234, eval=-1152.700, score=-1197.833\n",
      "Ep 290: train=-1143.581, eval=-1255.780, score=-1200.656\n",
      "Ep 300: train=-1206.014, eval=-1392.123, score=-1210.107\n",
      "Ep 310: train=-1137.437, eval=-1320.063, score=-1215.556\n",
      "Ep 320: train=-1161.309, eval=-1256.728, score=-1217.575\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-657da33c5429>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m#agent.model.epsilon = epsilon * epsilon_decay ** (total_episodes / epsilon_decay_episodes)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;31m#print('** Iteration {}/{} **'.format(itr+1, num_iter))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mtrain_rewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0meval_rewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'eval'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_eval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mtotal_episodes\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/c/Users/anwan/OneDrive - UW/Khan/AIResearch/foundation/rl/light/utils.py\u001b[0m in \u001b[0;36mrun_iteration\u001b[0;34m(mode, N, agent, gen, horizon, render)\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0mlearn_stats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m                 \u001b[0mlearn_stats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0mrewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/c/Users/anwan/OneDrive - UW/Khan/AIResearch/foundation/rl/light/agents.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, states, actions, rewards)\u001b[0m\n\u001b[1;32m     43\u001b[0m                 \u001b[0mperf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0madvantages\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mpi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m                 \u001b[0mperf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mperf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m                 \u001b[0mperf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/p36/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \"\"\"\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/p36/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for itr in range(args.num_iter):\n",
    "    #agent.model.epsilon = epsilon * epsilon_decay ** (total_episodes / epsilon_decay_episodes)\n",
    "    #print('** Iteration {}/{} **'.format(itr+1, num_iter))\n",
    "    train_rewards = utils.run_iteration('train', args.num_train, agent, gen)\n",
    "    eval_rewards = utils.run_iteration('eval', args.num_eval, agent, gen)\n",
    "    total_episodes += args.num_train\n",
    "    score.update_all(eval_rewards)\n",
    "    stats.update('score', score.val)\n",
    "    stats.update('rewards-train', train_rewards.mean())\n",
    "    stats.update('rewards-eval', eval_rewards.mean())\n",
    "    \n",
    "    vals = stats.vals()\n",
    "    logger.update(vals, step=total_episodes)\n",
    "    if args.small_print:\n",
    "        print('Ep {}: train={:.3f}, eval={:.3f}, score={:.3f}'.format(total_episodes, vals['rewards-train'], vals['rewards-eval'], vals['score']))\n",
    "    else:\n",
    "        print('Episode: {}'.format(total_episodes))\n",
    "        print(tabulate(vals))\n",
    "    \n",
    "    # save model\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1722.5884], device='cuda:0')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utils.run_iteration('eval', 1, agent, gen, render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.train()\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.2589], device='cuda:0')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.2463], device='cuda:0', grad_fn=<ExpandBackward>),\n",
       " tensor([0.9736], device='cuda:0', grad_fn=<ExpandBackward>))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = agent.actor.get_pi(s)\n",
    "d.loc, d.scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.3435], device='cuda:0')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created iterator\n",
      "0.15731143951416016\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([191, 4]), torch.Size([191]), torch.Size([191]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = time.time()\n",
    "out = islice(g, 10)\n",
    "states, actions, rewards = map(torch.cat, zip(*out))\n",
    "print(time.time() - start)\n",
    "states.size(), actions.size(), rewards.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\n"
     ]
    }
   ],
   "source": [
    "# define hyperparameters\n",
    "env = Pytorch_Gym_Env('CartPole-v1')\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DQN(\n",
       "  (model): QNet(\n",
       "    (net): Sequential(\n",
       "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
       "      (1): LeakyReLU(negative_slope=0.01)\n",
       "      (2): Linear(in_features=8, out_features=2, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (target_model): QNet(\n",
       "    (net): Sequential(\n",
       "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
       "      (1): LeakyReLU(negative_slope=0.01)\n",
       "      (2): Linear(in_features=8, out_features=2, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (criterion): SmoothL1Loss()\n",
       ")"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#agent = DDPG(state_dim, action_dim, max_buffer_size=1000, min_buffer_size=50)\n",
    "total_episodes = 0\n",
    "epsilon = 0.01\n",
    "agent = DQN(state_dim, action_dim, \n",
    "            max_buffer_size=1000, min_buffer_size=200, batch_size=128, use_replica=False,\n",
    "            lr=1e-3, tau=0.001, weight_decay=1e-3, epsilon=epsilon)\n",
    "agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create objects\n",
    "# if True:\n",
    "#     device = 'cuda'\n",
    "#     agent.to(device)\n",
    "#     env.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon_decay_episodes = 1000\n",
    "epsilon_decay = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep:50: reward=9.800, loss=0.075, eval=23.400\n",
      "Ep:100: reward=12.160, loss=0.082, eval=14.400\n",
      "Ep:150: reward=24.320, loss=0.047, eval=9.800\n",
      "Ep:200: reward=24.220, loss=0.033, eval=42.600\n",
      "Ep:250: reward=39.800, loss=0.015, eval=34.400\n",
      "Ep:300: reward=30.880, loss=0.016, eval=17.800\n",
      "Ep:350: reward=27.180, loss=0.017, eval=75.000\n",
      "Ep:400: reward=28.700, loss=0.021, eval=17.000\n",
      "Ep:450: reward=33.120, loss=0.016, eval=14.200\n",
      "Ep:500: reward=34.120, loss=0.013, eval=57.600\n",
      "Ep:550: reward=50.880, loss=0.011, eval=68.400\n",
      "Ep:600: reward=41.140, loss=0.013, eval=10.000\n",
      "Ep:650: reward=38.120, loss=0.014, eval=43.400\n",
      "Ep:700: reward=46.040, loss=0.013, eval=34.400\n",
      "Ep:750: reward=51.880, loss=0.013, eval=11.800\n",
      "Ep:800: reward=51.860, loss=0.011, eval=93.800\n",
      "Ep:850: reward=40.580, loss=0.012, eval=57.200\n"
     ]
    }
   ],
   "source": [
    "num_iter = 20\n",
    "num_train = 50\n",
    "num_eval = 5\n",
    "for itr in range(num_iter):\n",
    "    #agent.model.epsilon = epsilon * epsilon_decay ** (total_episodes / epsilon_decay_episodes)\n",
    "    #print('** Iteration {}/{} **'.format(itr+1, num_iter))\n",
    "    train_reward, train_loss = run_episodes('train', num_train, agent, env)\n",
    "    eval_reward, _ = run_episodes('eval', num_eval, agent, env)\n",
    "    total_episodes += num_train\n",
    "    print('Ep:{}: reward={:.3f}, loss={:.3f}, eval={:.3f}'.format(total_episodes, train_reward, train_loss, eval_reward))\n",
    "    \n",
    "    # save model\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.critic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75.9"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_iteration('eval', 1, agent, gen, render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N, D, M = 10, 4, 1\n",
    "\n",
    "f = nn.Linear(D, M)\n",
    "x = torch.randn(N,D)\n",
    "y = f(x).detach()\n",
    "g = util.solve(x,y)\n",
    "g.weight.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env = gym.make('Pong-v4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(210, 160, 3)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env = gym.make('LunarLander-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.00551643,  1.4003555 , -0.55877346, -0.46954992,  0.00639898,\n",
       "        0.12657054,  0.        ,  0.        ], dtype=float32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([192,   0,   0,   0, 110,  38,   0,   7,  63,   1,  60,  59,   0,\n",
       "         0,   0,  62, 255,   0, 255, 253,   0,   8,   0,  24, 128,  32,\n",
       "         1,  86, 247,  86, 247,  86, 247, 134, 243, 245, 243, 240, 240,\n",
       "       242, 242,  32,  32,  64,  64,  64, 188,  65, 189,   0,   8, 109,\n",
       "        37,  37,  60,   0,   0,   0,   0, 109, 109,  37,  37, 192, 192,\n",
       "       192, 192,   1, 192, 202, 247, 202, 247, 202, 247, 202, 247,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,  54, 236, 242, 121, 240], dtype=uint8)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "for _ in range(1000):\n",
    "    env.render()\n",
    "    _,_,done,_ = env.step(env.action_space.sample())\n",
    "    plt.pause(0.02)\n",
    "    if done:\n",
    "        print('stop')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
