{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import time\n",
    "import numpy as np\n",
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "import matplotlib.pyplot as plt\n",
    "import configargparse\n",
    "from tabulate import tabulate\n",
    "import torch.multiprocessing as mp\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from dm_control import suite\n",
    "from dm_control import viewer\n",
    "\n",
    "import foundation as fd\n",
    "from foundation import util\n",
    "from foundation import models\n",
    "from foundation import rl\n",
    "from foundation import envs\n",
    "from foundation import train\n",
    "\n",
    "from rlhw_backend import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 acrobot swingup\n",
      "1 acrobot swingup_sparse\n",
      "2 ball_in_cup catch\n",
      "3 cartpole balance\n",
      "4 cartpole balance_sparse\n",
      "5 cartpole swingup\n",
      "6 cartpole swingup_sparse\n",
      "7 cartpole two_poles\n",
      "8 cartpole three_poles\n",
      "9 cheetah run\n",
      "10 finger spin\n",
      "11 finger turn_easy\n",
      "12 finger turn_hard\n",
      "13 fish upright\n",
      "14 fish swim\n",
      "15 hopper stand\n",
      "16 hopper hop\n",
      "17 humanoid stand\n",
      "18 humanoid walk\n",
      "19 humanoid run\n",
      "20 humanoid run_pure_state\n",
      "21 humanoid_CMU stand\n",
      "22 humanoid_CMU run\n",
      "23 lqr lqr_2_1\n",
      "24 lqr lqr_6_2\n",
      "25 manipulator bring_ball\n",
      "26 manipulator bring_peg\n",
      "27 manipulator insert_ball\n",
      "28 manipulator insert_peg\n",
      "29 pendulum swingup\n",
      "30 point_mass easy\n",
      "31 point_mass hard\n",
      "32 reacher easy\n",
      "33 reacher hard\n",
      "34 stacker stack_2\n",
      "35 stacker stack_4\n",
      "36 swimmer swimmer6\n",
      "37 swimmer swimmer15\n",
      "38 walker stand\n",
      "39 walker walk\n",
      "40 walker run\n"
     ]
    }
   ],
   "source": [
    "for i, task in enumerate(suite.ALL_TASKS):\n",
    "    print(i,*task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['config', 'name', 'save_root', 'log_date', 'log_tb', 'log_txt', 'save_freq', 'agent', 'clip', 'policy', 'model', 'baseline', 'env', 'device', 'seed', 'budget_steps', 'steps_per_itr', 'tau', 'epochs', 'batch_size', 'norm_adv', 'optim_type', 'lr', 'weight_decay', 'momentum', 'step_size', 'discount', 'subsample', 'gae_lambda', 'nonlin', 'hidden', 'min_log_std', 'b_hidden', 'b_scale_max', 'b_epochs', 'b_batch_size', 'b_nonlin', 'b_optim_type', 'b_lr', 'b_weight_decay', 'b_momentum', 'b_nesterov', 'b_time_order', 'b_obs_order'])\n"
     ]
    }
   ],
   "source": [
    "parser = train.setup_rl_options()\n",
    "args = parser.parse_args(['--config', '../config/ppo.yaml'])\n",
    "print(args.__dict__.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manually changing args\n",
    "\n",
    "args.name = 'test-ppo-nb-cp'\n",
    "\n",
    "args.log_tb = True\n",
    "\n",
    "args.env = suite.ALL_TASKS[3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save dir: results/test-ppo-nb-cp\\19-03-09-020705\n",
      "Logging/Saving in results/test-ppo-nb-cp\\19-03-09-020705 (tb=True,txt=False)\n",
      "Generating random seed: -1702636919\n"
     ]
    }
   ],
   "source": [
    "now = time.strftime(\"%y-%m-%d-%H%M%S\")\n",
    "if args.log_date:\n",
    "    args.name = os.path.join(args.name, now)\n",
    "args.save_dir = os.path.join(args.save_root, args.name)\n",
    "print('Save dir: {}'.format(args.save_dir))\n",
    "if args.log_tb or args.log_txt or args.save_freq is not None:\n",
    "    util.create_dir(args.save_dir)\n",
    "    print('Logging/Saving in {} (tb={},txt={})'.format(args.save_dir, args.log_tb, args.log_txt))\n",
    "logger = util.Logger(args.save_dir, tensorboard=args.log_tb, txt=args.log_txt)\n",
    "\n",
    "if args.seed is None:\n",
    "    args.seed = util.get_random_seed()\n",
    "    print('Generating random seed: {}'.format(args.seed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu\n",
      "Env name=<dm_control.suite.cartpole.Balance object at 0x0000018315E429E8> (obs=5, act=1)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(args.seed)\n",
    "print('Using {}'.format(args.device))\n",
    "\n",
    "env = envs.Pytorch_DMC_Env(*args.env, seed=args.seed, device=args.device)\n",
    "\n",
    "args.state_dim, args.action_dim = env.obs_dim, env.act_dim\n",
    "print('Env name={} (obs={}, act={})'.format(env._env.task, args.state_dim, args.action_dim))\n",
    "\n",
    "n_batch = args.budget_steps / args.steps_per_itr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'mlp' in args.baseline:\n",
    "\n",
    "    baseline_model = NormalizedMLP(args.state_dim, 1, norm='norm' in args.baseline,\n",
    "                                   hidden_dims=args.b_hidden, nonlin=args.b_nonlin)\n",
    "\n",
    "    baseline_model.optim = util.get_optimizer(args.b_optim_type, baseline_model.parameters(), lr=args.b_lr, weight_decay=args.b_weight_decay)\n",
    "    baseline_model.scheduler = torch.optim.lr_scheduler.LambdaLR(\n",
    "                baseline_model.optim, lambda x: (n_batch - x) / n_batch, -1)\n",
    "\n",
    "    # print(baseline_model.optim)\n",
    "    # quit()\n",
    "\n",
    "    #assert args.baseline == 'norm-mlp'\n",
    "    baseline = rl.Deep_Baseline(baseline_model, scale_max=args.b_scale_max,\n",
    "                        batch_size=args.b_batch_size, epochs_per_step=args.b_epochs, )\n",
    "\n",
    "elif args.baseline == 'lin':\n",
    "    baseline = rl.Linear_Baseline(state_dim=args.state_dim, value_dim=1)\n",
    "else:\n",
    "    raise Exception('unknown baseline: {}'.format(args.baseline))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PPOClip(\n",
      "  (policy): NormalPolicy(\n",
      "    (model): NormalizedMLP(\n",
      "      (criterion): MSELoss()\n",
      "      (norm): RunningNormalization()\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=5, out_features=8, bias=True)\n",
      "        (1): PReLU(num_parameters=1)\n",
      "        (2): Linear(in_features=8, out_features=8, bias=True)\n",
      "        (3): PReLU(num_parameters=1)\n",
      "        (4): Linear(in_features=8, out_features=2, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (baseline): Deep_Baseline(\n",
      "    (model): NormalizedMLP(\n",
      "      (criterion): MSELoss()\n",
      "      (norm): RunningNormalization()\n",
      "      (net): Sequential(\n",
      "        (0): Linear(in_features=5, out_features=8, bias=True)\n",
      "        (1): PReLU(num_parameters=1)\n",
      "        (2): Linear(in_features=8, out_features=8, bias=True)\n",
      "        (3): PReLU(num_parameters=1)\n",
      "        (4): Linear(in_features=8, out_features=1, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Agent has 271 parameters\n"
     ]
    }
   ],
   "source": [
    "assert args.policy == 'normal'\n",
    "assert args.model == 'norm-mlp'\n",
    "policy_model = NormalizedMLP(args.state_dim, 2 * args.action_dim, hidden_dims=args.hidden, nonlin=args.nonlin)\n",
    "policy = rl.NormalPolicy(policy_model, )\n",
    "\n",
    "assert args.agent == 'ppoclip'\n",
    "\n",
    "agent = rl.PPOClip(policy=policy, baseline=baseline, clip=args.clip, normalize_adv=args.norm_adv,\n",
    "            optim_type=args.optim_type, lr=args.lr, scheduler_lin=n_batch, weight_decay=args.b_weight_decay,\n",
    "            batch_size=args.batch_size, epochs_per_step=args.epochs,\n",
    "            ).to(args.device)\n",
    "\n",
    "print(agent)\n",
    "print('Agent has {} parameters'.format(util.count_parameters(agent)))\n",
    "\n",
    "gen = fd.data.Generator(env, agent, step_limit=args.budget_steps,\n",
    "                step_threshold=args.steps_per_itr, drop_last_state=True)\n",
    "\n",
    "itr = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\anwan\\onedrive - uw\\khan\\airesearch\\foundation\\foundation\\util\\stats.py:288: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.val = torch.tensor(val).float()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 03-09-19 02:12:12 ] 42000/1000000 (ep=42) : last=273.001 max=401.993 - 377.244 \n",
      "[ 03-09-19 02:12:21 ] 45000/1000000 (ep=45) : last=199.005 max=401.993 - 343.674 \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-f958bcb4fac7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m stats = train.run_rl_training(gen, agent, args=args, logger=logger, save_freq=None, \n\u001b[1;32m----> 2\u001b[1;33m                               num_iter=N, continue_gen_stats=True)\n\u001b[0m\u001b[0;32m      3\u001b[0m path = train.save_checkpoint({\n\u001b[0;32m      4\u001b[0m             \u001b[1;34m'agent_state_dict'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m             \u001b[1;34m'stats'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstats\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\anwan\\onedrive - uw\\khan\\airesearch\\foundation\\foundation\\train\\running.py\u001b[0m in \u001b[0;36mrun_rl_training\u001b[1;34m(gen, agent, args, logger, stats, tau, save_freq, print_freq, num_iter, continue_gen_stats)\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m                 \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m                 \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mrollouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m                 \u001b[0mstats\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'time-learn'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\anwan\\onedrive - uw\\khan\\airesearch\\foundation\\foundation\\rl\\agents\\mixins.py\u001b[0m in \u001b[0;36mlearn\u001b[1;34m(self, **paths)\u001b[0m\n\u001b[0;32m     37\u001b[0m                         \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpaths\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'rewards'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbaseline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpaths\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'states'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mpaths\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\anwan\\onedrive - uw\\khan\\airesearch\\foundation\\foundation\\rl\\baselines.py\u001b[0m in \u001b[0;36mlearn\u001b[1;34m(self, states, values)\u001b[0m\n\u001b[0;32m     25\u001b[0m                         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscale\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m                         \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalues\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m                 \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\anwan\\onedrive - uw\\khan\\airesearch\\foundation\\foundation\\framework.py\u001b[0m in \u001b[0;36mlearn\u001b[1;34m(self, states, values)\u001b[0m\n\u001b[0;32m    227\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    228\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mlearn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstates\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 229\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    230\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    231\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\anwan\\onedrive - uw\\khan\\airesearch\\foundation\\foundation\\rl\\baselines.py\u001b[0m in \u001b[0;36m_update\u001b[1;34m(self, states, values)\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 84\u001b[1;33m                         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_epoch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstats\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'error-after'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\anwan\\onedrive - uw\\khan\\airesearch\\foundation\\foundation\\framework.py\u001b[0m in \u001b[0;36mtrain_epoch\u001b[1;34m(self, loader)\u001b[0m\n\u001b[0;32m     72\u001b[0m                         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m                         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstats\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'loss'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mget_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\anwan\\onedrive - uw\\khan\\airesearch\\foundation\\foundation\\util\\stats.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, name, value, n)\u001b[0m\n\u001b[0;32m    178\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    179\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 180\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stats\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    181\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    182\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0m__add__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\anwan\\onedrive - uw\\khan\\airesearch\\foundation\\foundation\\util\\stats.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, val, n)\u001b[0m\n\u001b[0;32m    286\u001b[0m                 \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    287\u001b[0m                         \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 288\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    289\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mval\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    290\u001b[0m                 \u001b[0mprev_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "stats = train.run_rl_training(gen, agent, args=args, logger=logger, save_freq=None, \n",
    "                              num_iter=N, continue_gen_stats=True)\n",
    "path = train.save_checkpoint({\n",
    "            'agent_state_dict': agent.state_dict(),\n",
    "            'stats': stats,\n",
    "            'args': args,\n",
    "            'steps': gen.steps_generated(),\n",
    "            'episodes': gen.episodes_generated(),\n",
    "        }, args.save_dir, epoch=itr)\n",
    "itr += N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.view(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
