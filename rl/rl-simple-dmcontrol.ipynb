{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# install\n",
    "# pytorch - pip install torch\n",
    "# gym - pip install gym\n",
    "\n",
    "# gym[atari] - pip install gym[atari]\n",
    "\n",
    "from collections import deque\n",
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.multiprocessing as mp\n",
    "from torch import distributions\n",
    "from torch.distributions import Categorical\n",
    "import foundation as fd\n",
    "from foundation import util\n",
    "from itertools import islice\n",
    "from tabulate import tabulate\n",
    "\n",
    "import zmq\n",
    "import subprocess\n",
    "\n",
    "#import gym\n",
    "from dm_control import suite\n",
    "from dm_control import viewer\n",
    "\n",
    "from light import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'torch.utils.data' from '/home/fleeb/anaconda3/lib/python3.5/site-packages/torch/utils/data/__init__.py'>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.utils.data.dataloader.default_collate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class A(object):\n",
    "    def __init__(self, x=None):\n",
    "        self.x = x\n",
    "        self.n = 'a'\n",
    "    def f(self, y=None):\n",
    "        print('a', self.n, self.x, y)\n",
    "\n",
    "class B(A):\n",
    "    def __init__(self, x=None):\n",
    "        self.x = x\n",
    "        self.n = 'b'\n",
    "    def f(self, y=None):\n",
    "        super().f(y)\n",
    "        print('b', self.n, self.x, y)\n",
    "\n",
    "class C(A):\n",
    "    def __init__(self, x=None):\n",
    "        self.x = x\n",
    "        self.n = 'c'\n",
    "    def f(self, y=None):\n",
    "        super().f(y)\n",
    "        print('c', self.n, self.x, y)\n",
    "\n",
    "class D(B,C):\n",
    "    def __init__(self, x=None):\n",
    "        self.x = x\n",
    "        self.n = 'd'\n",
    "    def f(self, y=None):\n",
    "        super().f(y)\n",
    "        print('d', self.n, self.x, y)\n",
    "\n",
    "    \n",
    "d = D()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a d None None\n",
      "c d None None\n",
      "b d None None\n",
      "d d None None\n"
     ]
    }
   ],
   "source": [
    "d.f()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(__main__.D, __main__.B, __main__.C, __main__.A, object)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.__class__.__mro__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cartpole balance\n"
     ]
    }
   ],
   "source": [
    "task = tasks[3]\n",
    "print(*task)\n",
    "env = suite.load(*task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "args = util.NS()\n",
    "\n",
    "args.name = 'test-ppo-dmc'\n",
    "args.save_dir = 'results'\n",
    "\n",
    "args.num_iter = 10\n",
    "args.num_train = 1#10\n",
    "args.num_eval = 2#5\n",
    "args.logdate = True\n",
    "args.tblog = True\n",
    "args.txtlog = False\n",
    "args.small_print = True\n",
    "\n",
    "args.agent = 'ppo'#'ddpg'#'vpg'\n",
    "args.task = tasks[3]\n",
    "\n",
    "args.policy = 'full'\n",
    "args.hidden_dims = [8,8]\n",
    "args.nonlin = 'prelu'\n",
    "args.discount = 0.99\n",
    "args.epsilon = 0.01\n",
    "args.tau = 0.001\n",
    "args.use_replica = True\n",
    "args.actor_steps = 1\n",
    "\n",
    "args.critic_hidden_dims = [8,8]\n",
    "args.critic_nonlin = 'prelu'\n",
    "\n",
    "args.buffer_max_episodes = 20\n",
    "args.buffer_min_start = 1000\n",
    "args.buffer_batch_size = 128\n",
    "\n",
    "args.cpi_clip = 0.3\n",
    "args.target_kl = None\n",
    "args.kl_weight = 1.\n",
    "args.agent_epochs = 5\n",
    "args.agent_batch_size = 32\n",
    "\n",
    "args.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "args.optim = 'adam'\n",
    "args.lr = 1e-4\n",
    "args.weight_decay = 1e-4\n",
    "\n",
    "print('Using device: {}'.format(args.device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving to results/test-ppo-dmc/Mar-04-2019-171313\n",
      "Task: cartpole balance\n",
      "Env: cartpole-balance - state-dim=5, action-dim=1\n"
     ]
    }
   ],
   "source": [
    "now = time.strftime(\"%b-%d-%Y-%H%M%S\")\n",
    "args.save = os.path.join(args.save_dir, args.name, now if args.logdate else '')\n",
    "util.create_dir(args.save)\n",
    "logger = util.Logger(args.save, tensorboard=args.tblog, txt=args.txtlog)\n",
    "print('Saving to {}'.format(args.save))\n",
    "\n",
    "env = utils.Pytorch_DMC_Env(*args.task, device=args.device)\n",
    "env.max_steps = 1000\n",
    "args.discrete_action_space = False\n",
    "print('Task:',*args.task)\n",
    "args.state_dim, args.action_dim = env.obs_dim, env.act_dim\n",
    "print('Env: {}-{} - state-dim={}, action-dim={}'.format(args.task[0], args.task[1], args.state_dim, args.action_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PPO(\n",
       "  (policy): Full_Gaussian_Policy(\n",
       "    (net): Sequential(\n",
       "      (0): Linear(in_features=5, out_features=8, bias=True)\n",
       "      (1): PReLU(num_parameters=1)\n",
       "      (2): Linear(in_features=8, out_features=8, bias=True)\n",
       "      (3): PReLU(num_parameters=1)\n",
       "      (4): Linear(in_features=8, out_features=2, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (baseline): Linear(in_features=5, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if args.agent == 'dqn':\n",
    "    policy = policies.ActionOut_QFunction(args.state_dim, args.action_dim, hidden_dims=args.hidden_dims, nonlin=args.nonlin,\n",
    "                                          epsilon=args.epsilon)\n",
    "elif args.discrete_action_space:\n",
    "    policy = policies.Discrete_Policy(args.state_dim, args.action_dim, hidden_dims=args.hidden_dims, nonlin=args.nonlin)\n",
    "elif args.policy == 'full':\n",
    "    policy = policies.Full_Gaussian_Policy(args.state_dim, args.action_dim, hidden_dims=args.hidden_dims, nonlin=args.nonlin)\n",
    "else:\n",
    "    policy = policies.Gaussian_Policy(args.state_dim, args.action_dim, hidden_dims=args.hidden_dims, nonlin=args.nonlin)\n",
    "    \n",
    "if args.agent == 'vpg':\n",
    "    agent = agents.REINFORCE(policy, discount=args.discount, \n",
    "                             optim_type=args.optim, lr=args.lr, weight_decay=args.weight_decay)\n",
    "elif args.agent == 'ppo':\n",
    "    agent = agents.PPO(policy, discount=args.discount, \n",
    "                       optim_type=args.optim, lr=args.lr, weight_decay=args.weight_decay,\n",
    "                       clipping=args.cpi_clip, target_kl=args.target_kl, kl_weight=args.kl_weight, \n",
    "                       epochs=args.agent_epochs, batch_size=args.agent_batch_size, )\n",
    "elif args.agent == 'a3c':\n",
    "    value_fn = policies.ValueFunction(args.state_dim, hidden_dims=args.critic_hidden_dims, nonlin=args.critic_nonlin)\n",
    "    agent = agents.A3C(actor=policy, critic=value_fn, discount=args.discount, \n",
    "                       optim_type=args.optim, lr=args.lr, weight_decay=args.weight_decay)\n",
    "elif args.agent == 'dqn':\n",
    "    buffer = utils.Replay_Buffer(max_transition_size=args.buffer_max_transitions, device=args.device)\n",
    "    agent = agents.DQN(policy, discount=args.discount, buffer=buffer, batch_size=args.buffer_batch_size, \n",
    "                       min_buffer_size=args.buffer_min_start, tau=args.tau, use_replica=args.use_replica,\n",
    "                       optim_type=args.optim, lr=args.lr, weight_decay=args.weight_decay)\n",
    "elif args.agent == 'ddpg':\n",
    "    assert not args.discrete_action_space\n",
    "    qnet = policies.QFunction(args.state_dim, args.action_dim, hidden_dims=args.critic_hidden_dims, nonlin=args.critic_nonlin)\n",
    "    policy = policies.ActorCritic(policy, qnet)\n",
    "    buffer = utils.Replay_Buffer(max_episode_size=args.buffer_max_episodes, device=args.device)\n",
    "    agent = agents.DDPG(policy, discount=args.discount, actor_steps=args.actor_steps, buffer=buffer, \n",
    "                        min_buffer_size=args.buffer_min_start, batch_size=args.buffer_batch_size, tau=args.tau,\n",
    "                        optim_type=args.optim, lr=args.lr, weight_decay=args.weight_decay)\n",
    "else:\n",
    "    raise Exception('Unknown agent: {}'.format(args.agent))\n",
    "    \n",
    "agent.to(args.device)\n",
    "gen = utils.Generator(env, agent)\n",
    "score = utils.Score(tau=0.01)\n",
    "stats = util.StatsMeter('score', 'rewards-train', 'rewards-eval')\n",
    "stats.shallow_join(agent.stats)\n",
    "total_episodes = 0\n",
    "agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fleeb/workspace/marl/foundation/foundation/util/stats.py:283: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.val = torch.tensor(val).float()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1: train=208.268, eval=171.877, score=173.714\n",
      "Ep 2: train=271.462, eval=174.028, score=173.720\n",
      "Ep 3: train=195.835, eval=177.055, score=173.787\n",
      "Ep 4: train=182.961, eval=172.358, score=173.759\n",
      "Ep 5: train=305.441, eval=177.234, score=173.828\n",
      "Ep 6: train=183.152, eval=173.501, score=173.821\n",
      "Ep 7: train=214.467, eval=175.355, score=173.852\n",
      "Ep 8: train=251.347, eval=172.270, score=173.820\n",
      "Ep 9: train=262.787, eval=172.123, score=173.786\n",
      "Ep 10: train=182.314, eval=175.037, score=173.811\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "for itr in range(args.num_iter):\n",
    "    #agent.model.epsilon = epsilon * epsilon_decay ** (total_episodes / epsilon_decay_episodes)\n",
    "    #print('** Iteration {}/{} **'.format(itr+1, num_iter))\n",
    "    train_rewards = utils.run_iteration('train', args.num_train, agent, gen)\n",
    "    eval_rewards = utils.run_iteration('eval', args.num_eval, agent, gen)\n",
    "    total_episodes += args.num_train\n",
    "    score.update_all(eval_rewards)\n",
    "    stats.update('score', score.val)\n",
    "    stats.update('rewards-train', train_rewards.mean())\n",
    "    stats.update('rewards-eval', eval_rewards.mean())\n",
    "    \n",
    "    vals = stats.vals()\n",
    "    logger.update(vals, step=total_episodes)\n",
    "    if args.small_print:\n",
    "        print('Ep {}: train={:.3f}, eval={:.3f}, score={:.3f}'.format(total_episodes, vals['rewards-train'], vals['rewards-eval'], vals['score']))\n",
    "    else:\n",
    "        print('Episode: {}'.format(total_episodes))\n",
    "        print(tabulate(vals))\n",
    "    \n",
    "    # save model\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.view(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1722.5884], device='cuda:0')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utils.run_iteration('eval', 1, agent, gen, render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.train()\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.2589], device='cuda:0')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.2463], device='cuda:0', grad_fn=<ExpandBackward>),\n",
       " tensor([0.9736], device='cuda:0', grad_fn=<ExpandBackward>))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = agent.actor.get_pi(s)\n",
    "d.loc, d.scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.3435], device='cuda:0')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created iterator\n",
      "0.15731143951416016\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([191, 4]), torch.Size([191]), torch.Size([191]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = time.time()\n",
    "out = islice(g, 10)\n",
    "states, actions, rewards = map(torch.cat, zip(*out))\n",
    "print(time.time() - start)\n",
    "states.size(), actions.size(), rewards.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\n"
     ]
    }
   ],
   "source": [
    "# define hyperparameters\n",
    "env = Pytorch_Gym_Env('CartPole-v1')\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DQN(\n",
       "  (model): QNet(\n",
       "    (net): Sequential(\n",
       "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
       "      (1): LeakyReLU(negative_slope=0.01)\n",
       "      (2): Linear(in_features=8, out_features=2, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (target_model): QNet(\n",
       "    (net): Sequential(\n",
       "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
       "      (1): LeakyReLU(negative_slope=0.01)\n",
       "      (2): Linear(in_features=8, out_features=2, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (criterion): SmoothL1Loss()\n",
       ")"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#agent = DDPG(state_dim, action_dim, max_buffer_size=1000, min_buffer_size=50)\n",
    "total_episodes = 0\n",
    "epsilon = 0.01\n",
    "agent = DQN(state_dim, action_dim, \n",
    "            max_buffer_size=1000, min_buffer_size=200, batch_size=128, use_replica=False,\n",
    "            lr=1e-3, tau=0.001, weight_decay=1e-3, epsilon=epsilon)\n",
    "agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create objects\n",
    "# if True:\n",
    "#     device = 'cuda'\n",
    "#     agent.to(device)\n",
    "#     env.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon_decay_episodes = 1000\n",
    "epsilon_decay = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep:50: reward=9.800, loss=0.075, eval=23.400\n",
      "Ep:100: reward=12.160, loss=0.082, eval=14.400\n",
      "Ep:150: reward=24.320, loss=0.047, eval=9.800\n",
      "Ep:200: reward=24.220, loss=0.033, eval=42.600\n",
      "Ep:250: reward=39.800, loss=0.015, eval=34.400\n",
      "Ep:300: reward=30.880, loss=0.016, eval=17.800\n",
      "Ep:350: reward=27.180, loss=0.017, eval=75.000\n",
      "Ep:400: reward=28.700, loss=0.021, eval=17.000\n",
      "Ep:450: reward=33.120, loss=0.016, eval=14.200\n",
      "Ep:500: reward=34.120, loss=0.013, eval=57.600\n",
      "Ep:550: reward=50.880, loss=0.011, eval=68.400\n",
      "Ep:600: reward=41.140, loss=0.013, eval=10.000\n",
      "Ep:650: reward=38.120, loss=0.014, eval=43.400\n",
      "Ep:700: reward=46.040, loss=0.013, eval=34.400\n",
      "Ep:750: reward=51.880, loss=0.013, eval=11.800\n",
      "Ep:800: reward=51.860, loss=0.011, eval=93.800\n",
      "Ep:850: reward=40.580, loss=0.012, eval=57.200\n"
     ]
    }
   ],
   "source": [
    "num_iter = 20\n",
    "num_train = 50\n",
    "num_eval = 5\n",
    "for itr in range(num_iter):\n",
    "    #agent.model.epsilon = epsilon * epsilon_decay ** (total_episodes / epsilon_decay_episodes)\n",
    "    #print('** Iteration {}/{} **'.format(itr+1, num_iter))\n",
    "    train_reward, train_loss = run_episodes('train', num_train, agent, env)\n",
    "    eval_reward, _ = run_episodes('eval', num_eval, agent, env)\n",
    "    total_episodes += num_train\n",
    "    print('Ep:{}: reward={:.3f}, loss={:.3f}, eval={:.3f}'.format(total_episodes, train_reward, train_loss, eval_reward))\n",
    "    \n",
    "    # save model\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.critic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75.9"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_iteration('eval', 1, agent, gen, render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N, D, M = 10, 4, 1\n",
    "\n",
    "f = nn.Linear(D, M)\n",
    "x = torch.randn(N,D)\n",
    "y = f(x).detach()\n",
    "g = util.solve(x,y)\n",
    "g.weight.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env = gym.make('Pong-v4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(210, 160, 3)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env = gym.make('LunarLander-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.00551643,  1.4003555 , -0.55877346, -0.46954992,  0.00639898,\n",
       "        0.12657054,  0.        ,  0.        ], dtype=float32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([192,   0,   0,   0, 110,  38,   0,   7,  63,   1,  60,  59,   0,\n",
       "         0,   0,  62, 255,   0, 255, 253,   0,   8,   0,  24, 128,  32,\n",
       "         1,  86, 247,  86, 247,  86, 247, 134, 243, 245, 243, 240, 240,\n",
       "       242, 242,  32,  32,  64,  64,  64, 188,  65, 189,   0,   8, 109,\n",
       "        37,  37,  60,   0,   0,   0,   0, 109, 109,  37,  37, 192, 192,\n",
       "       192, 192,   1, 192, 202, 247, 202, 247, 202, 247, 202, 247,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,  54, 236, 242, 121, 240], dtype=uint8)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "for _ in range(1000):\n",
    "    env.render()\n",
    "    _,_,done,_ = env.step(env.action_space.sample())\n",
    "    plt.pause(0.02)\n",
    "    if done:\n",
    "        print('stop')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
