{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "To run this you need several packages. First of all, you need anaconda, which you most likely already have if you're viewing this through jupyter. If not then check the readme on the class page.\n",
    "\n",
    "System requirements: This should work on all operating systems (Linux, Mac, and Windows). However, several of the environments in the OpenAI-gym require additional simulators which don't aren't easy to get on Windows. In any case, it is strongly recommended that you use Linux, although you should be ok with Mac. (HINT: if you're on Windows check out the Windows Subsystem for Linux (WSL), although it'll make visualizing your policies a little tricky).\n",
    "\n",
    "Then install the following packages (using conda or pip):\n",
    "\n",
    "- pytorch --> `conda install pytorch -c pytorch`\n",
    "- gym --> `pip install gym`\n",
    "- gym (the cool environments, doesnt work on Windows) --> `pip install gym[all]`\n",
    "(When install gym[all] don't worry if the mujoco installation doesn't work. That's a more advanced 3D physics simulator that has to be set up separately (see website). Anyway, we don't need it necessarily)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
<<<<<<< HEAD
   "metadata": {
    "collapsed": true
   },
=======
   "metadata": {},
>>>>>>> e53191496aa90881b7f882c44d6899f41de113be
   "outputs": [],
   "source": [
    "import sys, os, time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.multiprocessing as mp\n",
    "from torch import distributions\n",
    "from torch.distributions import Categorical\n",
    "from itertools import islice\n",
    "\n",
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "Welcome to the RL playground. Your task is to implement the REINFORCE and A3C algorithm to solve various OpenAI-gym environments. If you are not familiar with OpenAI-gym, stop reading and visit https://gym.openai.com/envs/ to see all the tasks you can try to solve.\n",
    "\n",
    "In this homework, we will only look at tasks with a discrete (and small) action space. That being said, both algorithms can be modified slightly to work on tasks with continuous action spaces. For full credit you must fill in the code below so you achieve an average total reward per episode on the cartpole task (CartPole-v1) of at least 499 (for an episode length of 500) for both REINFORCE and A3C. Then you must apply your code to any one other environment in OpenAI-gym, and plot and compare the learning curves (average total reward per episode vs number of episodes trained on) between REINFORCE and A3C (where at least one of the algorithms shows significant improvement from initialization).\n",
    "\n",
    "Below there's an overview of what every iteration will look like, regardless of whether you want to train or evaluate your agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
<<<<<<< HEAD
   "metadata": {
    "collapsed": true
   },
=======
   "metadata": {},
>>>>>>> e53191496aa90881b7f882c44d6899f41de113be
   "outputs": [],
   "source": [
    "from rlhw_util import * # <-- look whats inside here - it could save you a lot of work!\n",
    "\n",
    "def run_iteration(mode, N, agent, gen, horizon=None, render=False):\n",
    "    train = mode == 'train'\n",
    "    if train:\n",
    "        agent.train()\n",
    "    else:\n",
    "        agent.eval()\n",
    "\n",
    "    states, actions, rewards = zip(*[gen(horizon=horizon, render=render) for _ in range(N)])\n",
    "\n",
    "    loss = None\n",
    "    if train:\n",
    "        loss = agent.learn(states, actions, rewards)\n",
    "\n",
    "    reward = sum([r.sum() for r in rewards]) / N\n",
    "\n",
    "    return reward, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Actor\n",
    "\n",
    "We need to learn a policy which, given some state, outputs a distribution over all possible actions. As this is deep RL, we'll use a deep neural network to turn the observed state into the requisite action distribution. From this action distribution we can choose what action to take using `get_action`. Pytorch, brilliant as it is, makes our task incredibly easy, as we can use the `torch.distributions.Categorical` class for sampling.\n",
    "\n",
    "You can experiment with all sorts of network architectures, but remember this is RL, not image classification on ImageNet, so you probably won't need a very deep network (HINT: look below at the state and action dimensionality to get a feel for the task)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
<<<<<<< HEAD
   "metadata": {
    "collapsed": true
   },
=======
   "metadata": {},
>>>>>>> e53191496aa90881b7f882c44d6899f41de113be
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(Actor, self).__init__()\n",
    "        \n",
    "        # TODO: Fill in the code to define you policy\n",
    "        \n",
    "        self.net = nn.Sequential(nn.Linear(state_dim, state_dim*2),\n",
    "                                 nn.ELU(),\n",
    "                                 nn.Linear(state_dim*2, action_dim*2),\n",
    "                                 nn.ELU(),\n",
    "                                 nn.Linear(action_dim*2, action_dim),\n",
    "                                 nn.Softmax(),\n",
    "                                )\n",
    "\n",
    "    def forward(self, state):\n",
    "        \n",
    "        # TODO: Fill in the code to run a forward pass of your policy to get a distribution over actions (HINT: probabilities sum to 1)\n",
    "        \n",
    "        return self.net(state)\n",
    "\n",
    "    def get_policy(self, state):\n",
    "        return Categorical(self(state))\n",
    "\n",
    "    def get_action(self, state, greedy=None):\n",
    "        if greedy is None:\n",
    "            greedy = not self.training\n",
    "\n",
    "        policy = self.get_policy(state)\n",
    "        return MLE(policy) if greedy else policy.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The REINFORCE Agent\n",
    "\n",
    "The Actor defines our policy, but we also have to define how and when we'll be updating our policy, which brings us to the agent. The agent will house the policy (an `Actor`), and can then be used to generate rollouts (using `forward()`) or update the policy given a list of rollouts (using `learn()`).\n",
    "\n",
    "The REINFORCE algorithm naively uses the returns directly to weight the gradients, however this makes the variance in the policy gradient estimation very large. As a result, we will use a baseline which is a linear model which takes in a state and outputs the return (sounds like a value function, right?). Except we're not going to train our baseline using gradient descent, instead we'll just solve the linear system analytically in every iteration, and use the solution in the next iteration. Don't worry about training/updating the baseline, but you do have to use it in the right way. (Optional experiment: try removing the baseline and see how performance changes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
<<<<<<< HEAD
   "metadata": {
    "collapsed": true
   },
=======
   "metadata": {},
>>>>>>> e53191496aa90881b7f882c44d6899f41de113be
   "outputs": [],
   "source": [
    "class REINFORCE(nn.Module):\n",
    "    \n",
    "    def __init__(self, state_dim, action_dim, discount=0.97, lr=1e-3, weight_decay=1e-4):\n",
    "        super(REINFORCE, self).__init__()\n",
    "        self.actor = Actor(state_dim, action_dim)\n",
    "        \n",
    "        self.baseline = nn.Linear(state_dim, 1)\n",
    "        \n",
    "        # TODO: create an optimizer for the parameters of your actor (HINT: use the passed in lr and weight_decay args)\n",
    "        self.optim = optim.RMSprop(self.actor.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        \n",
    "        self.discount = discount\n",
    "        \n",
    "    def forward(self, state):\n",
    "        return self.actor.get_action(state)\n",
    "    \n",
    "    def learn(self, states, actions, rewards):\n",
    "        '''\n",
    "        Takes in three arguments each of which is a list with equal length. Each element in the list is a \n",
    "        pytorch tensor with 1 row for every step in the episode, and the columns are state_dim, action_dim, \n",
    "        and 1, respectively.\n",
    "        '''\n",
    "        \n",
    "        # TODO: implement the REINFORCE algorithm (HINT: check the slides/papers)\n",
    "        \n",
    "        returns = [compute_returns(rs, discount=self.discount) for rs in rewards]\n",
    "        \n",
    "        states, actions, returns = torch.cat(states), torch.cat(actions), torch.cat(returns)\n",
    "        \n",
    "        advantages = returns - self.baseline(states).squeeze()\n",
    "        \n",
    "        pi = self.actor.get_policy(states)\n",
    "        \n",
    "        self.optim.zero_grad()\n",
    "        perf = -advantages * pi.log_prob(actions)\n",
    "        perf = perf.mean()\n",
    "        perf.backward()\n",
    "        self.optim.step()\n",
    "        \n",
    "        error = F.mse_loss(self.baseline(states).squeeze(), returns).detach()\n",
    "        solve(states, returns, out=self.baseline)\n",
    "        #error = F.mse_loss(self.baseline(states).squeeze(), returns).detach()\n",
    "        \n",
    "        return error.item() # Returns a rough estimate of the error in the baseline (dont worry about this too much)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Critic\n",
    "\n",
    "Now we can introduce a critic, which is essentially a value function to estimate the expected discounted reward of a state."
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
=======
   "execution_count": 39,
   "metadata": {},
>>>>>>> e53191496aa90881b7f882c44d6899f41de113be
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_dim):\n",
    "        super(Critic, self).__init__()\n",
    "        \n",
    "        # TODO: define your value function network\n",
    "        \n",
    "        self.net = nn.Sequential(nn.Linear(state_dim, state_dim),\n",
    "                                 nn.ELU(),\n",
    "                                 nn.Linear(state_dim, state_dim//2),\n",
    "                                 nn.ELU(),\n",
    "                                 nn.Linear(state_dim//2, 1),\n",
    "                                )\n",
    "\n",
    "    def forward(self, state):\n",
    "        \n",
    "        # TODO: apply your value function network to get a value given this batch of states\n",
    "        \n",
    "        return self.net(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The A3C Agent\n",
    "\n",
    "Now we can put the actor and critic together using the A3C algorithm. It turns out, the tasks in the gym are all so simple that there is essentially no gain in parallelization, so technically we're implementing A2C (no async), but the RL part is the same."
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
=======
   "execution_count": 40,
   "metadata": {},
>>>>>>> e53191496aa90881b7f882c44d6899f41de113be
   "outputs": [],
   "source": [
    "class A3C(nn.Module):\n",
    "    \n",
    "    def __init__(self, state_dim, action_dim, discount=0.97, lr=1e-3, weight_decay=1e-4):\n",
    "        super(A3C, self).__init__()\n",
    "        self.actor = Actor(state_dim, action_dim)\n",
    "        self.critic = Critic(state_dim)\n",
    "        \n",
    "        # TODO: create an optimizer for the parameters of your actor (HINT: use the passed in lr and weight_decay args)\n",
    "        # (HINT: the actor and critic have different objectives, so how many optimizers do you need?)\n",
    "        self.actor_optim = optim.Adam(self.actor.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        self.critic_optim = optim.Adam(self.critic.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        \n",
    "        self.discount = discount\n",
    "        \n",
    "    def forward(self, state):\n",
    "        return self.actor.get_action(state)\n",
    "    \n",
    "    def learn(self, states, actions, rewards):\n",
    "        \n",
    "        returns = [compute_returns(rs, discount=self.discount) for rs in rewards]\n",
    "        \n",
    "        states, actions, returns = torch.cat(states), torch.cat(actions), torch.cat(returns)\n",
    "        \n",
    "        # TODO: implement A3C (HINT: algorithm details found in A3C paper supplement) \n",
    "        # (HINT2: the algorithm is actually very similar to REINFORCE, the only difference is now we have a critic, what might that do?)\n",
    "        \n",
    "        advantages = returns - self.critic(states).squeeze()\n",
    "        \n",
    "        pi = self.actor.get_policy(states)\n",
    "        \n",
    "        self.actor_optim.zero_grad()\n",
    "        perf = -advantages.detach() * pi.log_prob(actions)\n",
    "        perf = perf.mean()\n",
    "        perf.backward()\n",
    "        self.actor_optim.step()\n",
    "        \n",
    "        self.critic_optim.zero_grad()\n",
    "        loss = advantages.pow(2).mean()\n",
    "        loss.backward()\n",
    "        self.critic_optim.step()\n",
    "        \n",
    "        return loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Balancing a pole with a cart\n",
    "\n",
    "First, we'll test both algorithms on a very simple toy system: the cartpole. Eventhough it's very low dimensional (state=4, action=2), this task is nontrival because it is underactuated. Nevertheless after a few thousand episodes our policy shouldn't have a problem! "
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
=======
   "execution_count": 41,
   "metadata": {},
>>>>>>> e53191496aa90881b7f882c44d6899f41de113be
   "outputs": [],
   "source": [
    "# Optimization hyperparameters\n",
    "lr = 1e-3\n",
    "weight_decay = 1e-4"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
=======
   "execution_count": 42,
   "metadata": {},
>>>>>>> e53191496aa90881b7f882c44d6899f41de113be
   "outputs": [],
   "source": [
    "#env_name = 'CartPole-v1' \n",
    "env_name = 'LunarLander-v2'\n",
    "e = Pytorch_Gym_Env(env_name)\n",
    "state_dim = e.observation_space.shape[0]\n",
    "action_dim = e.action_space.n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 9,
=======
   "execution_count": 43,
>>>>>>> e53191496aa90881b7f882c44d6899f41de113be
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A3C(\n",
      "  (actor): Actor(\n",
      "    (net): Sequential(\n",
<<<<<<< HEAD
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ELU(alpha=1.0)\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ELU(alpha=1.0)\n",
      "      (4): Linear(in_features=4, out_features=2, bias=True)\n",
=======
      "      (0): Linear(in_features=8, out_features=16, bias=True)\n",
      "      (1): ELU(alpha=1.0)\n",
      "      (2): Linear(in_features=16, out_features=8, bias=True)\n",
      "      (3): ELU(alpha=1.0)\n",
      "      (4): Linear(in_features=8, out_features=4, bias=True)\n",
>>>>>>> e53191496aa90881b7f882c44d6899f41de113be
      "      (5): Softmax()\n",
      "    )\n",
      "  )\n",
      "  (critic): Critic(\n",
      "    (net): Sequential(\n",
<<<<<<< HEAD
      "      (0): Linear(in_features=4, out_features=2, bias=True)\n",
      "      (1): ELU(alpha=1.0)\n",
      "      (2): Linear(in_features=2, out_features=1, bias=True)\n",
=======
      "      (0): Linear(in_features=8, out_features=8, bias=True)\n",
      "      (1): ELU(alpha=1.0)\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ELU(alpha=1.0)\n",
      "      (4): Linear(in_features=4, out_features=1, bias=True)\n",
>>>>>>> e53191496aa90881b7f882c44d6899f41de113be
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Choose what agent to use\n",
    "#agent = REINFORCE(state_dim, action_dim, lr=lr, weight_decay=weight_decay)\n",
    "agent = A3C(state_dim, action_dim, lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "total_episodes = 0\n",
    "print(agent) # Let's take a look at what we're working with..."
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
=======
   "execution_count": 15,
   "metadata": {},
>>>>>>> e53191496aa90881b7f882c44d6899f41de113be
   "outputs": [],
   "source": [
    "# Create a \n",
    "gen = Generator(e, agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's do this!!\n",
    "\n",
    "Below is the loop to train and evaluate your agent. You can play around with the number of iterations to run, and the number of rollouts per iteration. \n",
    "\n",
    "You can rerun this cell multiple times to keep training your model for more episodes. In any case, it shouldn't take more than 30 min to an 1 hour to train. (training never took me more than 5 min). HINT: Keep an eye on the eval_reward, it'll be pretty noisy, but if that should be slowly increasing."
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
=======
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fleeb/anaconda3/lib/python3.5/site-packages/torch/nn/modules/container.py:91: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep:20: reward=-982.541, loss=43422.527, eval=-986.350\n",
      "Ep:40: reward=-1159.231, loss=54260.668, eval=-910.419\n",
      "Ep:60: reward=-1059.192, loss=46507.801, eval=-810.510\n",
      "Ep:80: reward=-910.276, loss=43403.000, eval=-840.645\n",
      "Ep:100: reward=-1169.885, loss=64642.785, eval=-620.449\n",
      "Ep:120: reward=-866.101, loss=38498.371, eval=-1057.822\n",
      "Ep:140: reward=-960.674, loss=42907.520, eval=-877.643\n",
      "Ep:160: reward=-815.475, loss=34737.785, eval=-708.105\n",
      "Ep:180: reward=-1226.797, loss=64242.691, eval=-895.134\n",
      "Ep:200: reward=-822.800, loss=32163.867, eval=-903.181\n",
      "Ep:220: reward=-843.009, loss=36840.336, eval=-608.282\n",
      "Ep:240: reward=-842.578, loss=34871.719, eval=-1393.923\n",
      "Ep:260: reward=-853.779, loss=34813.574, eval=-875.303\n",
      "Ep:280: reward=-690.802, loss=26626.510, eval=-730.922\n",
      "Ep:300: reward=-785.374, loss=35267.156, eval=-955.443\n",
      "Ep:320: reward=-797.702, loss=35034.637, eval=-1416.064\n",
      "Ep:340: reward=-815.275, loss=36943.461, eval=-805.041\n",
      "Ep:360: reward=-789.224, loss=36193.402, eval=-1135.329\n",
      "Ep:380: reward=-909.020, loss=37486.328, eval=-808.701\n",
      "Ep:400: reward=-978.951, loss=43515.684, eval=-621.987\n",
      "Ep:420: reward=-971.523, loss=47465.164, eval=-1022.726\n",
      "Ep:440: reward=-718.381, loss=30043.971, eval=-931.725\n",
      "Ep:460: reward=-1186.040, loss=55297.590, eval=-1105.070\n",
      "Ep:480: reward=-1029.447, loss=42974.645, eval=-877.299\n",
      "Ep:500: reward=-983.053, loss=47918.176, eval=-1173.877\n",
      "Ep:520: reward=-821.131, loss=36710.234, eval=-880.172\n",
      "Ep:540: reward=-1073.302, loss=53595.836, eval=-1207.369\n",
      "Ep:560: reward=-1042.585, loss=51071.090, eval=-781.555\n",
      "Ep:580: reward=-1040.002, loss=43930.223, eval=-1081.340\n",
      "Ep:600: reward=-978.813, loss=41837.008, eval=-970.328\n",
      "Ep:620: reward=-739.088, loss=31651.463, eval=-591.628\n",
      "Ep:640: reward=-1005.884, loss=44158.062, eval=-719.652\n",
      "Ep:660: reward=-869.054, loss=37660.398, eval=-813.984\n",
      "Ep:680: reward=-1036.187, loss=52097.148, eval=-754.353\n",
      "Ep:700: reward=-1209.915, loss=52495.508, eval=-654.913\n",
      "Ep:720: reward=-1100.483, loss=53121.992, eval=-784.648\n",
      "Ep:740: reward=-855.010, loss=35887.168, eval=-928.483\n",
      "Ep:760: reward=-897.844, loss=38879.477, eval=-979.580\n",
      "Ep:780: reward=-942.047, loss=41108.262, eval=-684.097\n",
      "Ep:800: reward=-859.854, loss=37889.184, eval=-778.679\n",
      "Ep:820: reward=-847.203, loss=33721.785, eval=-686.459\n",
      "Ep:840: reward=-779.881, loss=34368.551, eval=-655.434\n",
      "Ep:860: reward=-1104.705, loss=74596.172, eval=-766.944\n",
      "Ep:880: reward=-832.094, loss=36555.816, eval=-801.655\n",
      "Ep:900: reward=-1155.563, loss=55673.523, eval=-767.278\n",
      "Ep:920: reward=-931.562, loss=39942.984, eval=-828.476\n",
      "Ep:940: reward=-879.113, loss=41358.164, eval=-715.209\n",
      "Ep:960: reward=-955.577, loss=39289.961, eval=-1195.069\n",
      "Ep:980: reward=-855.896, loss=30996.295, eval=-742.900\n",
      "Ep:1000: reward=-759.004, loss=32409.510, eval=-1214.098\n",
      "Ep:1020: reward=-945.016, loss=47202.438, eval=-748.847\n",
      "Ep:1040: reward=-980.796, loss=44335.477, eval=-870.556\n",
      "Ep:1060: reward=-945.647, loss=40481.348, eval=-731.246\n",
      "Ep:1080: reward=-963.976, loss=43491.051, eval=-919.965\n",
      "Ep:1100: reward=-731.193, loss=28080.545, eval=-1270.172\n",
      "Ep:1120: reward=-1356.541, loss=67790.852, eval=-892.883\n",
      "Ep:1140: reward=-836.571, loss=38166.312, eval=-1180.475\n",
      "Ep:1160: reward=-1108.156, loss=58330.609, eval=-749.621\n",
      "Ep:1180: reward=-830.560, loss=32797.344, eval=-887.611\n",
      "Ep:1200: reward=-862.136, loss=35714.832, eval=-980.623\n",
      "Ep:1220: reward=-1086.935, loss=49589.000, eval=-790.054\n",
      "Ep:1240: reward=-1050.715, loss=50039.508, eval=-878.798\n",
      "Ep:1260: reward=-978.763, loss=45445.430, eval=-820.800\n",
      "Ep:1280: reward=-952.203, loss=42415.191, eval=-830.405\n",
      "Ep:1300: reward=-1026.203, loss=44153.168, eval=-929.813\n",
      "Ep:1320: reward=-791.358, loss=33348.840, eval=-588.544\n",
      "Ep:1340: reward=-893.650, loss=39232.250, eval=-1369.317\n",
      "Ep:1360: reward=-1075.972, loss=49756.500, eval=-656.214\n",
      "Ep:1380: reward=-1050.727, loss=46252.566, eval=-750.573\n",
      "Ep:1400: reward=-1104.969, loss=57748.426, eval=-709.247\n",
      "Ep:1420: reward=-900.192, loss=37260.496, eval=-797.005\n",
      "Ep:1440: reward=-972.795, loss=39746.223, eval=-1363.595\n",
      "Ep:1460: reward=-874.798, loss=38666.512, eval=-876.062\n",
      "Ep:1480: reward=-737.383, loss=29244.701, eval=-1215.646\n",
      "Ep:1500: reward=-927.693, loss=44523.227, eval=-811.981\n",
      "Ep:1520: reward=-928.835, loss=42551.020, eval=-1084.722\n",
      "Ep:1540: reward=-919.728, loss=41403.074, eval=-885.600\n",
      "Ep:1560: reward=-729.201, loss=30039.186, eval=-802.693\n",
      "Ep:1580: reward=-845.876, loss=37454.629, eval=-925.436\n",
      "Ep:1600: reward=-930.151, loss=39285.992, eval=-867.235\n",
      "Ep:1620: reward=-781.544, loss=32957.918, eval=-851.931\n",
      "Ep:1640: reward=-888.294, loss=37700.531, eval=-758.771\n",
      "Ep:1660: reward=-833.043, loss=35191.504, eval=-1016.129\n",
      "Ep:1680: reward=-828.915, loss=35802.625, eval=-785.065\n",
      "Ep:1700: reward=-680.379, loss=27361.525, eval=-900.497\n",
      "Ep:1720: reward=-807.790, loss=33790.035, eval=-707.494\n",
      "Ep:1740: reward=-929.405, loss=43813.977, eval=-752.938\n",
      "Ep:1760: reward=-938.295, loss=41704.184, eval=-947.536\n",
      "Ep:1780: reward=-753.661, loss=30233.480, eval=-746.330\n",
      "Ep:1800: reward=-907.070, loss=37290.184, eval=-753.127\n",
      "Ep:1820: reward=-877.541, loss=37377.461, eval=-732.783\n",
      "Ep:1840: reward=-941.554, loss=40116.105, eval=-817.975\n",
      "Ep:1860: reward=-739.205, loss=31851.963, eval=-777.338\n",
      "Ep:1880: reward=-903.554, loss=40362.523, eval=-797.716\n",
      "Ep:1900: reward=-949.431, loss=41211.086, eval=-1114.876\n",
      "Ep:1920: reward=-1686.226, loss=98616.820, eval=-1363.371\n",
      "Ep:1940: reward=-819.152, loss=34339.152, eval=-1114.204\n",
      "Ep:1960: reward=-975.573, loss=42757.055, eval=-1411.915\n",
      "Ep:1980: reward=-842.543, loss=39911.059, eval=-857.059\n",
      "Ep:2000: reward=-782.232, loss=33181.797, eval=-785.181\n",
      "Ep:2020: reward=-886.369, loss=37555.664, eval=-737.731\n",
      "Ep:2040: reward=-940.662, loss=39705.348, eval=-916.081\n",
      "Ep:2060: reward=-882.094, loss=35868.168, eval=-1142.426\n",
      "Ep:2080: reward=-735.559, loss=33771.727, eval=-728.043\n",
      "Ep:2100: reward=-988.886, loss=42554.434, eval=-797.684\n",
      "Ep:2120: reward=-849.285, loss=34935.945, eval=-786.230\n",
      "Ep:2140: reward=-1063.532, loss=46527.984, eval=-1034.309\n",
      "Ep:2160: reward=-711.574, loss=28188.846, eval=-726.650\n",
      "Ep:2180: reward=-750.846, loss=31215.793, eval=-953.882\n",
      "Ep:2200: reward=-1061.223, loss=43947.797, eval=-1167.798\n",
      "Ep:2220: reward=-1137.397, loss=52135.125, eval=-865.472\n",
      "Ep:2240: reward=-936.138, loss=42415.414, eval=-1033.847\n",
      "Ep:2260: reward=-846.529, loss=33838.059, eval=-739.152\n",
      "Ep:2280: reward=-1028.951, loss=48551.152, eval=-729.442\n",
      "Ep:2300: reward=-1021.808, loss=46605.887, eval=-938.555\n",
      "Ep:2320: reward=-832.807, loss=34994.336, eval=-685.486\n",
      "Ep:2340: reward=-729.628, loss=32530.566, eval=-950.645\n",
      "Ep:2360: reward=-914.651, loss=39296.340, eval=-929.737\n",
      "Ep:2380: reward=-1082.631, loss=57773.750, eval=-746.193\n",
      "Ep:2400: reward=-908.077, loss=39717.449, eval=-890.235\n",
      "Ep:2420: reward=-940.928, loss=41856.590, eval=-865.094\n",
      "Ep:2440: reward=-1160.605, loss=56070.406, eval=-1169.597\n",
      "Ep:2460: reward=-1012.694, loss=45396.242, eval=-988.633\n",
      "Ep:2480: reward=-1035.469, loss=44490.297, eval=-815.052\n",
      "Ep:2500: reward=-910.559, loss=42633.367, eval=-975.300\n",
      "Ep:2520: reward=-975.746, loss=46185.879, eval=-817.468\n",
      "Ep:2540: reward=-759.829, loss=31219.414, eval=-947.682\n",
      "Ep:2560: reward=-785.951, loss=30235.135, eval=-687.155\n",
      "Ep:2580: reward=-1105.546, loss=49728.117, eval=-736.369\n",
      "Ep:2600: reward=-1227.875, loss=54731.832, eval=-733.920\n",
      "Ep:2620: reward=-905.766, loss=47361.387, eval=-586.655\n",
      "Ep:2640: reward=-802.404, loss=32511.322, eval=-991.233\n",
      "Ep:2660: reward=-812.111, loss=35085.254, eval=-771.519\n",
      "Ep:2680: reward=-1173.622, loss=56097.789, eval=-968.908\n",
      "Ep:2700: reward=-1003.406, loss=51263.723, eval=-838.484\n",
      "Ep:2720: reward=-808.168, loss=31146.744, eval=-1086.174\n",
      "Ep:2740: reward=-807.570, loss=35375.641, eval=-836.335\n",
      "Ep:2760: reward=-743.124, loss=32246.369, eval=-963.074\n",
      "Ep:2780: reward=-717.326, loss=29325.621, eval=-743.633\n",
      "Ep:2800: reward=-1201.658, loss=54807.055, eval=-783.586\n",
      "Ep:2820: reward=-766.274, loss=29370.098, eval=-808.544\n",
      "Ep:2840: reward=-902.744, loss=40804.902, eval=-980.336\n",
      "Ep:2860: reward=-1091.737, loss=51777.676, eval=-1183.522\n",
      "Ep:2880: reward=-918.407, loss=39768.613, eval=-1208.097\n",
      "Ep:2900: reward=-729.689, loss=29827.098, eval=-804.083\n",
      "Ep:2920: reward=-1164.307, loss=51632.824, eval=-934.324\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep:2940: reward=-778.259, loss=33431.770, eval=-605.981\n",
      "Ep:2960: reward=-1033.170, loss=46333.734, eval=-948.845\n",
      "Ep:2980: reward=-978.162, loss=45778.117, eval=-698.193\n",
      "Ep:3000: reward=-1066.552, loss=51503.570, eval=-1103.965\n",
      "Ep:3020: reward=-979.192, loss=44905.898, eval=-971.093\n",
      "Ep:3040: reward=-849.324, loss=33756.555, eval=-786.955\n",
      "Ep:3060: reward=-995.359, loss=37099.207, eval=-689.783\n",
      "Ep:3080: reward=-900.337, loss=36448.688, eval=-869.403\n",
      "Ep:3100: reward=-986.125, loss=41553.219, eval=-772.416\n",
      "Ep:3120: reward=-773.666, loss=30275.914, eval=-846.229\n",
      "Ep:3140: reward=-780.334, loss=31630.844, eval=-875.061\n",
      "Ep:3160: reward=-757.356, loss=33054.793, eval=-988.794\n",
      "Ep:3180: reward=-966.359, loss=41166.164, eval=-1270.119\n",
      "Ep:3200: reward=-1230.687, loss=55157.973, eval=-1040.073\n",
      "Ep:3220: reward=-889.825, loss=35136.703, eval=-1256.269\n",
      "Ep:3240: reward=-755.706, loss=33255.781, eval=-777.203\n",
      "Ep:3260: reward=-1025.416, loss=47334.660, eval=-915.232\n",
      "Ep:3280: reward=-1023.408, loss=52069.453, eval=-1100.943\n",
      "Ep:3300: reward=-882.807, loss=37340.102, eval=-1299.381\n",
      "Ep:3320: reward=-735.104, loss=29462.867, eval=-697.786\n",
      "Ep:3340: reward=-814.847, loss=32981.504, eval=-684.001\n",
      "Ep:3360: reward=-1028.924, loss=48876.078, eval=-937.148\n",
      "Ep:3380: reward=-895.374, loss=35682.535, eval=-931.497\n",
      "Ep:3400: reward=-903.308, loss=37024.695, eval=-917.530\n",
      "Ep:3420: reward=-880.687, loss=36332.637, eval=-1493.944\n",
      "Ep:3440: reward=-1152.939, loss=52185.035, eval=-693.064\n",
      "Ep:3460: reward=-775.603, loss=31459.498, eval=-718.460\n",
      "Ep:3480: reward=-997.851, loss=42180.121, eval=-945.067\n",
      "Ep:3500: reward=-721.305, loss=29154.691, eval=-887.939\n",
      "Ep:3520: reward=-852.843, loss=33958.262, eval=-1080.940\n",
      "Ep:3540: reward=-990.876, loss=45834.965, eval=-1070.990\n",
      "Ep:3560: reward=-1008.349, loss=41647.160, eval=-1001.655\n",
      "Ep:3580: reward=-693.466, loss=24169.711, eval=-872.580\n",
      "Ep:3600: reward=-878.450, loss=34861.871, eval=-1264.885\n",
      "Ep:3620: reward=-921.830, loss=37394.938, eval=-1118.890\n",
      "Ep:3640: reward=-1068.916, loss=45831.473, eval=-1153.711\n",
      "Ep:3660: reward=-870.465, loss=37154.102, eval=-1022.603\n",
      "Ep:3680: reward=-737.430, loss=32232.311, eval=-1072.833\n",
      "Ep:3700: reward=-991.935, loss=42834.996, eval=-824.293\n",
      "Ep:3720: reward=-990.092, loss=41935.727, eval=-765.511\n",
      "Ep:3740: reward=-1199.340, loss=54083.254, eval=-764.706\n",
      "Ep:3760: reward=-715.521, loss=26262.859, eval=-919.275\n",
      "Ep:3780: reward=-808.687, loss=33162.160, eval=-762.494\n",
      "Ep:3800: reward=-932.920, loss=34166.254, eval=-864.385\n",
      "Ep:3820: reward=-787.357, loss=33344.379, eval=-692.313\n",
      "Ep:3840: reward=-882.052, loss=35889.273, eval=-777.995\n",
      "Ep:3860: reward=-818.117, loss=34007.422, eval=-654.544\n",
      "Ep:3880: reward=-852.114, loss=33247.707, eval=-934.522\n",
      "Ep:3900: reward=-979.647, loss=44348.832, eval=-933.222\n",
      "Ep:3920: reward=-940.956, loss=43349.688, eval=-1021.809\n",
      "Ep:3940: reward=-901.485, loss=33198.316, eval=-691.389\n",
      "Ep:3960: reward=-752.627, loss=30026.236, eval=-709.434\n",
      "Ep:3980: reward=-729.813, loss=28784.301, eval=-1075.879\n",
      "Ep:4000: reward=-735.562, loss=28925.840, eval=-919.847\n",
      "Ep:4020: reward=-800.467, loss=29910.109, eval=-687.384\n",
      "Ep:4040: reward=-910.979, loss=39115.199, eval=-721.595\n",
      "Ep:4060: reward=-832.898, loss=32500.775, eval=-784.082\n",
      "Ep:4080: reward=-778.884, loss=30694.156, eval=-712.764\n",
      "Ep:4100: reward=-843.321, loss=35056.926, eval=-717.144\n",
      "Ep:4120: reward=-854.152, loss=39218.965, eval=-643.660\n",
      "Ep:4140: reward=-756.709, loss=27773.922, eval=-1707.499\n",
      "Ep:4160: reward=-939.644, loss=37337.582, eval=-842.386\n",
      "Ep:4180: reward=-760.866, loss=30119.551, eval=-832.777\n",
      "Ep:4200: reward=-772.567, loss=28586.658, eval=-799.690\n",
      "Ep:4220: reward=-1043.371, loss=43595.121, eval=-789.400\n",
      "Ep:4240: reward=-1000.222, loss=39891.496, eval=-767.527\n",
      "Ep:4260: reward=-892.472, loss=37939.426, eval=-1316.825\n",
      "Ep:4280: reward=-839.406, loss=32840.605, eval=-1349.369\n",
      "Ep:4300: reward=-779.445, loss=31366.840, eval=-1113.980\n",
      "Ep:4320: reward=-1049.743, loss=41313.637, eval=-994.062\n",
      "Ep:4340: reward=-1101.483, loss=45745.117, eval=-994.595\n",
      "Ep:4360: reward=-1043.277, loss=40065.766, eval=-941.545\n",
      "Ep:4380: reward=-1036.908, loss=48482.387, eval=-1016.308\n",
      "Ep:4400: reward=-997.780, loss=37924.109, eval=-852.954\n",
      "Ep:4420: reward=-878.845, loss=35737.895, eval=-878.223\n",
      "Ep:4440: reward=-861.853, loss=33714.738, eval=-840.722\n",
      "Ep:4460: reward=-766.613, loss=30367.414, eval=-862.923\n",
      "Ep:4480: reward=-861.466, loss=34842.629, eval=-1073.837\n",
      "Ep:4500: reward=-938.769, loss=41311.039, eval=-883.410\n",
      "Ep:4520: reward=-901.119, loss=34917.562, eval=-1001.919\n",
      "Ep:4540: reward=-954.186, loss=43230.656, eval=-755.004\n",
      "Ep:4560: reward=-1142.925, loss=53784.438, eval=-915.145\n",
      "Ep:4580: reward=-854.763, loss=37386.051, eval=-1333.475\n",
      "Ep:4600: reward=-1278.660, loss=61438.223, eval=-1489.482\n",
      "Ep:4620: reward=-911.858, loss=38085.176, eval=-1004.853\n",
      "Ep:4640: reward=-1030.464, loss=44737.258, eval=-860.796\n",
      "Ep:4660: reward=-1294.177, loss=69145.914, eval=-846.654\n",
      "Ep:4680: reward=-766.994, loss=29775.654, eval=-833.901\n",
      "Ep:4700: reward=-820.205, loss=32048.096, eval=-805.242\n",
      "Ep:4720: reward=-875.863, loss=33628.414, eval=-826.933\n",
      "Ep:4740: reward=-922.725, loss=37726.359, eval=-884.291\n",
      "Ep:4760: reward=-721.945, loss=26323.008, eval=-707.775\n",
      "Ep:4780: reward=-847.789, loss=33336.305, eval=-630.389\n",
      "Ep:4800: reward=-846.328, loss=33419.285, eval=-843.766\n",
      "Ep:4820: reward=-763.993, loss=30646.033, eval=-1089.435\n",
      "Ep:4840: reward=-844.267, loss=37670.289, eval=-886.854\n",
      "Ep:4860: reward=-931.261, loss=35889.273, eval=-900.319\n",
      "Ep:4880: reward=-773.800, loss=31502.664, eval=-872.017\n",
      "Ep:4900: reward=-837.720, loss=31883.832, eval=-1158.659\n",
      "Ep:4920: reward=-815.717, loss=30388.033, eval=-944.835\n",
      "Ep:4940: reward=-746.261, loss=28868.328, eval=-707.416\n",
      "Ep:4960: reward=-1003.837, loss=39472.488, eval=-1377.672\n",
      "Ep:4980: reward=-824.496, loss=31284.281, eval=-979.129\n",
      "Ep:5000: reward=-731.220, loss=26613.434, eval=-729.669\n",
      "Ep:5020: reward=-1229.772, loss=49837.223, eval=-787.250\n",
      "Ep:5040: reward=-923.818, loss=39235.113, eval=-737.774\n",
      "Ep:5060: reward=-846.838, loss=31925.959, eval=-938.244\n",
      "Ep:5080: reward=-922.642, loss=33528.582, eval=-1640.420\n",
      "Ep:5100: reward=-806.225, loss=29950.992, eval=-853.963\n",
      "Ep:5120: reward=-1157.476, loss=46484.965, eval=-1021.612\n",
      "Ep:5140: reward=-1034.730, loss=46150.637, eval=-986.817\n",
      "Ep:5160: reward=-849.909, loss=32352.426, eval=-1669.523\n",
      "Ep:5180: reward=-907.306, loss=31529.156, eval=-893.886\n",
      "Ep:5200: reward=-1041.995, loss=39237.246, eval=-1083.070\n",
      "Ep:5220: reward=-1027.057, loss=40892.074, eval=-926.581\n",
      "Ep:5240: reward=-908.347, loss=34977.746, eval=-721.087\n",
      "Ep:5260: reward=-831.091, loss=32925.820, eval=-845.160\n",
      "Ep:5280: reward=-1028.793, loss=44314.176, eval=-870.806\n",
      "Ep:5300: reward=-802.470, loss=31672.260, eval=-874.575\n",
      "Ep:5320: reward=-863.601, loss=33697.266, eval=-972.739\n",
      "Ep:5340: reward=-730.504, loss=24550.004, eval=-1143.058\n",
      "Ep:5360: reward=-968.489, loss=40317.379, eval=-1384.365\n",
      "Ep:5380: reward=-861.843, loss=31672.627, eval=-839.169\n",
      "Ep:5400: reward=-928.420, loss=34617.949, eval=-802.799\n",
      "Ep:5420: reward=-664.883, loss=25715.373, eval=-876.961\n",
      "Ep:5440: reward=-894.103, loss=32686.150, eval=-1638.917\n",
      "Ep:5460: reward=-766.269, loss=28098.053, eval=-762.779\n",
      "Ep:5480: reward=-899.869, loss=32515.441, eval=-1412.160\n",
      "Ep:5500: reward=-745.300, loss=29162.398, eval=-815.962\n",
      "Ep:5520: reward=-633.084, loss=23791.314, eval=-906.819\n",
      "Ep:5540: reward=-957.973, loss=38319.668, eval=-694.681\n",
      "Ep:5560: reward=-1146.362, loss=44579.500, eval=-882.757\n",
      "Ep:5580: reward=-1019.332, loss=39284.898, eval=-1402.732\n",
      "Ep:5600: reward=-720.404, loss=23837.510, eval=-712.549\n",
      "Ep:5620: reward=-1044.788, loss=39085.590, eval=-881.050\n",
      "Ep:5640: reward=-895.432, loss=32673.135, eval=-1727.693\n",
      "Ep:5660: reward=-1023.486, loss=35834.934, eval=-899.102\n",
      "Ep:5680: reward=-789.585, loss=30674.545, eval=-1066.736\n",
      "Ep:5700: reward=-797.208, loss=28863.338, eval=-765.878\n",
      "Ep:5720: reward=-898.967, loss=37487.566, eval=-875.851\n",
      "Ep:5740: reward=-753.098, loss=27791.156, eval=-705.332\n",
      "Ep:5760: reward=-1150.053, loss=47739.723, eval=-1053.552\n",
      "Ep:5780: reward=-871.151, loss=35038.441, eval=-1009.296\n",
      "Ep:5800: reward=-836.340, loss=29732.396, eval=-926.321\n",
      "Ep:5820: reward=-1105.217, loss=52666.922, eval=-1037.917\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep:5840: reward=-990.968, loss=35698.098, eval=-714.697\n",
      "Ep:5860: reward=-930.974, loss=38025.805, eval=-744.863\n",
      "Ep:5880: reward=-855.635, loss=34394.945, eval=-860.253\n",
      "Ep:5900: reward=-928.222, loss=31605.740, eval=-1034.158\n",
      "Ep:5920: reward=-938.659, loss=38934.953, eval=-1082.322\n",
      "Ep:5940: reward=-1071.655, loss=43729.141, eval=-986.413\n",
      "Ep:5960: reward=-777.024, loss=26833.289, eval=-612.603\n",
      "Ep:5980: reward=-990.174, loss=40634.184, eval=-972.403\n",
      "Ep:6000: reward=-993.596, loss=36611.426, eval=-895.341\n",
      "Ep:6020: reward=-865.812, loss=30373.527, eval=-903.267\n",
      "Ep:6040: reward=-828.643, loss=28298.285, eval=-819.533\n",
      "Ep:6060: reward=-733.114, loss=26264.328, eval=-754.324\n",
      "Ep:6080: reward=-845.353, loss=31551.639, eval=-963.763\n",
      "Ep:6100: reward=-992.179, loss=33196.082, eval=-981.196\n",
      "Ep:6120: reward=-732.832, loss=25964.428, eval=-982.036\n",
      "Ep:6140: reward=-939.606, loss=36698.664, eval=-727.854\n",
      "Ep:6160: reward=-835.453, loss=29394.596, eval=-643.069\n",
      "Ep:6180: reward=-791.795, loss=28066.121, eval=-1417.242\n",
      "Ep:6200: reward=-740.209, loss=24126.504, eval=-1231.729\n",
      "Ep:6220: reward=-705.241, loss=24398.717, eval=-875.511\n",
      "Ep:6240: reward=-775.071, loss=23500.322, eval=-852.529\n",
      "Ep:6260: reward=-749.677, loss=26045.512, eval=-1416.239\n",
      "Ep:6280: reward=-738.886, loss=22814.840, eval=-665.656\n",
      "Ep:6300: reward=-753.164, loss=26624.219, eval=-741.268\n",
      "Ep:6320: reward=-1028.211, loss=34579.082, eval=-853.698\n",
      "Ep:6340: reward=-1144.028, loss=41745.152, eval=-759.737\n",
      "Ep:6360: reward=-967.848, loss=32749.117, eval=-880.135\n",
      "Ep:6380: reward=-937.766, loss=33755.387, eval=-853.386\n",
      "Ep:6400: reward=-1023.201, loss=41318.504, eval=-1205.822\n",
      "Ep:6420: reward=-1272.150, loss=41550.449, eval=-762.914\n",
      "Ep:6440: reward=-847.458, loss=28624.285, eval=-921.884\n",
      "Ep:6460: reward=-1419.811, loss=51190.086, eval=-757.015\n",
      "Ep:6480: reward=-908.924, loss=29997.650, eval=-926.574\n",
      "Ep:6500: reward=-890.859, loss=29053.479, eval=-1126.207\n",
      "Ep:6520: reward=-1063.679, loss=37679.957, eval=-895.779\n",
      "Ep:6540: reward=-910.267, loss=29731.318, eval=-928.973\n",
      "Ep:6560: reward=-906.586, loss=32716.381, eval=-819.521\n",
      "Ep:6580: reward=-644.288, loss=20555.891, eval=-1055.944\n",
      "Ep:6600: reward=-1217.648, loss=40354.762, eval=-1006.588\n",
      "Ep:6620: reward=-855.673, loss=27014.168, eval=-1098.903\n",
      "Ep:6640: reward=-819.840, loss=28800.465, eval=-793.099\n",
      "Ep:6660: reward=-955.529, loss=34847.816, eval=-892.422\n",
      "Ep:6680: reward=-1008.916, loss=32628.354, eval=-786.205\n",
      "Ep:6700: reward=-666.101, loss=19991.363, eval=-760.097\n",
      "Ep:6720: reward=-841.807, loss=30128.822, eval=-1025.645\n",
      "Ep:6740: reward=-873.856, loss=29229.850, eval=-1019.050\n",
      "Ep:6760: reward=-1286.430, loss=48829.180, eval=-902.985\n",
      "Ep:6780: reward=-888.682, loss=27959.223, eval=-807.569\n",
      "Ep:6800: reward=-789.969, loss=25471.574, eval=-1115.038\n",
      "Ep:6820: reward=-1009.062, loss=34767.523, eval=-1394.899\n",
      "Ep:6840: reward=-952.544, loss=29398.410, eval=-1182.627\n",
      "Ep:6860: reward=-868.508, loss=27654.848, eval=-1049.471\n",
      "Ep:6880: reward=-815.411, loss=24634.834, eval=-720.894\n",
      "Ep:6900: reward=-918.760, loss=28186.818, eval=-995.134\n",
      "Ep:6920: reward=-1034.476, loss=33284.129, eval=-952.546\n",
      "Ep:6940: reward=-943.625, loss=35264.703, eval=-1281.308\n",
      "Ep:6960: reward=-953.185, loss=30290.529, eval=-787.931\n",
      "Ep:6980: reward=-1011.229, loss=34453.965, eval=-1186.906\n",
      "Ep:7000: reward=-934.639, loss=28714.217, eval=-742.964\n",
      "Ep:7020: reward=-707.060, loss=22326.646, eval=-689.100\n",
      "Ep:7040: reward=-1063.709, loss=34174.664, eval=-718.382\n",
      "Ep:7060: reward=-1091.330, loss=35551.047, eval=-894.399\n",
      "Ep:7080: reward=-727.810, loss=24700.777, eval=-786.428\n",
      "Ep:7100: reward=-975.825, loss=29491.871, eval=-1187.816\n",
      "Ep:7120: reward=-934.820, loss=30557.607, eval=-1625.271\n",
      "Ep:7140: reward=-1041.799, loss=34620.023, eval=-1110.609\n",
      "Ep:7160: reward=-681.443, loss=19150.420, eval=-850.005\n",
      "Ep:7180: reward=-739.842, loss=20371.059, eval=-869.896\n",
      "Ep:7200: reward=-1068.281, loss=31405.283, eval=-799.377\n",
      "Ep:7220: reward=-764.565, loss=21370.172, eval=-1184.479\n",
      "Ep:7240: reward=-1177.742, loss=36471.363, eval=-901.199\n",
      "Ep:7260: reward=-869.852, loss=26728.537, eval=-1490.592\n",
      "Ep:7280: reward=-877.330, loss=23890.189, eval=-859.649\n",
      "Ep:7300: reward=-727.466, loss=22141.955, eval=-717.257\n",
      "Ep:7320: reward=-822.576, loss=23234.508, eval=-1140.914\n",
      "Ep:7340: reward=-1138.331, loss=38036.559, eval=-861.197\n",
      "Ep:7360: reward=-904.540, loss=27021.705, eval=-879.125\n",
      "Ep:7380: reward=-803.811, loss=23796.701, eval=-869.468\n",
      "Ep:7400: reward=-896.075, loss=27497.562, eval=-1073.936\n",
      "Ep:7420: reward=-895.403, loss=24347.373, eval=-783.200\n",
      "Ep:7440: reward=-951.431, loss=27963.973, eval=-704.865\n",
      "Ep:7460: reward=-932.709, loss=27550.670, eval=-1307.406\n",
      "Ep:7480: reward=-812.297, loss=25085.307, eval=-1048.696\n",
      "Ep:7500: reward=-721.675, loss=19311.176, eval=-853.177\n",
      "Ep:7520: reward=-1004.234, loss=31461.037, eval=-730.237\n",
      "Ep:7540: reward=-932.008, loss=25144.945, eval=-999.239\n",
      "Ep:7560: reward=-997.601, loss=30460.445, eval=-1215.562\n",
      "Ep:7580: reward=-1017.930, loss=29782.930, eval=-895.112\n",
      "Ep:7600: reward=-865.911, loss=23350.574, eval=-832.002\n",
      "Ep:7620: reward=-915.843, loss=26001.762, eval=-919.126\n",
      "Ep:7640: reward=-829.534, loss=23750.547, eval=-1136.452\n",
      "Ep:7660: reward=-790.711, loss=21216.816, eval=-672.121\n",
      "Ep:7680: reward=-828.968, loss=23387.268, eval=-674.423\n",
      "Ep:7700: reward=-872.739, loss=25723.383, eval=-848.797\n",
      "Ep:7720: reward=-994.386, loss=30289.877, eval=-1067.754\n",
      "Ep:7740: reward=-881.576, loss=26278.523, eval=-978.808\n",
      "Ep:7760: reward=-1004.392, loss=28828.412, eval=-835.169\n",
      "Ep:7780: reward=-956.208, loss=28707.332, eval=-845.845\n",
      "Ep:7800: reward=-759.937, loss=18948.232, eval=-764.901\n",
      "Ep:7820: reward=-1179.689, loss=33004.973, eval=-983.532\n",
      "Ep:7840: reward=-830.956, loss=22345.070, eval=-706.972\n",
      "Ep:7860: reward=-850.802, loss=21946.717, eval=-1372.739\n",
      "Ep:7880: reward=-1040.569, loss=28237.402, eval=-720.645\n",
      "Ep:7900: reward=-737.566, loss=20469.455, eval=-727.497\n",
      "Ep:7920: reward=-943.960, loss=25754.389, eval=-686.004\n",
      "Ep:7940: reward=-739.206, loss=20069.168, eval=-912.130\n",
      "Ep:7960: reward=-920.216, loss=24766.725, eval=-710.496\n",
      "Ep:7980: reward=-997.066, loss=28172.029, eval=-1232.654\n",
      "Ep:8000: reward=-1081.914, loss=30374.346, eval=-982.610\n",
      "Ep:8020: reward=-881.122, loss=22877.510, eval=-1076.310\n",
      "Ep:8040: reward=-886.242, loss=22610.609, eval=-947.088\n",
      "Ep:8060: reward=-821.566, loss=21982.531, eval=-735.207\n",
      "Ep:8080: reward=-785.018, loss=20620.488, eval=-1111.894\n",
      "Ep:8100: reward=-950.025, loss=25627.730, eval=-1052.258\n",
      "Ep:8120: reward=-1187.428, loss=30940.562, eval=-796.846\n",
      "Ep:8140: reward=-817.251, loss=19979.971, eval=-738.274\n",
      "Ep:8160: reward=-774.407, loss=20996.176, eval=-616.294\n",
      "Ep:8180: reward=-958.101, loss=24737.051, eval=-855.942\n",
      "Ep:8200: reward=-761.020, loss=19536.223, eval=-775.542\n",
      "Ep:8220: reward=-1215.705, loss=32173.184, eval=-749.382\n",
      "Ep:8240: reward=-818.169, loss=21305.184, eval=-1038.250\n",
      "Ep:8260: reward=-896.287, loss=20993.260, eval=-937.187\n",
      "Ep:8280: reward=-789.973, loss=21581.576, eval=-1111.918\n",
      "Ep:8300: reward=-750.078, loss=19765.705, eval=-787.723\n",
      "Ep:8320: reward=-861.600, loss=22175.453, eval=-750.511\n",
      "Ep:8340: reward=-879.181, loss=24099.857, eval=-904.805\n",
      "Ep:8360: reward=-812.104, loss=20183.439, eval=-1013.682\n",
      "Ep:8380: reward=-974.716, loss=23831.178, eval=-768.281\n",
      "Ep:8400: reward=-862.370, loss=20610.896, eval=-888.126\n",
      "Ep:8420: reward=-865.039, loss=20229.984, eval=-935.751\n",
      "Ep:8440: reward=-956.096, loss=23767.340, eval=-664.913\n",
      "Ep:8460: reward=-745.102, loss=19181.295, eval=-1050.880\n",
      "Ep:8480: reward=-807.561, loss=20281.248, eval=-941.972\n",
      "Ep:8500: reward=-832.807, loss=21059.041, eval=-1055.178\n",
      "Ep:8520: reward=-924.132, loss=22063.904, eval=-673.874\n",
      "Ep:8540: reward=-671.148, loss=15023.732, eval=-657.639\n",
      "Ep:8560: reward=-889.483, loss=20335.506, eval=-953.809\n",
      "Ep:8580: reward=-894.198, loss=23422.660, eval=-888.068\n",
      "Ep:8600: reward=-1005.111, loss=25099.348, eval=-793.941\n",
      "Ep:8620: reward=-822.918, loss=19617.473, eval=-1054.819\n",
      "Ep:8640: reward=-1028.046, loss=21694.691, eval=-861.041\n",
      "Ep:8660: reward=-1148.567, loss=26754.873, eval=-1156.410\n",
      "Ep:8680: reward=-682.497, loss=15867.270, eval=-916.393\n",
      "Ep:8700: reward=-1049.482, loss=22875.607, eval=-614.106\n",
      "Ep:8720: reward=-970.768, loss=21661.613, eval=-877.283\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep:8740: reward=-874.481, loss=18754.486, eval=-1094.505\n",
      "Ep:8760: reward=-993.679, loss=23058.900, eval=-744.471\n",
      "Ep:8780: reward=-1161.712, loss=25489.250, eval=-1120.706\n",
      "Ep:8800: reward=-920.504, loss=21553.094, eval=-913.788\n",
      "Ep:8820: reward=-995.411, loss=23795.941, eval=-729.859\n",
      "Ep:8840: reward=-629.663, loss=14230.414, eval=-952.475\n",
      "Ep:8860: reward=-762.890, loss=17404.709, eval=-917.470\n",
      "Ep:8880: reward=-1295.280, loss=28539.029, eval=-737.524\n",
      "Ep:8900: reward=-807.925, loss=18824.734, eval=-845.920\n",
      "Ep:8920: reward=-913.877, loss=19952.748, eval=-800.403\n",
      "Ep:8940: reward=-764.021, loss=15399.441, eval=-760.074\n",
      "Ep:8960: reward=-1053.045, loss=22161.363, eval=-1124.184\n",
      "Ep:8980: reward=-871.608, loss=18823.201, eval=-783.516\n",
      "Ep:9000: reward=-846.319, loss=19150.010, eval=-797.226\n",
      "Ep:9020: reward=-1140.305, loss=24175.160, eval=-707.848\n",
      "Ep:9040: reward=-760.991, loss=17473.914, eval=-999.959\n",
      "Ep:9060: reward=-877.200, loss=19447.426, eval=-1166.216\n",
      "Ep:9080: reward=-796.897, loss=16281.357, eval=-845.469\n",
      "Ep:9100: reward=-799.581, loss=15816.551, eval=-886.366\n",
      "Ep:9120: reward=-693.220, loss=14329.275, eval=-859.121\n",
      "Ep:9140: reward=-794.279, loss=14485.074, eval=-785.299\n",
      "Ep:9160: reward=-884.653, loss=18720.707, eval=-960.062\n",
      "Ep:9180: reward=-693.966, loss=14616.417, eval=-720.184\n",
      "Ep:9200: reward=-988.485, loss=19193.461, eval=-708.314\n",
      "Ep:9220: reward=-1182.927, loss=22508.980, eval=-723.985\n",
      "Ep:9240: reward=-811.794, loss=14554.359, eval=-1112.031\n",
      "Ep:9260: reward=-803.091, loss=13715.722, eval=-981.110\n",
      "Ep:9280: reward=-909.046, loss=17676.760, eval=-1042.078\n",
      "Ep:9300: reward=-767.336, loss=13907.818, eval=-900.246\n",
      "Ep:9320: reward=-1112.255, loss=21239.893, eval=-914.507\n",
      "Ep:9340: reward=-913.271, loss=18899.881, eval=-977.430\n",
      "Ep:9360: reward=-924.107, loss=17725.672, eval=-835.310\n",
      "Ep:9380: reward=-805.831, loss=15200.083, eval=-855.274\n",
      "Ep:9400: reward=-759.950, loss=14648.896, eval=-785.455\n",
      "Ep:9420: reward=-966.766, loss=20038.195, eval=-1217.873\n",
      "Ep:9440: reward=-763.773, loss=14039.585, eval=-1198.894\n",
      "Ep:9460: reward=-1049.206, loss=21121.031, eval=-763.792\n",
      "Ep:9480: reward=-869.234, loss=16394.535, eval=-840.342\n",
      "Ep:9500: reward=-877.883, loss=16488.889, eval=-1123.754\n",
      "Ep:9520: reward=-837.661, loss=16057.391, eval=-1314.773\n",
      "Ep:9540: reward=-773.955, loss=13993.506, eval=-776.643\n",
      "Ep:9560: reward=-1021.662, loss=19120.086, eval=-1021.118\n",
      "Ep:9580: reward=-674.679, loss=13643.131, eval=-1342.529\n",
      "Ep:9600: reward=-919.802, loss=16533.236, eval=-895.426\n",
      "Ep:9620: reward=-721.027, loss=12581.720, eval=-835.485\n",
      "Ep:9640: reward=-765.958, loss=12840.486, eval=-865.359\n",
      "Ep:9660: reward=-902.453, loss=17183.066, eval=-943.965\n",
      "Ep:9680: reward=-999.674, loss=16871.041, eval=-1315.177\n",
      "Ep:9700: reward=-771.179, loss=13696.798, eval=-1130.894\n",
      "Ep:9720: reward=-974.511, loss=17268.174, eval=-705.351\n",
      "Ep:9740: reward=-807.831, loss=14703.394, eval=-584.138\n",
      "Ep:9760: reward=-945.277, loss=17850.254, eval=-944.635\n",
      "Ep:9780: reward=-1317.106, loss=22686.820, eval=-769.127\n",
      "Ep:9800: reward=-884.287, loss=14690.824, eval=-865.135\n",
      "Ep:9820: reward=-1019.897, loss=17171.773, eval=-720.927\n",
      "Ep:9840: reward=-1102.449, loss=19740.910, eval=-1329.086\n",
      "Ep:9860: reward=-952.883, loss=14880.018, eval=-820.260\n",
      "Ep:9880: reward=-821.725, loss=13650.380, eval=-877.498\n",
      "Ep:9900: reward=-934.266, loss=17175.688, eval=-766.957\n",
      "Ep:9920: reward=-1041.471, loss=17446.100, eval=-934.212\n",
      "Ep:9940: reward=-979.445, loss=17593.953, eval=-725.486\n",
      "Ep:9960: reward=-983.777, loss=16192.359, eval=-944.295\n",
      "Ep:9980: reward=-984.373, loss=16767.420, eval=-958.475\n",
      "Ep:10000: reward=-1009.751, loss=17852.430, eval=-757.744\n",
      "Ep:10020: reward=-946.232, loss=15868.526, eval=-806.961\n",
      "Ep:10040: reward=-727.379, loss=11429.854, eval=-818.169\n",
      "Ep:10060: reward=-914.734, loss=15202.208, eval=-833.963\n",
      "Ep:10080: reward=-930.550, loss=14747.710, eval=-1200.349\n",
      "Ep:10100: reward=-1123.405, loss=18131.205, eval=-1400.156\n",
      "Ep:10120: reward=-709.122, loss=11351.076, eval=-887.343\n",
      "Ep:10140: reward=-1053.326, loss=17259.445, eval=-1071.130\n",
      "Ep:10160: reward=-1010.889, loss=17654.234, eval=-918.184\n",
      "Ep:10180: reward=-703.749, loss=11953.748, eval=-894.088\n",
      "Ep:10200: reward=-974.053, loss=16400.953, eval=-866.087\n",
      "Ep:10220: reward=-865.576, loss=14157.293, eval=-949.785\n",
      "Ep:10240: reward=-824.415, loss=13521.303, eval=-765.646\n",
      "Ep:10260: reward=-1134.431, loss=17922.982, eval=-720.919\n",
      "Ep:10280: reward=-826.609, loss=14220.939, eval=-892.841\n",
      "Ep:10300: reward=-1015.048, loss=15709.175, eval=-693.045\n",
      "Ep:10320: reward=-762.005, loss=11888.806, eval=-717.076\n",
      "Ep:10340: reward=-856.812, loss=12693.815, eval=-921.985\n",
      "Ep:10360: reward=-916.508, loss=14161.917, eval=-1070.695\n",
      "Ep:10380: reward=-891.037, loss=14436.290, eval=-1085.258\n",
      "Ep:10400: reward=-957.515, loss=14577.468, eval=-557.477\n",
      "Ep:10420: reward=-764.276, loss=11814.461, eval=-815.019\n",
      "Ep:10440: reward=-806.950, loss=12745.635, eval=-942.467\n",
      "Ep:10460: reward=-740.680, loss=11033.276, eval=-813.308\n",
      "Ep:10480: reward=-910.624, loss=13727.182, eval=-1107.875\n",
      "Ep:10500: reward=-777.449, loss=11379.488, eval=-951.549\n",
      "Ep:10520: reward=-935.051, loss=12694.699, eval=-907.440\n",
      "Ep:10540: reward=-1174.147, loss=17985.475, eval=-955.052\n",
      "Ep:10560: reward=-655.926, loss=10069.640, eval=-1062.556\n",
      "Ep:10580: reward=-796.355, loss=11186.070, eval=-1600.116\n",
      "Ep:10600: reward=-800.592, loss=12258.087, eval=-1013.689\n",
      "Ep:10620: reward=-921.261, loss=13591.902, eval=-744.814\n",
      "Ep:10640: reward=-1444.358, loss=21234.285, eval=-1033.220\n",
      "Ep:10660: reward=-1087.505, loss=14236.689, eval=-1036.269\n",
      "Ep:10680: reward=-770.889, loss=11932.891, eval=-1010.916\n",
      "Ep:10700: reward=-970.264, loss=13386.128, eval=-955.038\n",
      "Ep:10720: reward=-925.367, loss=12375.654, eval=-905.216\n",
      "Ep:10740: reward=-796.700, loss=11063.104, eval=-852.436\n",
      "Ep:10760: reward=-878.401, loss=11993.975, eval=-713.747\n",
      "Ep:10780: reward=-810.295, loss=12237.441, eval=-896.772\n",
      "Ep:10800: reward=-793.786, loss=10077.304, eval=-1420.378\n",
      "Ep:10820: reward=-822.517, loss=10530.348, eval=-723.198\n",
      "Ep:10840: reward=-892.009, loss=12272.362, eval=-858.667\n",
      "Ep:10860: reward=-947.135, loss=13180.896, eval=-636.482\n",
      "Ep:10880: reward=-937.206, loss=12843.290, eval=-969.651\n",
      "Ep:10900: reward=-1125.496, loss=13447.930, eval=-741.953\n",
      "Ep:10920: reward=-773.834, loss=8713.869, eval=-739.020\n",
      "Ep:10940: reward=-1043.482, loss=12792.907, eval=-944.711\n",
      "Ep:10960: reward=-1165.216, loss=20597.385, eval=-723.093\n",
      "Ep:10980: reward=-1190.903, loss=16122.483, eval=-728.211\n",
      "Ep:11000: reward=-905.082, loss=13159.919, eval=-728.896\n",
      "Ep:11020: reward=-722.158, loss=8708.378, eval=-767.297\n",
      "Ep:11040: reward=-806.229, loss=9905.540, eval=-709.050\n",
      "Ep:11060: reward=-759.169, loss=9447.326, eval=-904.212\n",
      "Ep:11080: reward=-904.530, loss=12637.495, eval=-920.427\n",
      "Ep:11100: reward=-813.820, loss=10389.819, eval=-786.454\n",
      "Ep:11120: reward=-1025.289, loss=16313.344, eval=-733.794\n",
      "Ep:11140: reward=-889.475, loss=11386.015, eval=-939.038\n",
      "Ep:11160: reward=-930.736, loss=11931.015, eval=-938.972\n",
      "Ep:11180: reward=-759.703, loss=9874.022, eval=-1317.227\n",
      "Ep:11200: reward=-724.643, loss=8789.715, eval=-713.667\n",
      "Ep:11220: reward=-1185.418, loss=13141.030, eval=-984.580\n",
      "Ep:11240: reward=-730.697, loss=8173.312, eval=-876.547\n",
      "Ep:11260: reward=-883.686, loss=11131.897, eval=-919.790\n",
      "Ep:11280: reward=-996.781, loss=12003.995, eval=-1035.773\n",
      "Ep:11300: reward=-841.998, loss=10021.745, eval=-684.185\n",
      "Ep:11320: reward=-914.158, loss=11189.619, eval=-720.245\n",
      "Ep:11340: reward=-886.251, loss=10289.218, eval=-1021.911\n",
      "Ep:11360: reward=-764.768, loss=9951.173, eval=-1084.939\n",
      "Ep:11380: reward=-1095.798, loss=12950.721, eval=-1016.620\n",
      "Ep:11400: reward=-967.419, loss=12838.646, eval=-845.778\n",
      "Ep:11420: reward=-939.594, loss=10479.913, eval=-1319.330\n",
      "Ep:11440: reward=-914.912, loss=10094.364, eval=-1051.362\n",
      "Ep:11460: reward=-906.524, loss=9907.422, eval=-694.155\n",
      "Ep:11480: reward=-1022.537, loss=11967.857, eval=-660.391\n",
      "Ep:11500: reward=-684.909, loss=7820.376, eval=-728.231\n",
      "Ep:11520: reward=-838.850, loss=10663.174, eval=-869.501\n",
      "Ep:11540: reward=-887.750, loss=10210.112, eval=-1263.727\n",
      "Ep:11560: reward=-937.651, loss=11173.949, eval=-767.738\n",
      "Ep:11580: reward=-865.412, loss=8835.267, eval=-934.860\n",
      "Ep:11600: reward=-828.011, loss=9381.512, eval=-628.446\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep:11620: reward=-761.951, loss=8655.131, eval=-1026.283\n",
      "Ep:11640: reward=-1252.506, loss=15474.159, eval=-739.404\n",
      "Ep:11660: reward=-1002.902, loss=11361.536, eval=-1325.600\n",
      "Ep:11680: reward=-781.922, loss=8641.188, eval=-831.724\n",
      "Ep:11700: reward=-1149.225, loss=12947.271, eval=-1246.991\n",
      "Ep:11720: reward=-799.464, loss=7739.581, eval=-833.059\n",
      "Ep:11740: reward=-807.878, loss=8008.874, eval=-1176.960\n",
      "Ep:11760: reward=-749.027, loss=8394.729, eval=-636.154\n",
      "Ep:11780: reward=-851.564, loss=9357.487, eval=-730.846\n",
      "Ep:11800: reward=-843.221, loss=10155.898, eval=-883.181\n",
      "Ep:11820: reward=-933.120, loss=9921.647, eval=-1109.255\n",
      "Ep:11840: reward=-1027.504, loss=12164.937, eval=-903.620\n",
      "Ep:11860: reward=-880.554, loss=8638.778, eval=-857.373\n",
      "Ep:11880: reward=-1368.877, loss=19878.090, eval=-876.674\n",
      "Ep:11900: reward=-1116.217, loss=12549.321, eval=-888.197\n",
      "Ep:11920: reward=-957.133, loss=10467.293, eval=-869.563\n",
      "Ep:11940: reward=-987.309, loss=12250.912, eval=-957.429\n",
      "Ep:11960: reward=-705.722, loss=7487.648, eval=-1060.590\n",
      "Ep:11980: reward=-1145.079, loss=18426.033, eval=-1259.114\n",
      "Ep:12000: reward=-1163.605, loss=12108.622, eval=-820.127\n",
      "Ep:12020: reward=-1321.103, loss=29741.477, eval=-746.104\n",
      "Ep:12040: reward=-934.782, loss=9168.902, eval=-947.727\n",
      "Ep:12060: reward=-1069.570, loss=9656.036, eval=-1539.556\n",
      "Ep:12080: reward=-784.717, loss=7248.618, eval=-783.457\n",
      "Ep:12100: reward=-1144.317, loss=14583.562, eval=-724.922\n",
      "Ep:12120: reward=-903.743, loss=8719.091, eval=-1277.863\n",
      "Ep:12140: reward=-835.016, loss=9227.393, eval=-1395.948\n",
      "Ep:12160: reward=-1011.621, loss=9717.285, eval=-1030.628\n",
      "Ep:12180: reward=-894.877, loss=8878.677, eval=-1503.527\n",
      "Ep:12200: reward=-1157.847, loss=24244.986, eval=-763.675\n",
      "Ep:12220: reward=-863.367, loss=8874.344, eval=-784.229\n",
      "Ep:12240: reward=-824.412, loss=8254.229, eval=-1054.555\n",
      "Ep:12260: reward=-827.487, loss=9683.745, eval=-969.442\n",
      "Ep:12280: reward=-845.052, loss=9289.885, eval=-966.258\n",
      "Ep:12300: reward=-900.221, loss=9292.059, eval=-939.454\n",
      "Ep:12320: reward=-869.142, loss=9824.287, eval=-749.910\n",
      "Ep:12340: reward=-779.092, loss=7964.976, eval=-1036.837\n",
      "Ep:12360: reward=-931.826, loss=9099.481, eval=-832.655\n",
      "Ep:12380: reward=-987.332, loss=10265.342, eval=-571.280\n",
      "Ep:12400: reward=-782.151, loss=7722.361, eval=-774.256\n",
      "Ep:12420: reward=-1009.846, loss=11086.351, eval=-800.325\n",
      "Ep:12440: reward=-868.071, loss=8222.653, eval=-888.117\n",
      "Ep:12460: reward=-949.223, loss=9915.203, eval=-859.987\n",
      "Ep:12480: reward=-1072.758, loss=10612.939, eval=-1182.859\n",
      "Ep:12500: reward=-1243.169, loss=12965.386, eval=-791.061\n",
      "Ep:12520: reward=-757.685, loss=8084.001, eval=-1203.945\n",
      "Ep:12540: reward=-825.034, loss=8363.926, eval=-814.490\n",
      "Ep:12560: reward=-712.007, loss=8292.931, eval=-1092.199\n",
      "Ep:12580: reward=-1069.452, loss=11318.858, eval=-706.315\n",
      "Ep:12600: reward=-1130.761, loss=12085.984, eval=-745.001\n",
      "Ep:12620: reward=-1063.387, loss=10751.225, eval=-770.811\n",
      "Ep:12640: reward=-970.006, loss=9507.266, eval=-1189.686\n",
      "Ep:12660: reward=-957.030, loss=9600.825, eval=-824.970\n",
      "Ep:12680: reward=-729.357, loss=6881.947, eval=-814.427\n",
      "Ep:12700: reward=-787.788, loss=8615.023, eval=-763.498\n",
      "Ep:12720: reward=-1068.136, loss=9739.101, eval=-798.098\n",
      "Ep:12740: reward=-625.622, loss=5749.673, eval=-962.664\n",
      "Ep:12760: reward=-912.344, loss=8466.566, eval=-912.320\n",
      "Ep:12780: reward=-799.583, loss=7295.153, eval=-895.568\n",
      "Ep:12800: reward=-1000.873, loss=9536.853, eval=-878.935\n",
      "Ep:12820: reward=-862.875, loss=7878.315, eval=-925.142\n",
      "Ep:12840: reward=-827.096, loss=9179.403, eval=-944.497\n",
      "Ep:12860: reward=-810.971, loss=7376.326, eval=-1186.562\n",
      "Ep:12880: reward=-791.331, loss=7550.504, eval=-1137.547\n",
      "Ep:12900: reward=-850.142, loss=8636.642, eval=-833.821\n",
      "Ep:12920: reward=-991.300, loss=8045.189, eval=-684.685\n",
      "Ep:12940: reward=-872.860, loss=9150.411, eval=-1057.660\n",
      "Ep:12960: reward=-792.627, loss=7193.869, eval=-853.284\n",
      "Ep:12980: reward=-1067.566, loss=11075.588, eval=-1615.302\n",
      "Ep:13000: reward=-732.168, loss=5867.383, eval=-697.678\n",
      "Ep:13020: reward=-784.760, loss=8082.891, eval=-857.540\n",
      "Ep:13040: reward=-1167.621, loss=11159.227, eval=-1532.214\n",
      "Ep:13060: reward=-815.054, loss=6019.341, eval=-943.569\n",
      "Ep:13080: reward=-1021.595, loss=8764.642, eval=-954.075\n",
      "Ep:13100: reward=-850.124, loss=8058.829, eval=-868.645\n",
      "Ep:13120: reward=-1033.100, loss=10755.246, eval=-1116.195\n",
      "Ep:13140: reward=-733.086, loss=6352.758, eval=-878.176\n",
      "Ep:13160: reward=-790.024, loss=6889.616, eval=-804.424\n",
      "Ep:13180: reward=-772.303, loss=6269.609, eval=-1068.315\n",
      "Ep:13200: reward=-930.063, loss=9006.429, eval=-1512.932\n",
      "Ep:13220: reward=-957.502, loss=9059.248, eval=-840.735\n",
      "Ep:13240: reward=-827.253, loss=6267.584, eval=-721.329\n",
      "Ep:13260: reward=-1159.361, loss=11270.450, eval=-924.533\n",
      "Ep:13280: reward=-750.944, loss=5890.458, eval=-938.144\n",
      "Ep:13300: reward=-886.568, loss=7253.384, eval=-1744.980\n",
      "Ep:13320: reward=-1264.676, loss=11508.458, eval=-1050.213\n",
      "Ep:13340: reward=-673.949, loss=5347.542, eval=-877.858\n",
      "Ep:13360: reward=-962.217, loss=8895.467, eval=-958.308\n",
      "Ep:13380: reward=-942.933, loss=8302.920, eval=-918.493\n",
      "Ep:13400: reward=-720.829, loss=6056.854, eval=-822.314\n",
      "Ep:13420: reward=-872.983, loss=8372.398, eval=-826.820\n",
      "Ep:13440: reward=-1061.411, loss=10544.141, eval=-1479.239\n",
      "Ep:13460: reward=-914.683, loss=8701.791, eval=-1484.027\n",
      "Ep:13480: reward=-880.763, loss=9042.566, eval=-908.342\n",
      "Ep:13500: reward=-842.108, loss=7416.777, eval=-800.380\n",
      "Ep:13520: reward=-1067.170, loss=9874.417, eval=-1137.459\n",
      "Ep:13540: reward=-1100.477, loss=16850.480, eval=-658.717\n",
      "Ep:13560: reward=-728.980, loss=5917.236, eval=-949.678\n",
      "Ep:13580: reward=-727.851, loss=5402.999, eval=-839.486\n",
      "Ep:13600: reward=-794.779, loss=6630.533, eval=-987.767\n",
      "Ep:13620: reward=-871.834, loss=7600.694, eval=-759.256\n",
      "Ep:13640: reward=-1149.834, loss=15329.343, eval=-808.872\n",
      "Ep:13660: reward=-934.503, loss=8041.020, eval=-1227.702\n",
      "Ep:13680: reward=-1136.778, loss=11892.272, eval=-944.297\n",
      "Ep:13700: reward=-748.453, loss=6408.184, eval=-809.824\n",
      "Ep:13720: reward=-945.261, loss=8546.517, eval=-908.860\n",
      "Ep:13740: reward=-799.036, loss=6377.146, eval=-788.317\n",
      "Ep:13760: reward=-869.219, loss=7237.846, eval=-883.955\n",
      "Ep:13780: reward=-998.310, loss=8355.286, eval=-622.459\n",
      "Ep:13800: reward=-1157.190, loss=10172.779, eval=-896.213\n",
      "Ep:13820: reward=-916.404, loss=8319.007, eval=-1022.800\n",
      "Ep:13840: reward=-1030.664, loss=10780.134, eval=-779.813\n",
      "Ep:13860: reward=-873.536, loss=6965.710, eval=-922.291\n",
      "Ep:13880: reward=-1074.974, loss=10198.438, eval=-937.686\n",
      "Ep:13900: reward=-946.699, loss=7849.797, eval=-1130.605\n",
      "Ep:13920: reward=-787.340, loss=6478.900, eval=-665.090\n",
      "Ep:13940: reward=-923.849, loss=7032.182, eval=-673.568\n",
      "Ep:13960: reward=-1006.127, loss=8416.619, eval=-805.726\n",
      "Ep:13980: reward=-716.371, loss=5300.386, eval=-1074.947\n",
      "Ep:14000: reward=-826.266, loss=7006.695, eval=-900.871\n",
      "Ep:14020: reward=-825.714, loss=5766.621, eval=-611.061\n",
      "Ep:14040: reward=-1130.769, loss=14073.159, eval=-1269.367\n",
      "Ep:14060: reward=-899.476, loss=7889.869, eval=-1196.000\n",
      "Ep:14080: reward=-973.304, loss=9192.625, eval=-999.566\n",
      "Ep:14100: reward=-912.534, loss=6992.741, eval=-1085.835\n",
      "Ep:14120: reward=-942.544, loss=7033.034, eval=-1074.175\n",
      "Ep:14140: reward=-1009.330, loss=15194.054, eval=-986.879\n",
      "Ep:14160: reward=-897.885, loss=6430.005, eval=-1140.018\n",
      "Ep:14180: reward=-697.216, loss=6433.840, eval=-765.290\n",
      "Ep:14200: reward=-1301.302, loss=15001.760, eval=-1096.395\n",
      "Ep:14220: reward=-1067.995, loss=9018.467, eval=-724.310\n",
      "Ep:14240: reward=-863.449, loss=7303.762, eval=-907.874\n",
      "Ep:14260: reward=-975.267, loss=8638.854, eval=-1913.015\n",
      "Ep:14280: reward=-904.449, loss=6564.919, eval=-1038.162\n",
      "Ep:14300: reward=-1010.688, loss=12698.153, eval=-930.880\n",
      "Ep:14320: reward=-1164.353, loss=16360.968, eval=-1355.744\n",
      "Ep:14340: reward=-740.134, loss=5889.921, eval=-696.802\n",
      "Ep:14360: reward=-782.686, loss=6196.673, eval=-939.110\n",
      "Ep:14380: reward=-1026.054, loss=7976.504, eval=-1093.898\n",
      "Ep:14400: reward=-916.497, loss=7829.914, eval=-985.439\n",
      "Ep:14420: reward=-858.555, loss=6581.445, eval=-753.732\n",
      "Ep:14440: reward=-839.461, loss=7157.454, eval=-842.504\n",
      "Ep:14460: reward=-742.219, loss=5047.446, eval=-761.923\n",
      "Ep:14480: reward=-868.143, loss=8567.486, eval=-1123.812\n",
      "Ep:14500: reward=-748.681, loss=5752.649, eval=-833.776\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep:14520: reward=-892.896, loss=6783.399, eval=-863.871\n",
      "Ep:14540: reward=-781.374, loss=7110.171, eval=-871.762\n",
      "Ep:14560: reward=-938.825, loss=6151.972, eval=-819.215\n",
      "Ep:14580: reward=-862.576, loss=8366.188, eval=-1012.177\n",
      "Ep:14600: reward=-999.468, loss=9534.776, eval=-687.260\n",
      "Ep:14620: reward=-1113.481, loss=17919.693, eval=-989.333\n",
      "Ep:14640: reward=-911.200, loss=9199.387, eval=-846.223\n",
      "Ep:14660: reward=-807.842, loss=5422.559, eval=-746.052\n",
      "Ep:14680: reward=-1216.408, loss=11114.793, eval=-833.628\n",
      "Ep:14700: reward=-921.718, loss=8845.270, eval=-728.330\n",
      "Ep:14720: reward=-1024.669, loss=8757.397, eval=-644.156\n",
      "Ep:14740: reward=-907.141, loss=6821.738, eval=-586.381\n",
      "Ep:14760: reward=-988.266, loss=7788.014, eval=-866.104\n",
      "Ep:14780: reward=-1093.144, loss=8423.946, eval=-737.885\n",
      "Ep:14800: reward=-911.165, loss=8094.701, eval=-686.760\n",
      "Ep:14820: reward=-801.416, loss=5583.048, eval=-815.865\n",
      "Ep:14840: reward=-1110.292, loss=9082.884, eval=-1172.626\n",
      "Ep:14860: reward=-754.302, loss=6636.397, eval=-706.694\n",
      "Ep:14880: reward=-1019.021, loss=8789.252, eval=-821.183\n",
      "Ep:14900: reward=-1212.111, loss=10018.002, eval=-1178.877\n",
      "Ep:14920: reward=-692.549, loss=5359.177, eval=-921.166\n",
      "Ep:14940: reward=-953.340, loss=8216.631, eval=-743.259\n",
      "Ep:14960: reward=-930.097, loss=10940.913, eval=-1135.448\n",
      "Ep:14980: reward=-1028.836, loss=11974.754, eval=-912.721\n",
      "Ep:15000: reward=-755.895, loss=5903.837, eval=-876.811\n",
      "Ep:15020: reward=-807.341, loss=6156.804, eval=-972.450\n",
      "Ep:15040: reward=-842.544, loss=6070.306, eval=-1095.366\n",
      "Ep:15060: reward=-963.502, loss=6771.770, eval=-1411.536\n",
      "Ep:15080: reward=-915.822, loss=7806.570, eval=-1153.233\n",
      "Ep:15100: reward=-828.097, loss=5819.309, eval=-791.461\n",
      "Ep:15120: reward=-926.575, loss=6257.042, eval=-688.453\n",
      "Ep:15140: reward=-1084.519, loss=11486.188, eval=-1069.589\n",
      "Ep:15160: reward=-943.171, loss=8114.468, eval=-1173.627\n",
      "Ep:15180: reward=-916.542, loss=6459.764, eval=-1007.981\n",
      "Ep:15200: reward=-801.415, loss=6003.993, eval=-1172.319\n",
      "Ep:15220: reward=-799.981, loss=5761.888, eval=-807.042\n",
      "Ep:15240: reward=-878.539, loss=6370.688, eval=-746.148\n",
      "Ep:15260: reward=-843.458, loss=6136.322, eval=-837.921\n",
      "Ep:15280: reward=-994.676, loss=11103.841, eval=-785.563\n",
      "Ep:15300: reward=-786.754, loss=5033.643, eval=-958.347\n",
      "Ep:15320: reward=-1027.376, loss=8160.083, eval=-788.831\n",
      "Ep:15340: reward=-742.197, loss=6020.962, eval=-749.078\n",
      "Ep:15360: reward=-822.605, loss=5901.270, eval=-1572.280\n",
      "Ep:15380: reward=-904.693, loss=7068.335, eval=-1217.524\n",
      "Ep:15400: reward=-937.383, loss=6702.147, eval=-838.119\n",
      "Ep:15420: reward=-987.051, loss=6802.683, eval=-1061.475\n",
      "Ep:15440: reward=-869.064, loss=6145.685, eval=-702.458\n",
      "Ep:15460: reward=-967.450, loss=8086.562, eval=-907.391\n",
      "Ep:15480: reward=-1009.514, loss=9012.947, eval=-817.006\n",
      "Ep:15500: reward=-899.222, loss=5889.979, eval=-958.563\n",
      "Ep:15520: reward=-1137.130, loss=10054.809, eval=-695.771\n",
      "Ep:15540: reward=-813.936, loss=5413.262, eval=-904.987\n",
      "Ep:15560: reward=-845.141, loss=5964.312, eval=-832.078\n",
      "Ep:15580: reward=-1058.512, loss=11984.979, eval=-869.450\n",
      "Ep:15600: reward=-1254.283, loss=14023.921, eval=-706.595\n",
      "Ep:15620: reward=-845.758, loss=6406.642, eval=-777.310\n",
      "Ep:15640: reward=-999.514, loss=6540.416, eval=-1056.068\n",
      "Ep:15660: reward=-836.121, loss=7728.930, eval=-789.341\n",
      "Ep:15680: reward=-858.551, loss=5111.880, eval=-1259.637\n",
      "Ep:15700: reward=-859.520, loss=5808.730, eval=-761.850\n",
      "Ep:15720: reward=-995.337, loss=7409.363, eval=-1025.420\n",
      "Ep:15740: reward=-901.849, loss=5905.489, eval=-767.314\n",
      "Ep:15760: reward=-704.371, loss=3797.122, eval=-778.625\n",
      "Ep:15780: reward=-782.352, loss=5367.423, eval=-722.017\n",
      "Ep:15800: reward=-816.246, loss=6499.879, eval=-921.864\n",
      "Ep:15820: reward=-864.880, loss=6791.009, eval=-780.889\n",
      "Ep:15840: reward=-930.664, loss=12049.377, eval=-1041.255\n",
      "Ep:15860: reward=-842.345, loss=8678.990, eval=-958.550\n",
      "Ep:15880: reward=-1055.238, loss=8452.468, eval=-733.015\n",
      "Ep:15900: reward=-806.603, loss=4828.230, eval=-1397.388\n",
      "Ep:15920: reward=-988.018, loss=7893.339, eval=-633.185\n",
      "Ep:15940: reward=-817.673, loss=7127.186, eval=-1050.339\n",
      "Ep:15960: reward=-843.987, loss=6445.530, eval=-683.693\n",
      "Ep:15980: reward=-1061.502, loss=7748.961, eval=-1059.990\n",
      "Ep:16000: reward=-922.039, loss=7260.334, eval=-1188.797\n",
      "Ep:16020: reward=-1021.153, loss=8999.967, eval=-841.499\n",
      "Ep:16040: reward=-950.673, loss=7221.121, eval=-847.794\n",
      "Ep:16060: reward=-1017.392, loss=9194.108, eval=-867.027\n",
      "Ep:16080: reward=-930.716, loss=6329.230, eval=-842.632\n",
      "Ep:16100: reward=-941.551, loss=8515.597, eval=-1030.463\n",
      "Ep:16120: reward=-1086.009, loss=10590.416, eval=-1330.032\n",
      "Ep:16140: reward=-1128.141, loss=9461.309, eval=-689.191\n",
      "Ep:16160: reward=-1062.993, loss=7460.646, eval=-718.276\n",
      "Ep:16180: reward=-1096.239, loss=7940.685, eval=-1048.276\n",
      "Ep:16200: reward=-972.878, loss=11400.515, eval=-1246.669\n",
      "Ep:16220: reward=-1058.104, loss=10721.796, eval=-1068.407\n",
      "Ep:16240: reward=-888.975, loss=6246.152, eval=-1327.364\n",
      "Ep:16260: reward=-765.037, loss=4735.814, eval=-861.858\n",
      "Ep:16280: reward=-988.440, loss=7005.756, eval=-1431.823\n",
      "Ep:16300: reward=-990.015, loss=8805.292, eval=-959.947\n",
      "Ep:16320: reward=-910.520, loss=5679.939, eval=-1262.373\n",
      "Ep:16340: reward=-809.992, loss=5649.984, eval=-693.431\n",
      "Ep:16360: reward=-806.845, loss=5969.079, eval=-1114.394\n",
      "Ep:16380: reward=-798.619, loss=4735.285, eval=-1082.384\n",
      "Ep:16400: reward=-849.415, loss=5670.362, eval=-862.251\n",
      "Ep:16420: reward=-879.854, loss=6882.360, eval=-722.445\n",
      "Ep:16440: reward=-902.808, loss=7285.819, eval=-969.669\n",
      "Ep:16460: reward=-873.326, loss=7370.354, eval=-1070.190\n",
      "Ep:16480: reward=-778.430, loss=4365.925, eval=-1253.331\n",
      "Ep:16500: reward=-997.361, loss=9628.673, eval=-656.910\n",
      "Ep:16520: reward=-919.658, loss=7239.198, eval=-685.226\n",
      "Ep:16540: reward=-757.925, loss=4929.288, eval=-1082.670\n",
      "Ep:16560: reward=-916.140, loss=7164.094, eval=-1070.591\n",
      "Ep:16580: reward=-996.359, loss=8757.676, eval=-742.549\n",
      "Ep:16600: reward=-919.168, loss=6287.078, eval=-741.005\n",
      "Ep:16620: reward=-921.326, loss=5714.121, eval=-1012.783\n",
      "Ep:16640: reward=-882.026, loss=6270.134, eval=-969.073\n",
      "Ep:16660: reward=-1010.834, loss=8075.091, eval=-955.451\n",
      "Ep:16680: reward=-888.963, loss=7931.457, eval=-907.870\n",
      "Ep:16700: reward=-887.600, loss=6814.785, eval=-1243.743\n",
      "Ep:16720: reward=-1022.701, loss=15047.978, eval=-882.987\n",
      "Ep:16740: reward=-801.333, loss=4361.470, eval=-736.089\n",
      "Ep:16760: reward=-805.161, loss=4995.251, eval=-644.250\n",
      "Ep:16780: reward=-918.098, loss=7046.864, eval=-986.028\n",
      "Ep:16800: reward=-1058.960, loss=8480.756, eval=-976.173\n",
      "Ep:16820: reward=-819.587, loss=5370.098, eval=-821.086\n",
      "Ep:16840: reward=-798.066, loss=4403.871, eval=-1126.428\n",
      "Ep:16860: reward=-702.077, loss=4282.167, eval=-980.401\n",
      "Ep:16880: reward=-818.985, loss=5512.385, eval=-822.118\n",
      "Ep:16900: reward=-796.398, loss=5454.899, eval=-800.039\n",
      "Ep:16920: reward=-914.067, loss=5913.850, eval=-755.672\n",
      "Ep:16940: reward=-1453.327, loss=18179.377, eval=-864.385\n",
      "Ep:16960: reward=-828.665, loss=5856.028, eval=-802.610\n",
      "Ep:16980: reward=-935.320, loss=5446.773, eval=-869.664\n",
      "Ep:17000: reward=-860.816, loss=6360.125, eval=-816.046\n",
      "Ep:17020: reward=-906.646, loss=6115.168, eval=-1150.287\n",
      "Ep:17040: reward=-835.805, loss=5246.497, eval=-828.930\n",
      "Ep:17060: reward=-867.922, loss=6727.758, eval=-819.373\n",
      "Ep:17080: reward=-917.727, loss=6262.518, eval=-1343.905\n",
      "Ep:17100: reward=-765.090, loss=5950.489, eval=-640.528\n",
      "Ep:17120: reward=-718.531, loss=4855.478, eval=-874.749\n",
      "Ep:17140: reward=-941.828, loss=7373.276, eval=-712.343\n",
      "Ep:17160: reward=-956.273, loss=6912.243, eval=-1280.100\n",
      "Ep:17180: reward=-793.741, loss=5384.628, eval=-820.343\n",
      "Ep:17200: reward=-867.278, loss=11727.398, eval=-863.726\n",
      "Ep:17220: reward=-1060.844, loss=8362.835, eval=-780.399\n",
      "Ep:17240: reward=-910.010, loss=6296.192, eval=-990.298\n",
      "Ep:17260: reward=-822.478, loss=5161.084, eval=-1006.345\n",
      "Ep:17280: reward=-882.983, loss=5784.931, eval=-719.494\n",
      "Ep:17300: reward=-770.055, loss=4812.980, eval=-962.767\n",
      "Ep:17320: reward=-682.122, loss=4717.223, eval=-667.474\n",
      "Ep:17340: reward=-803.666, loss=5685.090, eval=-1492.150\n",
      "Ep:17360: reward=-781.527, loss=5355.412, eval=-796.716\n",
      "Ep:17380: reward=-798.146, loss=5650.607, eval=-730.596\n",
      "Ep:17400: reward=-814.819, loss=5237.126, eval=-745.443\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep:17420: reward=-849.839, loss=6189.474, eval=-973.874\n",
      "Ep:17440: reward=-931.214, loss=9729.557, eval=-565.110\n",
      "Ep:17460: reward=-817.787, loss=6239.010, eval=-671.902\n",
      "Ep:17480: reward=-892.378, loss=5546.915, eval=-766.899\n",
      "Ep:17500: reward=-898.110, loss=7582.943, eval=-725.218\n",
      "Ep:17520: reward=-904.143, loss=6520.328, eval=-1300.883\n",
      "Ep:17540: reward=-939.894, loss=5767.913, eval=-1000.623\n",
      "Ep:17560: reward=-1174.919, loss=15083.149, eval=-1280.504\n",
      "Ep:17580: reward=-972.922, loss=8057.553, eval=-882.902\n",
      "Ep:17600: reward=-818.468, loss=5058.531, eval=-950.755\n",
      "Ep:17620: reward=-810.203, loss=4794.973, eval=-604.303\n",
      "Ep:17640: reward=-952.713, loss=8115.436, eval=-976.756\n",
      "Ep:17660: reward=-933.976, loss=9608.118, eval=-1005.468\n",
      "Ep:17680: reward=-1022.730, loss=9039.509, eval=-893.134\n",
      "Ep:17700: reward=-798.482, loss=5568.576, eval=-832.701\n",
      "Ep:17720: reward=-976.452, loss=9303.878, eval=-852.100\n",
      "Ep:17740: reward=-837.733, loss=5261.947, eval=-1018.987\n",
      "Ep:17760: reward=-878.877, loss=5998.440, eval=-772.241\n",
      "Ep:17780: reward=-755.433, loss=4855.604, eval=-866.274\n",
      "Ep:17800: reward=-1057.203, loss=8875.814, eval=-814.320\n",
      "Ep:17820: reward=-927.169, loss=7010.881, eval=-757.178\n",
      "Ep:17840: reward=-835.778, loss=4962.617, eval=-1062.324\n",
      "Ep:17860: reward=-948.100, loss=8557.682, eval=-691.752\n",
      "Ep:17880: reward=-837.075, loss=4996.843, eval=-811.802\n",
      "Ep:17900: reward=-967.735, loss=5693.620, eval=-828.876\n",
      "Ep:17920: reward=-850.329, loss=6560.242, eval=-768.120\n",
      "Ep:17940: reward=-1104.636, loss=13857.938, eval=-636.075\n",
      "Ep:17960: reward=-1059.015, loss=9766.776, eval=-1334.034\n",
      "Ep:17980: reward=-922.354, loss=7120.530, eval=-1251.546\n",
      "Ep:18000: reward=-935.732, loss=5531.601, eval=-1067.818\n",
      "Ep:18020: reward=-795.633, loss=5061.937, eval=-1140.415\n",
      "Ep:18040: reward=-986.757, loss=7964.812, eval=-984.238\n",
      "Ep:18060: reward=-829.602, loss=5535.419, eval=-1029.260\n",
      "Ep:18080: reward=-1197.199, loss=68479.734, eval=-986.598\n",
      "Ep:18100: reward=-1248.702, loss=20518.309, eval=-961.495\n",
      "Ep:18120: reward=-794.192, loss=4738.318, eval=-674.023\n",
      "Ep:18140: reward=-885.402, loss=5434.169, eval=-1237.249\n",
      "Ep:18160: reward=-1084.476, loss=8043.857, eval=-646.301\n",
      "Ep:18180: reward=-938.016, loss=5726.482, eval=-1216.100\n",
      "Ep:18200: reward=-842.795, loss=6206.438, eval=-888.558\n",
      "Ep:18220: reward=-999.473, loss=6939.728, eval=-943.707\n",
      "Ep:18240: reward=-828.090, loss=4890.460, eval=-856.427\n",
      "Ep:18260: reward=-653.858, loss=3793.002, eval=-906.059\n",
      "Ep:18280: reward=-1096.644, loss=8937.907, eval=-893.902\n",
      "Ep:18300: reward=-976.226, loss=6528.871, eval=-1212.386\n",
      "Ep:18320: reward=-975.863, loss=8691.717, eval=-906.572\n",
      "Ep:18340: reward=-880.413, loss=5438.732, eval=-712.439\n",
      "Ep:18360: reward=-868.112, loss=6444.416, eval=-819.023\n",
      "Ep:18380: reward=-931.993, loss=5717.457, eval=-906.635\n",
      "Ep:18400: reward=-1152.852, loss=11280.619, eval=-912.277\n",
      "Ep:18420: reward=-847.819, loss=5016.257, eval=-881.961\n",
      "Ep:18440: reward=-903.716, loss=5847.907, eval=-1104.391\n",
      "Ep:18460: reward=-1054.998, loss=7968.699, eval=-1097.804\n",
      "Ep:18480: reward=-1176.879, loss=11248.199, eval=-731.559\n",
      "Ep:18500: reward=-826.156, loss=5463.821, eval=-911.888\n",
      "Ep:18520: reward=-905.574, loss=6751.304, eval=-900.093\n",
      "Ep:18540: reward=-931.995, loss=5776.588, eval=-897.844\n",
      "Ep:18560: reward=-628.225, loss=3423.366, eval=-592.910\n",
      "Ep:18580: reward=-887.018, loss=6202.537, eval=-1292.747\n",
      "Ep:18600: reward=-1051.953, loss=14383.093, eval=-842.867\n",
      "Ep:18620: reward=-865.597, loss=5004.625, eval=-986.682\n",
      "Ep:18640: reward=-805.844, loss=6404.848, eval=-714.104\n",
      "Ep:18660: reward=-808.076, loss=5439.804, eval=-948.658\n",
      "Ep:18680: reward=-938.200, loss=7579.225, eval=-669.383\n",
      "Ep:18700: reward=-935.851, loss=6265.938, eval=-990.943\n",
      "Ep:18720: reward=-1083.583, loss=10930.785, eval=-1099.947\n",
      "Ep:18740: reward=-1107.629, loss=16849.584, eval=-729.796\n",
      "Ep:18760: reward=-877.200, loss=6671.972, eval=-1104.614\n",
      "Ep:18780: reward=-700.041, loss=5011.591, eval=-1193.229\n",
      "Ep:18800: reward=-910.515, loss=6277.886, eval=-872.534\n",
      "Ep:18820: reward=-990.137, loss=6737.189, eval=-833.263\n",
      "Ep:18840: reward=-1103.737, loss=7399.844, eval=-1281.432\n",
      "Ep:18860: reward=-821.534, loss=4649.836, eval=-1076.735\n",
      "Ep:18880: reward=-592.150, loss=3347.374, eval=-881.290\n",
      "Ep:18900: reward=-870.210, loss=4392.337, eval=-1043.066\n",
      "Ep:18920: reward=-1354.333, loss=27012.123, eval=-813.117\n",
      "Ep:18940: reward=-870.996, loss=6879.800, eval=-814.768\n",
      "Ep:18960: reward=-723.511, loss=5039.987, eval=-977.936\n",
      "Ep:18980: reward=-983.250, loss=6233.491, eval=-670.305\n",
      "Ep:19000: reward=-839.056, loss=9738.336, eval=-777.961\n",
      "Ep:19020: reward=-885.277, loss=6520.027, eval=-739.524\n",
      "Ep:19040: reward=-761.131, loss=4707.726, eval=-1477.654\n",
      "Ep:19060: reward=-867.031, loss=7823.399, eval=-1164.820\n",
      "Ep:19080: reward=-897.644, loss=6486.663, eval=-755.139\n",
      "Ep:19100: reward=-933.709, loss=7075.049, eval=-835.251\n",
      "Ep:19120: reward=-1063.841, loss=9726.024, eval=-976.935\n",
      "Ep:19140: reward=-1034.197, loss=7526.116, eval=-764.291\n",
      "Ep:19160: reward=-892.352, loss=6493.844, eval=-1171.834\n",
      "Ep:19180: reward=-922.036, loss=8837.647, eval=-720.217\n",
      "Ep:19200: reward=-712.043, loss=4507.785, eval=-923.056\n",
      "Ep:19220: reward=-912.481, loss=6530.888, eval=-736.346\n",
      "Ep:19240: reward=-1016.235, loss=10497.987, eval=-837.834\n",
      "Ep:19260: reward=-943.219, loss=7885.601, eval=-711.270\n",
      "Ep:19280: reward=-1095.146, loss=9207.817, eval=-973.102\n",
      "Ep:19300: reward=-1044.655, loss=9030.947, eval=-884.105\n",
      "Ep:19320: reward=-703.129, loss=5018.250, eval=-1170.570\n",
      "Ep:19340: reward=-844.535, loss=5430.045, eval=-990.521\n",
      "Ep:19360: reward=-880.073, loss=13292.665, eval=-917.711\n",
      "Ep:19380: reward=-1044.600, loss=7282.777, eval=-778.972\n",
      "Ep:19400: reward=-725.194, loss=4844.603, eval=-1085.610\n",
      "Ep:19420: reward=-1402.330, loss=16499.043, eval=-858.806\n",
      "Ep:19440: reward=-855.957, loss=6081.820, eval=-789.914\n",
      "Ep:19460: reward=-917.123, loss=6194.076, eval=-1306.437\n",
      "Ep:19480: reward=-767.417, loss=5053.401, eval=-949.092\n",
      "Ep:19500: reward=-946.698, loss=8594.986, eval=-823.957\n",
      "Ep:19520: reward=-886.029, loss=7201.657, eval=-767.852\n",
      "Ep:19540: reward=-1107.032, loss=8719.198, eval=-864.699\n",
      "Ep:19560: reward=-775.372, loss=4939.446, eval=-1132.751\n",
      "Ep:19580: reward=-750.546, loss=3900.694, eval=-704.772\n",
      "Ep:19600: reward=-852.640, loss=5572.677, eval=-968.094\n",
      "Ep:19620: reward=-844.403, loss=6309.753, eval=-1179.288\n",
      "Ep:19640: reward=-1030.446, loss=11987.326, eval=-743.889\n",
      "Ep:19660: reward=-807.542, loss=5249.845, eval=-897.896\n",
      "Ep:19680: reward=-849.852, loss=5755.098, eval=-1017.436\n",
      "Ep:19700: reward=-1056.611, loss=6658.448, eval=-991.517\n",
      "Ep:19720: reward=-1125.179, loss=9278.301, eval=-871.256\n",
      "Ep:19740: reward=-890.743, loss=5974.282, eval=-1050.435\n",
      "Ep:19760: reward=-679.082, loss=3880.690, eval=-847.956\n",
      "Ep:19780: reward=-841.798, loss=4244.521, eval=-839.744\n",
      "Ep:19800: reward=-724.534, loss=3886.704, eval=-836.982\n",
      "Ep:19820: reward=-1079.292, loss=8542.544, eval=-783.601\n",
      "Ep:19840: reward=-793.645, loss=4484.319, eval=-1020.962\n",
      "Ep:19860: reward=-771.438, loss=4735.502, eval=-845.635\n",
      "Ep:19880: reward=-724.921, loss=4092.967, eval=-845.556\n",
      "Ep:19900: reward=-811.833, loss=6772.228, eval=-1304.134\n",
      "Ep:19920: reward=-992.130, loss=8694.294, eval=-1004.382\n",
      "Ep:19940: reward=-1090.720, loss=8697.676, eval=-888.762\n",
      "Ep:19960: reward=-869.929, loss=5567.488, eval=-888.543\n",
      "Ep:19980: reward=-1171.875, loss=15199.823, eval=-676.644\n",
      "Ep:20000: reward=-921.095, loss=5791.697, eval=-776.683\n",
      "Done\n"
     ]
    }
   ],
>>>>>>> e53191496aa90881b7f882c44d6899f41de113be
   "source": [
    "num_iter = 1000\n",
    "num_train = 20\n",
    "num_eval = 10 # dont change this\n",
    "for itr in range(num_iter):\n",
    "    #agent.model.epsilon = epsilon * epsilon_decay ** (total_episodes / epsilon_decay_episodes)\n",
    "    #print('** Iteration {}/{} **'.format(itr+1, num_iter))\n",
    "    train_reward, train_loss = run_iteration('train', num_train, agent, gen)\n",
    "    eval_reward, _ = run_iteration('eval', num_eval, agent, gen)\n",
    "    total_episodes += num_train\n",
    "    print('Ep:{}: reward={:.3f}, loss={:.3f}, eval={:.3f}'.format(total_episodes, train_reward, train_loss, eval_reward))\n",
    "    \n",
    "    if eval_reward > 499 and env_name == 'CartPole-v1': # dont change this\n",
    "        print('Success!!! You have solved cartpole task! Time for a bigger challenge!')\n",
    "    \n",
    "    # save model\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "abstract",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-3ef01fef3ab7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# You can visualize your policy at any time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mrun_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'eval'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-db44b6473077>\u001b[0m in \u001b[0;36mrun_iteration\u001b[0;34m(mode, N, agent, gen, horizon, render)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhorizon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhorizon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-db44b6473077>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhorizon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhorizon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/c/Users/anwan/OneDrive - UW Office 365/Khan/AIResearch/foundation/rl/rlhw_util.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, horizon, render)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m                                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m                         \u001b[0mactions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/c/Users/anwan/OneDrive - UW Office 365/Khan/AIResearch/foundation/rl/rlhw_util.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_env\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/gym/core.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode, close)\u001b[0m\n\u001b[1;32m    148\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnsupportedMode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Unsupported rendering mode: {}. (Supported modes for {}: {})'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/gym/core.py\u001b[0m in \u001b[0;36m_render\u001b[0;34m(self, mode, close)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_render\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'human'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_close\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/gym/core.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode, close)\u001b[0m\n\u001b[1;32m    148\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnsupportedMode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Unsupported rendering mode: {}. (Supported modes for {}: {})'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/gym/envs/classic_control/cartpole.py\u001b[0m in \u001b[0;36m_render\u001b[0;34m(self, mode, close)\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m             \u001b[0;32mfrom\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassic_control\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrendering\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrendering\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mViewer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscreen_width\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscreen_height\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m             \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mcartwidth\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcartwidth\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcartheight\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mcartheight\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0maxleoffset\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mcartheight\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m4.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/gym/envs/classic_control/rendering.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, width, height, display)\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwidth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpyglet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWindow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwidth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_close\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow_closed_by_user\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeoms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pyglet/window/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, width, height, caption, resizable, style, fullscreen, visible, vsync, display, screen, config, context, mode)\u001b[0m\n\u001b[1;32m    502\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mscreen\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m             \u001b[0mscreen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_screen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    505\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pyglet/canvas/base.py\u001b[0m in \u001b[0;36mget_default_screen\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0mrtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;32mclass\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mScreen\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         '''\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_screens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_windows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pyglet/canvas/base.py\u001b[0m in \u001b[0;36mget_screens\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0mrtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;32mclass\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mScreen\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         '''\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'abstract'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_default_screen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: abstract"
     ]
=======
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fleeb/anaconda3/lib/python3.5/site-packages/torch/nn/modules/container.py:91: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(-565.5399), None)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
>>>>>>> e53191496aa90881b7f882c44d6899f41de113be
    }
   ],
   "source": [
    "# You can visualize your policy at any time\n",
    "run_iteration('eval', 1, agent, gen, render=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Analysis\n",
    "\n",
    "Plot the performance of each of your agents for the cartpole task and one additional task. When choosing a new environment, make sure is has a discrete action space. For each plot the x axis should show the total number of episodes the model was trained on, and the y axis shows the average total reward per episode.\n",
    "\n",
    "You can leave the plots as cell outputs below, or you can save them as images and submit them separately.\n",
    "\n",
    "### Deliverables\n",
    "- single plot showing both the REINFORCE algorithm's performance, and A3C's performance on the same plot for the cartpole environment (CartPole-v1).\n",
    "- single plot showing both the REINFORCE algorithm's performance, and A3C's performance on the same plot for a second environment of your choice (suggested -> LunarLander-v2, it's a little tricky but watching the agent fly spaceships is very entertaining!).\n",
    "- in every case you models have to learn something for full credit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
