{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# install\n",
    "# pytorch - pip install torch\n",
    "# gym - pip install gym\n",
    "\n",
    "# gym[atari] - pip install gym[atari]\n",
    "\n",
    "from collections import deque\n",
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.multiprocessing as mp\n",
    "from torch import distributions\n",
    "from torch.distributions import Categorical\n",
    "import foundation as fd\n",
    "import foundation.util as util\n",
    "from itertools import islice\n",
    "from tabulate import tabulate\n",
    "\n",
    "import gym\n",
    "\n",
    "from light import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "args = util.NS()\n",
    "\n",
    "args.name = 'test-vpg'\n",
    "args.save_dir = 'results'\n",
    "\n",
    "args.num_iter = 100\n",
    "args.num_train = 10\n",
    "args.num_eval = 5\n",
    "args.logdate = True\n",
    "args.tblog = True\n",
    "args.txtlog = False\n",
    "args.small_print = True\n",
    "\n",
    "args.agent = 'vpg'#'ddpg'#'vpg'\n",
    "args.env = 'InvertedPendulum-v2'#'CartPole-v1' # 'LunarLander-v2'\n",
    "\n",
    "args.policy = 'full'\n",
    "args.hidden_dims = [8,8]\n",
    "args.nonlin = 'prelu'\n",
    "args.discount = 0.99\n",
    "args.epsilon = 0.01\n",
    "args.tau = 0.001\n",
    "args.use_replica = True\n",
    "args.actor_steps = 1\n",
    "\n",
    "args.critic_hidden_dims = [8,8]\n",
    "args.critic_nonlin = 'prelu'\n",
    "\n",
    "args.buffer_max_episodes = 20\n",
    "args.buffer_min_start = 1000\n",
    "args.buffer_batch_size = 128\n",
    "\n",
    "args.cpi_clip = 0.3\n",
    "args.target_kl = None\n",
    "args.kl_weight = 1.\n",
    "args.agent_epochs = 5\n",
    "args.agent_batch_size = 32\n",
    "\n",
    "args.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "args.optim = 'adam'\n",
    "args.lr = 1e-4\n",
    "args.weight_decay = 1e-4\n",
    "\n",
    "print('Using device: {}'.format(args.device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving to results/test-vpg/Mar-04-2019-150220\n",
      "Env: InvertedPendulum-v2 - state-dim=4, action-dim=1 (continuous)\n"
     ]
    }
   ],
   "source": [
    "now = time.strftime(\"%b-%d-%Y-%H%M%S\")\n",
    "args.save = os.path.join(args.save_dir, args.name, now if args.logdate else '')\n",
    "util.create_dir(args.save)\n",
    "logger = util.Logger(args.save, tensorboard=args.tblog, txt=args.txtlog)\n",
    "print('Saving to {}'.format(args.save))\n",
    "\n",
    "env = utils.Pytorch_Gym_Env(args.env, device=args.device)\n",
    "args.state_dim = env.observation_space.shape[0]\n",
    "args.discrete_action_space = isinstance(env.action_space, gym.spaces.Discrete)\n",
    "args.action_dim = env.action_space.n if args.discrete_action_space else env.action_space.shape[0]\n",
    "print('Env: {} - state-dim={}, action-dim={} ({})'.format(args.env, args.state_dim, args.action_dim, 'discrete' if args.discrete_action_space else 'continuous'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "REINFORCE(\n",
       "  (policy): Full_Gaussian_Policy(\n",
       "    (net): Sequential(\n",
       "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
       "      (1): PReLU(num_parameters=1)\n",
       "      (2): Linear(in_features=8, out_features=8, bias=True)\n",
       "      (3): PReLU(num_parameters=1)\n",
       "      (4): Linear(in_features=8, out_features=2, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (baseline): Linear(in_features=4, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if args.agent == 'dqn':\n",
    "    policy = policies.ActionOut_QFunction(args.state_dim, args.action_dim, hidden_dims=args.hidden_dims, nonlin=args.nonlin,\n",
    "                                          epsilon=args.epsilon)\n",
    "elif args.discrete_action_space:\n",
    "    policy = policies.Discrete_Policy(args.state_dim, args.action_dim, hidden_dims=args.hidden_dims, nonlin=args.nonlin)\n",
    "elif args.policy == 'full':\n",
    "    policy = policies.Full_Gaussian_Policy(args.state_dim, args.action_dim, hidden_dims=args.hidden_dims, nonlin=args.nonlin)\n",
    "else:\n",
    "    policy = policies.Gaussian_Policy(args.state_dim, args.action_dim, hidden_dims=args.hidden_dims, nonlin=args.nonlin)\n",
    "    \n",
    "if args.agent == 'vpg':\n",
    "    agent = agents.REINFORCE(policy, discount=args.discount, \n",
    "                             optim_type=args.optim, lr=args.lr, weight_decay=args.weight_decay)\n",
    "elif args.agent == 'ppo':\n",
    "    agent = agents.PPO(policy, discount=args.discount, \n",
    "                       optim_type=args.optim, lr=args.lr, weight_decay=args.weight_decay,\n",
    "                       clipping=args.cpi_clip, target_kl=args.target_kl, kl_weight=args.kl_weight, \n",
    "                       epochs=args.agent_epochs, batch_size=args.agent_batch_size, )\n",
    "elif args.agent == 'a3c':\n",
    "    value_fn = policies.ValueFunction(args.state_dim, hidden_dims=args.critic_hidden_dims, nonlin=args.critic_nonlin)\n",
    "    agent = agents.A3C(actor=policy, critic=value_fn, discount=args.discount, \n",
    "                       optim_type=args.optim, lr=args.lr, weight_decay=args.weight_decay)\n",
    "elif args.agent == 'dqn':\n",
    "    buffer = utils.Replay_Buffer(max_transition_size=args.buffer_max_transitions, device=args.device)\n",
    "    agent = agents.DQN(policy, discount=args.discount, buffer=buffer, batch_size=args.buffer_batch_size, \n",
    "                       min_buffer_size=args.buffer_min_start, tau=args.tau, use_replica=args.use_replica,\n",
    "                       optim_type=args.optim, lr=args.lr, weight_decay=args.weight_decay)\n",
    "elif args.agent == 'ddpg':\n",
    "    assert not args.discrete_action_space\n",
    "    qnet = policies.QFunction(args.state_dim, args.action_dim, hidden_dims=args.critic_hidden_dims, nonlin=args.critic_nonlin)\n",
    "    policy = policies.ActorCritic(policy, qnet)\n",
    "    buffer = utils.Replay_Buffer(max_episode_size=args.buffer_max_episodes, device=args.device)\n",
    "    agent = agents.DDPG(policy, discount=args.discount, actor_steps=args.actor_steps, buffer=buffer, \n",
    "                        min_buffer_size=args.buffer_min_start, batch_size=args.buffer_batch_size, tau=args.tau,\n",
    "                        optim_type=args.optim, lr=args.lr, weight_decay=args.weight_decay)\n",
    "else:\n",
    "    raise Exception('Unknown agent: {}'.format(args.agent))\n",
    "    \n",
    "agent.to(args.device)\n",
    "gen = utils.Generator(env, agent)\n",
    "score = utils.Score(tau=0.01)\n",
    "stats = util.StatsMeter('score', 'rewards-train', 'rewards-eval')\n",
    "stats.shallow_join(agent.stats)\n",
    "total_episodes = 0\n",
    "agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fleeb/workspace/marl/foundation/foundation/util/stats.py:283: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.val = torch.tensor(val).float()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 10: train=6.300, eval=7.000, score=7.000\n",
      "Ep 20: train=8.400, eval=7.000, score=7.000\n",
      "Ep 30: train=7.800, eval=7.000, score=7.000\n",
      "Ep 40: train=7.100, eval=7.000, score=7.000\n",
      "Ep 50: train=7.400, eval=7.000, score=7.000\n",
      "Ep 60: train=6.200, eval=7.000, score=7.000\n",
      "Ep 70: train=6.900, eval=7.000, score=7.000\n",
      "Ep 80: train=5.000, eval=7.000, score=7.000\n",
      "Ep 90: train=6.700, eval=7.000, score=7.000\n",
      "Ep 100: train=7.500, eval=7.000, score=7.000\n",
      "Ep 110: train=8.600, eval=7.000, score=7.000\n",
      "Ep 120: train=6.300, eval=7.000, score=7.000\n",
      "Ep 130: train=7.100, eval=7.000, score=7.000\n",
      "Ep 140: train=8.100, eval=7.000, score=7.000\n",
      "Ep 150: train=9.500, eval=7.000, score=7.000\n",
      "Ep 160: train=8.100, eval=7.000, score=7.000\n",
      "Ep 170: train=5.600, eval=7.000, score=7.000\n",
      "Ep 180: train=5.300, eval=7.000, score=7.000\n",
      "Ep 190: train=7.200, eval=7.000, score=7.000\n",
      "Ep 200: train=7.200, eval=7.000, score=7.000\n",
      "Ep 210: train=6.700, eval=7.000, score=7.000\n",
      "Ep 220: train=7.400, eval=7.000, score=7.000\n",
      "Ep 230: train=5.200, eval=7.000, score=7.000\n",
      "Ep 240: train=7.500, eval=7.000, score=7.000\n",
      "Ep 250: train=7.800, eval=7.000, score=7.000\n",
      "Ep 260: train=5.400, eval=7.000, score=7.000\n",
      "Ep 270: train=10.100, eval=7.000, score=7.000\n",
      "Ep 280: train=8.000, eval=7.000, score=7.000\n",
      "Ep 290: train=7.300, eval=7.000, score=7.000\n",
      "Ep 300: train=4.300, eval=7.000, score=7.000\n",
      "Ep 310: train=6.200, eval=7.000, score=7.000\n",
      "Ep 320: train=6.900, eval=7.000, score=7.000\n",
      "Ep 330: train=8.400, eval=7.000, score=7.000\n",
      "Ep 340: train=6.000, eval=7.000, score=7.000\n",
      "Ep 350: train=5.900, eval=7.000, score=7.000\n",
      "Ep 360: train=6.400, eval=7.000, score=7.000\n",
      "Ep 370: train=8.400, eval=7.000, score=7.000\n",
      "Ep 380: train=6.900, eval=7.000, score=7.000\n",
      "Ep 390: train=8.900, eval=7.000, score=7.000\n",
      "Ep 400: train=8.100, eval=7.000, score=7.000\n",
      "Ep 410: train=5.800, eval=7.000, score=7.000\n",
      "Ep 420: train=8.400, eval=7.000, score=7.000\n",
      "Ep 430: train=6.100, eval=7.000, score=7.000\n",
      "Ep 440: train=6.100, eval=7.000, score=7.000\n",
      "Ep 450: train=7.100, eval=7.000, score=7.000\n",
      "Ep 460: train=7.300, eval=7.000, score=7.000\n",
      "Ep 470: train=8.300, eval=7.000, score=7.000\n",
      "Ep 480: train=4.900, eval=7.000, score=7.000\n",
      "Ep 490: train=4.400, eval=7.000, score=7.000\n",
      "Ep 500: train=5.300, eval=7.000, score=7.000\n",
      "Ep 510: train=6.900, eval=7.000, score=7.000\n",
      "Ep 520: train=7.500, eval=7.000, score=7.000\n",
      "Ep 530: train=6.700, eval=7.000, score=7.000\n",
      "Ep 540: train=6.900, eval=7.000, score=7.000\n",
      "Ep 550: train=8.700, eval=7.000, score=7.000\n",
      "Ep 560: train=7.000, eval=7.000, score=7.000\n",
      "Ep 570: train=6.100, eval=7.000, score=7.000\n",
      "Ep 580: train=5.700, eval=7.000, score=7.000\n",
      "Ep 590: train=5.600, eval=7.000, score=7.000\n",
      "Ep 600: train=5.300, eval=7.000, score=7.000\n",
      "Ep 610: train=6.500, eval=7.000, score=7.000\n",
      "Ep 620: train=6.600, eval=7.000, score=7.000\n",
      "Ep 630: train=8.100, eval=7.000, score=7.000\n",
      "Ep 640: train=7.300, eval=7.000, score=7.000\n",
      "Ep 650: train=6.500, eval=7.000, score=7.000\n",
      "Ep 660: train=8.100, eval=7.000, score=7.000\n",
      "Ep 670: train=6.700, eval=7.000, score=7.000\n",
      "Ep 680: train=7.700, eval=7.000, score=7.000\n",
      "Ep 690: train=6.700, eval=7.000, score=7.000\n",
      "Ep 700: train=6.000, eval=7.000, score=7.000\n",
      "Ep 710: train=7.300, eval=7.000, score=7.000\n",
      "Ep 720: train=6.800, eval=7.000, score=7.000\n",
      "Ep 730: train=4.700, eval=7.000, score=7.000\n",
      "Ep 740: train=7.200, eval=7.000, score=7.000\n",
      "Ep 750: train=7.000, eval=7.000, score=7.000\n",
      "Ep 760: train=6.300, eval=7.000, score=7.000\n",
      "Ep 770: train=8.400, eval=7.000, score=7.000\n",
      "Ep 780: train=7.400, eval=7.000, score=7.000\n",
      "Ep 790: train=8.700, eval=7.000, score=7.000\n",
      "Ep 800: train=8.900, eval=7.000, score=7.000\n",
      "Ep 810: train=7.300, eval=7.000, score=7.000\n",
      "Ep 820: train=8.600, eval=7.000, score=7.000\n",
      "Ep 830: train=9.500, eval=7.000, score=7.000\n",
      "Ep 840: train=7.400, eval=7.000, score=7.000\n",
      "Ep 850: train=9.700, eval=7.000, score=7.000\n",
      "Ep 860: train=6.500, eval=7.000, score=7.000\n",
      "Ep 870: train=5.500, eval=7.000, score=7.000\n",
      "Ep 880: train=5.200, eval=7.000, score=7.000\n",
      "Ep 890: train=5.300, eval=7.000, score=7.000\n",
      "Ep 900: train=6.600, eval=7.000, score=7.000\n",
      "Ep 910: train=8.000, eval=7.000, score=7.000\n",
      "Ep 920: train=6.100, eval=7.000, score=7.000\n",
      "Ep 930: train=6.800, eval=7.000, score=7.000\n",
      "Ep 940: train=6.900, eval=7.000, score=7.000\n",
      "Ep 950: train=6.300, eval=7.000, score=7.000\n",
      "Ep 960: train=7.400, eval=7.000, score=7.000\n",
      "Ep 970: train=7.900, eval=7.000, score=7.000\n",
      "Ep 980: train=11.400, eval=7.000, score=7.000\n",
      "Ep 990: train=7.000, eval=7.000, score=7.000\n",
      "Ep 1000: train=5.000, eval=7.000, score=7.000\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "for itr in range(args.num_iter):\n",
    "    #agent.model.epsilon = epsilon * epsilon_decay ** (total_episodes / epsilon_decay_episodes)\n",
    "    #print('** Iteration {}/{} **'.format(itr+1, num_iter))\n",
    "    train_rewards = utils.run_iteration('train', args.num_train, agent, gen)\n",
    "    eval_rewards = utils.run_iteration('eval', args.num_eval, agent, gen)\n",
    "    total_episodes += args.num_train\n",
    "    score.update_all(eval_rewards)\n",
    "    stats.update('score', score.val)\n",
    "    stats.update('rewards-train', train_rewards.mean())\n",
    "    stats.update('rewards-eval', eval_rewards.mean())\n",
    "    \n",
    "    vals = stats.vals()\n",
    "    logger.update(vals, step=total_episodes)\n",
    "    if args.small_print:\n",
    "        print('Ep {}: train={:.3f}, eval={:.3f}, score={:.3f}'.format(total_episodes, vals['rewards-train'], vals['rewards-eval'], vals['score']))\n",
    "    else:\n",
    "        print('Episode: {}'.format(total_episodes))\n",
    "        print(tabulate(vals))\n",
    "    \n",
    "    # save model\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1722.5884], device='cuda:0')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utils.run_iteration('eval', 1, agent, gen, render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.train()\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.2589], device='cuda:0')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.2463], device='cuda:0', grad_fn=<ExpandBackward>),\n",
       " tensor([0.9736], device='cuda:0', grad_fn=<ExpandBackward>))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = agent.actor.get_pi(s)\n",
    "d.loc, d.scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.3435], device='cuda:0')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created iterator\n",
      "0.15731143951416016\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([191, 4]), torch.Size([191]), torch.Size([191]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = time.time()\n",
    "out = islice(g, 10)\n",
    "states, actions, rewards = map(torch.cat, zip(*out))\n",
    "print(time.time() - start)\n",
    "states.size(), actions.size(), rewards.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\n"
     ]
    }
   ],
   "source": [
    "# define hyperparameters\n",
    "env = Pytorch_Gym_Env('CartPole-v1')\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DQN(\n",
       "  (model): QNet(\n",
       "    (net): Sequential(\n",
       "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
       "      (1): LeakyReLU(negative_slope=0.01)\n",
       "      (2): Linear(in_features=8, out_features=2, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (target_model): QNet(\n",
       "    (net): Sequential(\n",
       "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
       "      (1): LeakyReLU(negative_slope=0.01)\n",
       "      (2): Linear(in_features=8, out_features=2, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (criterion): SmoothL1Loss()\n",
       ")"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#agent = DDPG(state_dim, action_dim, max_buffer_size=1000, min_buffer_size=50)\n",
    "total_episodes = 0\n",
    "epsilon = 0.01\n",
    "agent = DQN(state_dim, action_dim, \n",
    "            max_buffer_size=1000, min_buffer_size=200, batch_size=128, use_replica=False,\n",
    "            lr=1e-3, tau=0.001, weight_decay=1e-3, epsilon=epsilon)\n",
    "agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create objects\n",
    "# if True:\n",
    "#     device = 'cuda'\n",
    "#     agent.to(device)\n",
    "#     env.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon_decay_episodes = 1000\n",
    "epsilon_decay = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep:50: reward=9.800, loss=0.075, eval=23.400\n",
      "Ep:100: reward=12.160, loss=0.082, eval=14.400\n",
      "Ep:150: reward=24.320, loss=0.047, eval=9.800\n",
      "Ep:200: reward=24.220, loss=0.033, eval=42.600\n",
      "Ep:250: reward=39.800, loss=0.015, eval=34.400\n",
      "Ep:300: reward=30.880, loss=0.016, eval=17.800\n",
      "Ep:350: reward=27.180, loss=0.017, eval=75.000\n",
      "Ep:400: reward=28.700, loss=0.021, eval=17.000\n",
      "Ep:450: reward=33.120, loss=0.016, eval=14.200\n",
      "Ep:500: reward=34.120, loss=0.013, eval=57.600\n",
      "Ep:550: reward=50.880, loss=0.011, eval=68.400\n",
      "Ep:600: reward=41.140, loss=0.013, eval=10.000\n",
      "Ep:650: reward=38.120, loss=0.014, eval=43.400\n",
      "Ep:700: reward=46.040, loss=0.013, eval=34.400\n",
      "Ep:750: reward=51.880, loss=0.013, eval=11.800\n",
      "Ep:800: reward=51.860, loss=0.011, eval=93.800\n",
      "Ep:850: reward=40.580, loss=0.012, eval=57.200\n"
     ]
    }
   ],
   "source": [
    "num_iter = 20\n",
    "num_train = 50\n",
    "num_eval = 5\n",
    "for itr in range(num_iter):\n",
    "    #agent.model.epsilon = epsilon * epsilon_decay ** (total_episodes / epsilon_decay_episodes)\n",
    "    #print('** Iteration {}/{} **'.format(itr+1, num_iter))\n",
    "    train_reward, train_loss = run_episodes('train', num_train, agent, env)\n",
    "    eval_reward, _ = run_episodes('eval', num_eval, agent, env)\n",
    "    total_episodes += num_train\n",
    "    print('Ep:{}: reward={:.3f}, loss={:.3f}, eval={:.3f}'.format(total_episodes, train_reward, train_loss, eval_reward))\n",
    "    \n",
    "    # save model\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.critic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75.9"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_iteration('eval', 1, agent, gen, render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N, D, M = 10, 4, 1\n",
    "\n",
    "f = nn.Linear(D, M)\n",
    "x = torch.randn(N,D)\n",
    "y = f(x).detach()\n",
    "g = util.solve(x,y)\n",
    "g.weight.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env = gym.make('Pong-v4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(210, 160, 3)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env = gym.make('LunarLander-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.00551643,  1.4003555 , -0.55877346, -0.46954992,  0.00639898,\n",
       "        0.12657054,  0.        ,  0.        ], dtype=float32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([192,   0,   0,   0, 110,  38,   0,   7,  63,   1,  60,  59,   0,\n",
       "         0,   0,  62, 255,   0, 255, 253,   0,   8,   0,  24, 128,  32,\n",
       "         1,  86, 247,  86, 247,  86, 247, 134, 243, 245, 243, 240, 240,\n",
       "       242, 242,  32,  32,  64,  64,  64, 188,  65, 189,   0,   8, 109,\n",
       "        37,  37,  60,   0,   0,   0,   0, 109, 109,  37,  37, 192, 192,\n",
       "       192, 192,   1, 192, 202, 247, 202, 247, 202, 247, 202, 247,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,  54, 236, 242, 121, 240], dtype=uint8)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "for _ in range(1000):\n",
    "    env.render()\n",
    "    _,_,done,_ = env.step(env.action_space.sample())\n",
    "    plt.pause(0.02)\n",
    "    if done:\n",
    "        print('stop')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
